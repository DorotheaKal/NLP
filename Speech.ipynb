{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Επεξεργασία Φωνής και Φυσικής Γλώσσας</h1> \n",
    "<h2 align = \"center\">1η Προπαρασκευαστική Εργασία</h2> \n",
    "<h3 align = \"center\"> Θεoδωρόπουλος Νικήτας -03115185</h3>\n",
    "<h3 align = \"center\"> Καλλιώρα Δωροθέα - 03115176</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Σκοπός:</h3>\n",
    "Σκοπός της εργαστηριακής άσκησης είναι η δημιουργία ενός απλού ορθογράφου με χρήση μηχανών πεπερασμένων καταστάσεων (fst) με τη βοήθεια της βιβλιοθήκης openfst (v1.6.1). Θα χρησιμοποιήσουμε corpus απο δημόσια διαθέσιμα βιβλία για να δημιουργήσουμε ενα γλωσσικό μοντέλο σε μορφή λεξικού. Τέλος θα γίνει εισαγωγή σε αναπαραστάσεις word2vec. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 1***\n",
    "\n",
    "(α)\n",
    "Κατασκευάζουμε το corpus με σύμπτηξη plain text βιβλίων, δημόσια διαθέσημα στο Project Gutenberg. Θα χρησιμοποιήσουμε corpus των δυο παρακάτω βιβλίων:  \n",
    "<br>\n",
    "<div class=\"image123\">\n",
    "    <div class=\"imgContainer\"  Style = \"float:left\">\n",
    "        <p>The War of the Worlds by H. G. Wells</p>\n",
    "        <img src=\"./img/book36.jpg\" height=\"50\" width=\"150\"/>\n",
    "    </div>\n",
    "    <div class=\"imgContainer\" Style = \"float:right\">\n",
    "        <p>Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm</p>\n",
    "        <img class=\"middle-img\" src=\"./img/book2591.jpg\"/ height=\"10\" width=\"150\"/>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -P ./data/ http://www.gutenberg.org/files/36/36-0.txt\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/2591/2591-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 2***  Niki\n",
    "ΔΙαβαζουμε αρχειο κτλπ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OqT0WsSYSLb"
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "def identity_preprocess(str):\n",
    "  return str\n",
    "\n",
    "def readfile(path,preprocess=identity_preprocess):\n",
    "  processed_txt=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    processed_txt = processed_txt + preprocess(line)\n",
    "  return processed_txt \n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.strip().lower()\n",
    "    s = ''.join(c if c.isalpha() else ' ' for c in s)\n",
    "    # We replace all non-alpha characters with ' '\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom :\n",
      " ['at', 'eight', 'o', 'clock', 'on', 'thursday', 'morning', 'arthur', 'didn', 't', 'feel', 'very', 'good', 'he', 'quickly', 'rushed', 'to', 'the', 'doctor'] \n",
      "\n",
      "nltk punkt :\n",
      " ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', '.Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'He', 'quickly', 'rushed', 'to', 'the', 'Doctor', '!'] \n",
      "\n",
      "nltk sentece detector :\n",
      " [\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\", 'He quickly rushed to the Doctor!'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/dorotheakal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Step 2 (d)\n",
    "\n",
    "#!pip install --user -U nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\n",
    "              He quickly rushed to the Doctor!\"\"\"\n",
    "tokenizers = [tokenize,nltk.word_tokenize,sent_detector.tokenize]\n",
    "names = [\"custom\",\"nltk punkt\",\"nltk sentece detector\"]\n",
    "for tokenizer,name in zip(tokenizers,names):\n",
    "    print(name,\":\\n\",tokenizer(sentence),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 2*** \n",
    "\n",
    "(δ) \n",
    "Για αυτό το ερώτημα επιλέξαμε να χρησιμοποιήσουμε τους παρακάτω δυο tokenizers απο την βιβλιοθήκη nltk καθώς και την δική μας υλοποίηση:\n",
    "\n",
    "<b>Sentence Tokenizer</b>: Αυτός ο tokenizer χωρίζει το κείμενο σε μία λίστα από προτάσεις χρησιμοποιώντας έναν unsupervised αλγόριθμο για να δημιουργήσει ένα μοντέλο που αναγμωρίζει τις λέξεις με απόστροφο, τον συνδυασμό λέξεων και τις λέξεις που ξεκινούν τις προτάσεις. Ο αλγόριθμος αυτός πρέπει να εκπαιδευτεί σε ένα μεγάλο σύνολο δεδομένων πριν μπορέσει να χρησιμοποιηθεί αποτελεσματικά. Το NLTK data package περιλαμβάνει ένα προ-εκπαιδευμένο Punkt tokenizer για την αγγλική γλώσσα.\n",
    "\n",
    "<b>Word Τokenizer</b>: Αυτός ο tokenizer εντοπίζει τις λέξεις του κειμένου (ενός string) και τις αποθηκεύει σε μία λίστα. Από τα σημεία στίξης κρατάει μόνο εκείνα που διαχωρίζουν προτάσεις. Ακόμα, όπως και ο Sentence Tokenizer χρησιμοποιεί ένα προ-εκπαιδευμένο Punkt tokenizer για την αγγλική γλώσσα το οποίο δεν χωρίζει τις λέξεις με απόστροφο όπως το didn't. \n",
    "\n",
    "<b>Tokenize()</b>: Αυτός είναι ο tokenizer που υλοποιήθηκε στα πλαίσια της άσκησης. Παίρνει σαν είσοδο ένα sting και επιστρέφει μόνο γραμματικούς χαρακτήρες (λέξεις) σε μία λίστα. Αυτό υλοποιέιται αφαιρώντας όλα τα σημεία στίξης, τα κενά, τις αλλαγές γραμμής, τους αριθμούς, και κάνοντας όλα τα γράμματα των λέξεων πεζά. \n",
    "\n",
    "Η κύρια διαφορά της tokenize() με τους παραπάνω tokenizers της βιβλιοθήκης nltk είναι ότι η tokenize() αφαιρώντας όλα τα σημεία στίξης, αφαιρεί και τις αποστρόφους από τις λέξεις. Αυτό έχει ως αποτέλεσμα η λέξη \"o'clock\" να μην εμφανίζεται ως έχει στην τελική λίστα με τις λέξεις του κειμένου, αλλά να διασπάται στις λέξεις \"o\" και \"clock\". Όμως, η λέξη \"didn\" που προκύπτει δεν αποτελεί μία σωστή αγγλική λέξη και άρα δεν θα έπρεπε να υπάρχει στο λεξικό που φτιάχνουμε.   \n",
    "\n",
    "Εμείς στα πλαίσια της άσκησης λοιπόν, βλέπουμε ότι έχουμε υλοποιήσει μια πολύ απλή έκδοση της tokenize. Ακόμα, συγκρίνοντας την υλοποιήση μας με τις υλοποιήσεις των δύο παραπάνω tokenizers συμπεραίνουμε, ότι ενώ θα μπορούσαμε να διορθώσουμε τον διαχωρισμό των λέξεων με απόστροφο, δεν θα μπορούσαμε εύκολα να εντοπίζουμε τα σημεία στίξης που διαχωρίζουν τις προτάσεις, αφού η Word Tokenizer χρησιμοποιεί έναν προ-εκπαιδευμένο Punkt tokenizer για αυτό το σκοπό.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1574168112504,
     "user": {
      "displayName": "Dorothea Kalliora",
      "photoUrl": "",
      "userId": "17412342597228224634"
     },
     "user_tz": -120
    },
    "id": "YXntX-DgfvSF",
    "outputId": "19b454d9-ca2c-4be2-cb8c-bfe654403163",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169392 words overall\n",
      "9288 unique words in corpus\n",
      "28 symbols in alphabet:\n",
      "['b', 'm', 'p', 'q', 'o', 'x', 's', 'n', 'l', 'z', 'k', 'w', 'g', 'æ', 'e', 'd', 'a', 'f', 'c', 't', 'i', 'ç', 'h', 'y', 'r', 'u', 'j', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "import random\n",
    "random.seed()\n",
    "\n",
    "text_1 = readfile('./data/36-0.txt',tokenize)\n",
    "text_2 = readfile('./data/2591-0.txt',tokenize)\n",
    "text = text_1 + text_2\n",
    "\n",
    "print(len(text),\"words overall\")\n",
    "word_corpus = list(set(text))\n",
    "print(len(word_corpus),\"unique words in corpus\")\n",
    "\n",
    "# Convert list of words to list of chars, get unique chars with set\n",
    "alphabet = list(set([c for word in word_corpus for c in word]))\n",
    "print(len(alphabet),\"symbols in alphabet:\")\n",
    "print(alphabet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με βάση το αλφάβητο που προέκυψε απο το word corpus δημιουργούμε το αρχείο συμβόλων εισόδου και εξόδου για τα fst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "def create_syms(alphabet):\n",
    "    f = open(\"./chars.syms\",\"w+\")\n",
    "    f.write(f'<epsilon>     0\\n')  #for <epsilon> index 0\n",
    "    for i in range(len(alphabet)):\n",
    "        f.write(f'{alphabet[i]}     {i+50}\\n')  #for other characters index i+50\n",
    "        \n",
    "create_syms(alphabet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 5*** niki\n",
    "\n",
    "(α)Ως μετρική για την απόσταση 2 λέξεων θα χρησιμοποιήσουμε την απόσταση Levenshtein (ή edit distance). Η απόσταση μπορέι να υπολογιστεί αναδρομικά με χρήση dynamic programming (DP). Ανάμεσα σε δύο λέξεις μπορούν να γίνουν τρείς τύποι μετατροπών: εισαγωγή ενός χαρακτήρα ($ \\epsilon \\rightarrow a$), μετατροπή ενός χαρακτήρα ($ a \\rightarrow b$) και διαγραφή ενός χαρακτήρα ($a \\rightarrow \\epsilon$). Θεωρούμε αρχικά ίδιο κόστος για όλες τις μετατροπές.\n",
    "\n",
    "Δημιουργούμε κατάλληλο fst με μία κατάσταση, αντιστοιχίζοντας κάθε χαρακτήρα σε κάθε άλλον και στο $\\epsilon$, και το $\\epsilon$ σε κάθε χαρακτήρα με βάρος 1. Αντιστοιχίζουμε ακόμα τον χαρακτήρα στον εαυτό του με βάρος 0. Αν πάρουμε το shortest path τότε η έξοδος θα είναι λέξη εισόδου, αφού το ελάχιστο βάρος προκύπτει αν δεν γίνει καμία μετατροπή. \n",
    "\n",
    "(β)Στην υλοποίηση που έγινε για το ερώτημα 5 έχουμε υποθέσει ότι όλα τα edits έχουν ίσο βάρος. Αυτή ειναι μια αφελής υπόθεση αφού κάθε είδος λάθους (παράλλειψη, επανάληψη ή λάθος τοποθέτηση γράμματος)  έχει την ίδια πιθανότητα εμφάνισης. Ιδανικά θα θέλαμε να γνωρίζουμε την κατανομή του λάθους, την οποία μπορούμε να εκτιμήσουμε απο ένα σύνολο δεδομένων. Για ενα ζεύγος (σωστή λέξη, λανθασμένη λέξη) μπορούμε να υπολογίσουμε τη διόρθωση που απαιτείται και η πιθανότητα θα προκύψει $ P[ a\\rightarrow b] = \\frac{|error = b \\rightarrow a|}{|errors|}, \\forall a,b \\in \\{A+\\epsilon\\} $, όπου $A$ το αλφάβητο. Αυτές εινα οι a priori πιθανότητες λαθών με βάση το training data. Αυτές μπορούν να χρησιμοποιηθούν για να υπολιστούν τα βάρη στο μοντέλο αναγνώρισης μας με fst, ώστε να είναι αντιστρόφως ανάλογα της πιθανότητας. Μια καλή συνάρτηση αντιστοίχησης που διατηρεί τις αναλογίες είναι η $\\frac{1}{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "\n",
    "# arc format: src dest ilabel olabel [weight]\n",
    "# final state format: state [weight]\n",
    "# lines may occur in any order except initial state must be first line\n",
    "# unspecified weights default to 0.0 (for the library-default Weight type) \n",
    "\n",
    "f = open('lev.fst',\"w+\")\n",
    "\n",
    "def format_arc(src,dst,src_sym,dst_sym,w,f,acceptor=False):\n",
    "    if not acceptor:\n",
    "        f.write(\"{} {} {} {} {} \\n\".format(src,dst,src_sym,dst_sym,w))\n",
    "    else:\n",
    "        f.write(\"{} {} {} \\n\".format(src,dst,src_sym))\n",
    "letters =  alphabet\n",
    "\n",
    "for i in range(0, len(letters)):\n",
    "    format_arc(src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=letters[i], w=1,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[i], w=0,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=\"<epsilon>\", w=1,f=f)\n",
    "    for j in range(0, len(letters)):\n",
    "        if(j!=i):\n",
    "            format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[j], w=1,f=f)   \n",
    "f.write('0\\n')\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev.fst lev.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 6***\n",
    "\n",
    "Ένας <b>αποδοχέας(acceptor)</b> είναι ένα fst όπου κάθε μετάβαση έχει ενα label (και προεραιτικά βάρος). Στην βιβλιοθήκη openfst1-6-1 μπορέι να θεωρηθεί ως μετατροπέας(transducer) με ίδιο input και output label.\n",
    "\n",
    "Σε αυτό το ερώτημα κατασκευάζουμε εναν αποδοχέα που αποδέχεται κάθε λέξη του λεξικού. Ο αποδοχέας έχει μία κοινή αρχική κατάσταση για κάθε λέξη και δεν έχουμε βάρος στις μεταβάσεις (w = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "\n",
    "def create_acceptor(words,file):\n",
    "    f = open(file,\"w+\")\n",
    "    s = 0\n",
    "    final_states = []\n",
    "    for word in words:\n",
    "        format_arc(0,s+1,word[0],word[0],0,f,acceptor=True)\n",
    "        s += 1\n",
    "        for letter in word[1:]:\n",
    "            format_arc(s,s+1,letter, letter, 0,f,acceptor=True)\n",
    "            s += 1\n",
    "        final_states.append(s)\n",
    "\n",
    "    for state in final_states:\n",
    "        f.write(f'{state}\\n')\n",
    "    f.close()    \n",
    "create_acceptor(word_corpus,\"acceptor.fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms --acceptor acceptor.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait rosebud.bin1.fst | dot -Tjpg >acceptor.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Τι κάνουν οι απο κατω συναρτήσεις απο την βιβλιοθήκη..... niki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6b\n",
    "\n",
    "!fstrmepsilon acceptor.bin.fst acceptor.bin.fst\n",
    "!fstdeterminize acceptor.bin.fst acceptor.bin.fst\n",
    "!fstminimize acceptor.bin.fst acceptor.bin.fst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "#mini.fst - lev.fst\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev.bin.fst lev.bin.fst\n",
    "!fstarcsort --sort_type=ilabel acceptor.bin.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7(a)\n",
    "\n",
    "!fstcompose  lev.bin.fst acceptor.bin.fst spell_checker.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Μια πιθανή πρόβλεψη του min edit spell checker για τη λέξη cit είναι η: \n",
      "fit"
     ]
    }
   ],
   "source": [
    "#Step 7b\n",
    "\n",
    "create_acceptor(['cit'],\"in.fst\")\n",
    "\n",
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor in.fst in.bin.fst\n",
    "!fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst \n",
    "print(\"Μια πιθανή πρόβλεψη του min edit spell checker για τη λέξη cit είναι η: \")\n",
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tc\tf\t1\r\n",
      "1\t2\ti\ti\r\n",
      "2\t3\tt\tt\r\n",
      "3\r\n"
     ]
    }
   ],
   "source": [
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1   \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contented', 'contented', 'contented', 'contented']\n",
      "['contenpted', 'contende', 'contended', 'contentid']\n",
      "270\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "# Step 8:\n",
    "\n",
    "file = open('./data/spell_checker_test_set',\"r\")\n",
    "y_test = []\n",
    "X_test = []\n",
    "for line in file:\n",
    "    [y,X] = line.split(':')\n",
    "    X = X.split()\n",
    "    for word in X:\n",
    "        X_test.append(word)\n",
    "        y_test.append(y)\n",
    "\n",
    "print(y_test[0:4])\n",
    "print(X_test[0:4])\n",
    "print(len(y_test))\n",
    "print(len(X_test))\n",
    "idxs = random.sample(range(0, len(y_test)), 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Y,X):\n",
    "    correct_pred=0\n",
    "    for y,x in zip(Y,X):\n",
    "        create_acceptor([x],\"input.fst\")\n",
    "\n",
    "        !fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor input.fst input.bin.fst\n",
    "        !fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "        !fstarcsort --sort_type=olabel input.bin.fst input.bin.fst \n",
    "        prediction = !fstcompose input.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "        | fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "        | cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n' \n",
    "        print(\"Wrong = \",x,\"-- Correct = \",y,\"-- Prediction = \",prediction[0])\n",
    "        if y == prediction[0]:\n",
    "            correct_pred+=1\n",
    "    print(f\"Accuracy:{correct_pred/len(Y)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong =  specal -- Correct =  special -- Prediction =  special\n",
      "Wrong =  oppossite -- Correct =  opposite -- Prediction =  opposite\n",
      "Wrong =  compair -- Correct =  compare -- Prediction =  complain\n",
      "Wrong =  stanerdizing -- Correct =  standardizing -- Prediction =  standing\n",
      "Wrong =  contende -- Correct =  contented -- Prediction =  content\n",
      "Wrong =  curtions -- Correct =  curtains -- Prediction =  curious\n",
      "Wrong =  scarsely -- Correct =  scarcely -- Prediction =  scarcely\n",
      "Wrong =  relly -- Correct =  really -- Prediction =  belly\n",
      "Wrong =  reafreshment -- Correct =  refreshment -- Prediction =  refreshed\n",
      "Wrong =  pertend -- Correct =  pretend -- Prediction =  extend\n",
      "Wrong =  possable -- Correct =  possible -- Prediction =  possible\n",
      "Wrong =  seperate -- Correct =  separate -- Prediction =  separate\n",
      "Wrong =  refreshmant -- Correct =  refreshment -- Prediction =  refresh\n",
      "Wrong =  desicate -- Correct =  desiccate -- Prediction =  delicate\n",
      "Wrong =  parrallel -- Correct =  parallel -- Prediction =  parallel\n",
      "Wrong =  experiance -- Correct =  experience -- Prediction =  experience\n",
      "Wrong =  possition -- Correct =  position -- Prediction =  position\n",
      "Wrong =  arrainged -- Correct =  arranged -- Prediction =  arranged\n",
      "Wrong =  singulaur -- Correct =  singular -- Prediction =  singularly\n",
      "Wrong =  bycicle -- Correct =  bicycle -- Prediction =  bicycle\n",
      "Accuracy:0.5%\n"
     ]
    }
   ],
   "source": [
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]\n",
    "predict(Y_rand,X_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 8***\n",
    "\n",
    "Σε αυτό το βήμα χρησιμοποιήσαμε το μετατροπέα που υλοποιήσαμε στο βήμα 7 για να διορθώσουμε κάποιες από τις λέξεις του test set. Παραπάνω φαίνεται το αποτέλεσμα για είκοσι τυχαίες λέξεις του test set.\n",
    "\n",
    "Αρχικά, βλέπουμε ότι το αν θα κάνει τη σωστή διόρθωση ο min edit distance spell checker εξαρτάται και από το αν η σωστή λέξη στην οποία θέλουμε να πάμε υπάρχει στο λεξικό. Αν η λέξη υπάρχει, η πιθανότητα να γίνει η σωστή διόρθωση αυξάνεται. \n",
    "Ακόμα, από τα παραπάνω αποτελέσματα παρατηρούμε ότι όταν έχουμε ισοβαρείς μεταβάσεις το αποτέλεσμα δεν είναι καθόλου ικανοποιητικό. Αυτό συμβαίνει γιατί όλα τα edits έχουν ίσο κόστος και άρα η επιλογή της διόρθωσης γίνεται τυχαία. \n",
    "Τέλος, παρατηρήσαμε ότι επειδή μπορεί να υπάρχουν πολλές λέξεις με το ίδιο edit distance, η επιλογή της λέξης που τυπώνεται γίνεται πάλι τυχαία. Σε αυτό ωφείλονται πάλι οι ισοβαρείς μεταβάσεις που ορίσαμε στον μετατροπέα του βήματος 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 9(a)\n",
    "\n",
    "def tokenized_list(path,preprocess=identity_preprocess):\n",
    "  list_of_sentences=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    l = preprocess(line)\n",
    "    if l:\n",
    "        list_of_sentences.append(l)\n",
    "  return list_of_sentences\n",
    "\n",
    "\n",
    "list1 = tokenized_list('./data/36-0.txt',tokenize)\n",
    "list2 = tokenized_list('./data/2591-0.txt',tokenize)\n",
    "final_list = list1 + list2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.10.25)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.25 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.25)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.25->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.25->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"bell\":\n",
      "     \"planted\" -- sim: 0.3844985067844391\n",
      "     \"dawned\" -- sim: 0.3270135521888733\n",
      "     \"lions\" -- sim: 0.321536123752594\n",
      "     \"party\" -- sim: 0.3153380751609802\n",
      "     \"waiting\" -- sim: 0.3137977421283722\n",
      "     \"drop\" -- sim: 0.2890227437019348\n",
      "     \"ulla\" -- sim: 0.2797062397003174\n",
      "     \"mr\" -- sim: 0.2777710556983948\n",
      "     \"printed\" -- sim: 0.27099916338920593\n",
      "     \"sunset\" -- sim: 0.26970311999320984\n",
      "Most similar words to \"silver\":\n",
      "     \"your\" -- sim: 0.33258122205734253\n",
      "     \"cask\" -- sim: 0.3314518928527832\n",
      "     \"sense\" -- sim: 0.3314470648765564\n",
      "     \"jewels\" -- sim: 0.319366991519928\n",
      "     \"formed\" -- sim: 0.31553730368614197\n",
      "     \"mighty\" -- sim: 0.31182727217674255\n",
      "     \"spun\" -- sim: 0.3105581998825073\n",
      "     \"treasure\" -- sim: 0.30659621953964233\n",
      "     \"knowledge\" -- sim: 0.3021213114261627\n",
      "     \"cake\" -- sim: 0.2975206971168518\n",
      "Most similar words to \"window\":\n",
      "     \"house\" -- sim: 0.4465496838092804\n",
      "     \"room\" -- sim: 0.40661484003067017\n",
      "     \"door\" -- sim: 0.40487104654312134\n",
      "     \"gates\" -- sim: 0.36346548795700073\n",
      "     \"street\" -- sim: 0.3316619396209717\n",
      "     \"kitchen\" -- sim: 0.31235194206237793\n",
      "     \"circle\" -- sim: 0.3040239214897156\n",
      "     \"gas\" -- sim: 0.3032095432281494\n",
      "     \"garden\" -- sim: 0.30258598923683167\n",
      "     \"forest\" -- sim: 0.3024669289588928\n",
      "Most similar words to \"thank\":\n",
      "     \"additional\" -- sim: 0.39114516973495483\n",
      "     \"tell\" -- sim: 0.3910753130912781\n",
      "     \"wish\" -- sim: 0.3426245450973511\n",
      "     \"tomorrow\" -- sim: 0.33571404218673706\n",
      "     \"do\" -- sim: 0.331668883562088\n",
      "     \"worse\" -- sim: 0.3284463882446289\n",
      "     \"kerchief\" -- sim: 0.32839155197143555\n",
      "     \"whatsoever\" -- sim: 0.32102930545806885\n",
      "     \"steal\" -- sim: 0.31937819719314575\n",
      "     \"happen\" -- sim: 0.3187811076641083\n",
      "Most similar words to \"copyright\":\n",
      "     \"foundation\" -- sim: 0.46748730540275574\n",
      "     \"domain\" -- sim: 0.43490883708000183\n",
      "     \"state\" -- sim: 0.4134789705276489\n",
      "     \"using\" -- sim: 0.41308853030204773\n",
      "     \"creating\" -- sim: 0.40383797883987427\n",
      "     \"complying\" -- sim: 0.40142112970352173\n",
      "     \"united\" -- sim: 0.39696788787841797\n",
      "     \"associated\" -- sim: 0.3889354169368744\n",
      "     \"defect\" -- sim: 0.3837425708770752\n",
      "     \"included\" -- sim: 0.38160187005996704\n",
      "Most similar words to \"bow\":\n",
      "     \"heads\" -- sim: 0.33395421504974365\n",
      "     \"shoot\" -- sim: 0.3298701047897339\n",
      "     \"points\" -- sim: 0.32713979482650757\n",
      "     \"simple\" -- sim: 0.30413442850112915\n",
      "     \"rage\" -- sim: 0.302979975938797\n",
      "     \"spun\" -- sim: 0.3023831248283386\n",
      "     \"wheel\" -- sim: 0.30002617835998535\n",
      "     \"wren\" -- sim: 0.29890066385269165\n",
      "     \"prize\" -- sim: 0.29155927896499634\n",
      "     \"summoned\" -- sim: 0.28377461433410645\n",
      "Most similar words to \"splendid\":\n",
      "     \"beautiful\" -- sim: 0.4010152518749237\n",
      "     \"golden\" -- sim: 0.39832061529159546\n",
      "     \"jorindel\" -- sim: 0.36716991662979126\n",
      "     \"dozen\" -- sim: 0.33695676922798157\n",
      "     \"changed\" -- sim: 0.32308459281921387\n",
      "     \"c\" -- sim: 0.3228348195552826\n",
      "     \"rising\" -- sim: 0.3165491223335266\n",
      "     \"queer\" -- sim: 0.3146907091140747\n",
      "     \"setting\" -- sim: 0.308771014213562\n",
      "     \"incredible\" -- sim: 0.3033125400543213\n",
      "Most similar words to \"death\":\n",
      "     \"notice\" -- sim: 0.3867424726486206\n",
      "     \"impossible\" -- sim: 0.3253523111343384\n",
      "     \"pieces\" -- sim: 0.3193732798099518\n",
      "     \"slain\" -- sim: 0.3173217177391052\n",
      "     \"guest\" -- sim: 0.31407099962234497\n",
      "     \"authorities\" -- sim: 0.31282567977905273\n",
      "     \"martian\" -- sim: 0.3128174841403961\n",
      "     \"purpose\" -- sim: 0.31139931082725525\n",
      "     \"attention\" -- sim: 0.30748656392097473\n",
      "     \"angry\" -- sim: 0.3000345826148987\n",
      "Most similar words to \"dragging\":\n",
      "     \"striding\" -- sim: 0.41540446877479553\n",
      "     \"lazy\" -- sim: 0.2996731996536255\n",
      "     \"send\" -- sim: 0.2815021872520447\n",
      "     \"sweetheart\" -- sim: 0.2811984419822693\n",
      "     \"aged\" -- sim: 0.2753296196460724\n",
      "     \"proved\" -- sim: 0.27337709069252014\n",
      "     \"peering\" -- sim: 0.2698829770088196\n",
      "     \"curdken\" -- sim: 0.2622620463371277\n",
      "     \"into\" -- sim: 0.24397411942481995\n",
      "     \"pigsty\" -- sim: 0.23839735984802246\n",
      "Most similar words to \"seven\":\n",
      "     \"four\" -- sim: 0.3368488848209381\n",
      "     \"nuts\" -- sim: 0.32633012533187866\n",
      "     \"nine\" -- sim: 0.3082568049430847\n",
      "     \"saved\" -- sim: 0.3060036301612854\n",
      "     \"three\" -- sim: 0.30599135160446167\n",
      "     \"discharged\" -- sim: 0.2879657745361328\n",
      "     \"floor\" -- sim: 0.28107547760009766\n",
      "     \"earth\" -- sim: 0.27686798572540283\n",
      "     \"ago\" -- sim: 0.2713291645050049\n",
      "     \"attempt\" -- sim: 0.2706957161426544\n"
     ]
    }
   ],
   "source": [
    "#Step 9(b,c)\n",
    "\n",
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "model = Word2Vec(final_list, window=5, size=100, workers=4)\n",
    "model.train(final_list, total_examples=len(final_list), epochs=1000)\n",
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "# get vector size\n",
    "dim = model.vector_size\n",
    "#pick 10 random words from the dictionary\n",
    "idxs = random.sample(range(0, len(voc)), 10)\n",
    "rand_words = [voc[i] for i in idxs]\n",
    "for word in rand_words:\n",
    "    print(f'Most similar words to \"{word}\":')\n",
    "    for word,sim in model.wv.most_similar(word):\n",
    "        print(f'     \"{word}\" -- sim: {sim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 9***\n",
    "\n",
    "(γ)Τα αποτελέσματα του ερωτήματος 9(γ) δεν είναι τόσο ποιοτικά όσο θα περιμέναμε αφού πολλές από τις λέξεις δεν φαίνεται να έχουν πολύ μεγάλο similarity.\n",
    "\n",
    "Για να προσπαθήσουμε να βελτιώσουμε τα αποτελέσματα:\n",
    "\n",
    "(i)αυξήσουμε αρχικά το μέγεθος του παραθύρου context κρατώντας τον αριθμό εποχών σταθερό,\n",
    "\n",
    "(ii)αυξήσουμε τον αριθμό των εποχών κρατώντας τον αριθμό του παραθύρου σταθερό,\n",
    "\n",
    "(iii)αυξήσουμε και τις δύο τιμές.\n",
    "\n",
    "Παρακάτω παραθέτουμε τα αποτελέσματα του similarity 20 τυχαίων λέξεων για κάθε μία από τις τρεις περιπτώσεις καθώς και ένα σχολιασμό των αποτελεσμάτων.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9(c (i))\n",
    "\n",
    "def similarity(w,s,e):\n",
    "    # Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "    model = Word2Vec(final_list, window=w, size=s, workers=4)\n",
    "    model.train(final_list, total_examples=len(final_list), epochs=e)\n",
    "    # get ordered vocabulary list\n",
    "    voc = model.wv.index2word\n",
    "    # get vector size\n",
    "    dim = model.vector_size\n",
    "    #pick 10 random words from the dictionary\n",
    "    rand_words = [voc[i] for i in idxs]\n",
    "    for word in rand_words:\n",
    "        print(f'Most similar words to \"{word}\":')\n",
    "        for word,sim in model.wv.most_similar(word):\n",
    "            print(f'     \"{word}\" -- sim: {sim}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"bell\":\n",
      "     \"hunting\" -- sim: 0.32449445128440857\n",
      "     \"grow\" -- sim: 0.30320775508880615\n",
      "     \"boiling\" -- sim: 0.2744957506656647\n",
      "     \"curdken\" -- sim: 0.2740527093410492\n",
      "     \"john\" -- sim: 0.2679397761821747\n",
      "     \"chain\" -- sim: 0.26712465286254883\n",
      "     \"f\" -- sim: 0.2648935914039612\n",
      "     \"party\" -- sim: 0.26468372344970703\n",
      "     \"cast\" -- sim: 0.2637123167514801\n",
      "     \"cake\" -- sim: 0.26236748695373535\n",
      "Most similar words to \"silver\":\n",
      "     \"torn\" -- sim: 0.3431171178817749\n",
      "     \"miserable\" -- sim: 0.3217596709728241\n",
      "     \"gifts\" -- sim: 0.32025212049484253\n",
      "     \"train\" -- sim: 0.31301024556159973\n",
      "     \"dancing\" -- sim: 0.30746889114379883\n",
      "     \"race\" -- sim: 0.3057774305343628\n",
      "     \"easy\" -- sim: 0.2964330315589905\n",
      "     \"tore\" -- sim: 0.29458776116371155\n",
      "     \"rampion\" -- sim: 0.29034423828125\n",
      "     \"exactly\" -- sim: 0.2902930974960327\n",
      "Most similar words to \"window\":\n",
      "     \"fight\" -- sim: 0.345350444316864\n",
      "     \"house\" -- sim: 0.3443455696105957\n",
      "     \"wall\" -- sim: 0.3301577866077423\n",
      "     \"road\" -- sim: 0.3251321017742157\n",
      "     \"pebbles\" -- sim: 0.3242195248603821\n",
      "     \"door\" -- sim: 0.32421034574508667\n",
      "     \"arms\" -- sim: 0.31830814480781555\n",
      "     \"gates\" -- sim: 0.31352323293685913\n",
      "     \"coal\" -- sim: 0.30924928188323975\n",
      "     \"object\" -- sim: 0.30820232629776\n",
      "Most similar words to \"thank\":\n",
      "     \"worse\" -- sim: 0.3291521668434143\n",
      "     \"tell\" -- sim: 0.3167377710342407\n",
      "     \"dame\" -- sim: 0.30844730138778687\n",
      "     \"mankind\" -- sim: 0.3022269308567047\n",
      "     \"lies\" -- sim: 0.29113584756851196\n",
      "     \"tomorrow\" -- sim: 0.2861083149909973\n",
      "     \"invited\" -- sim: 0.28546908497810364\n",
      "     \"gretel\" -- sim: 0.2839014530181885\n",
      "     \"steal\" -- sim: 0.28151220083236694\n",
      "     \"screamed\" -- sim: 0.28083717823028564\n",
      "Most similar words to \"copyright\":\n",
      "     \"united\" -- sim: 0.4404956102371216\n",
      "     \"e\" -- sim: 0.3954172432422638\n",
      "     \"grimm\" -- sim: 0.3662761449813843\n",
      "     \"domain\" -- sim: 0.34802496433258057\n",
      "     \"foundation\" -- sim: 0.34383082389831543\n",
      "     \"complying\" -- sim: 0.33942174911499023\n",
      "     \"proud\" -- sim: 0.3345624804496765\n",
      "     \"official\" -- sim: 0.332510769367218\n",
      "     \"lot\" -- sim: 0.3308095335960388\n",
      "     \"user\" -- sim: 0.3303309977054596\n",
      "Most similar words to \"bow\":\n",
      "     \"wren\" -- sim: 0.3716387152671814\n",
      "     \"included\" -- sim: 0.356658011674881\n",
      "     \"dresses\" -- sim: 0.3240189552307129\n",
      "     \"dearest\" -- sim: 0.29395541548728943\n",
      "     \"load\" -- sim: 0.2929774224758148\n",
      "     \"permission\" -- sim: 0.28851354122161865\n",
      "     \"turnip\" -- sim: 0.28029733896255493\n",
      "     \"been\" -- sim: 0.27576810121536255\n",
      "     \"tune\" -- sim: 0.27554887533187866\n",
      "     \"misfortune\" -- sim: 0.2688415050506592\n",
      "Most similar words to \"splendid\":\n",
      "     \"beautiful\" -- sim: 0.38934603333473206\n",
      "     \"tasted\" -- sim: 0.30972081422805786\n",
      "     \"golden\" -- sim: 0.3068625032901764\n",
      "     \"shipping\" -- sim: 0.3039754629135132\n",
      "     \"accident\" -- sim: 0.30003970861434937\n",
      "     \"setting\" -- sim: 0.2890866994857788\n",
      "     \"thief\" -- sim: 0.2813575863838196\n",
      "     \"primrose\" -- sim: 0.2791675925254822\n",
      "     \"including\" -- sim: 0.2748066782951355\n",
      "     \"shore\" -- sim: 0.27259472012519836\n",
      "Most similar words to \"death\":\n",
      "     \"wonder\" -- sim: 0.3574742078781128\n",
      "     \"pieces\" -- sim: 0.35106539726257324\n",
      "     \"impossible\" -- sim: 0.3277992308139801\n",
      "     \"o\" -- sim: 0.31762680411338806\n",
      "     \"purpose\" -- sim: 0.28436070680618286\n",
      "     \"case\" -- sim: 0.28205129504203796\n",
      "     \"resumed\" -- sim: 0.27614203095436096\n",
      "     \"willing\" -- sim: 0.2755258083343506\n",
      "     \"notice\" -- sim: 0.2574743628501892\n",
      "     \"guns\" -- sim: 0.25695645809173584\n",
      "Most similar words to \"dragging\":\n",
      "     \"striding\" -- sim: 0.34715718030929565\n",
      "     \"likewise\" -- sim: 0.27286264300346375\n",
      "     \"proved\" -- sim: 0.26737117767333984\n",
      "     \"lazy\" -- sim: 0.2661733031272888\n",
      "     \"quite\" -- sim: 0.2639486789703369\n",
      "     \"beheld\" -- sim: 0.2562434673309326\n",
      "     \"halfway\" -- sim: 0.25567305088043213\n",
      "     \"no\" -- sim: 0.25319668650627136\n",
      "     \"passing\" -- sim: 0.24964570999145508\n",
      "     \"force\" -- sim: 0.24520254135131836\n",
      "Most similar words to \"seven\":\n",
      "     \"nine\" -- sim: 0.3106468915939331\n",
      "     \"three\" -- sim: 0.29650700092315674\n",
      "     \"few\" -- sim: 0.28336143493652344\n",
      "     \"five\" -- sim: 0.28179246187210083\n",
      "     \"floor\" -- sim: 0.2810235321521759\n",
      "     \"length\" -- sim: 0.2808348536491394\n",
      "     \"looking\" -- sim: 0.2795862555503845\n",
      "     \"breezes\" -- sim: 0.2788514494895935\n",
      "     \"card\" -- sim: 0.2780665457248688\n",
      "     \"opening\" -- sim: 0.2764633297920227\n"
     ]
    }
   ],
   "source": [
    "similarity(10,100,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"bell\":\n",
      "     \"hunting\" -- sim: 0.3429935574531555\n",
      "     \"less\" -- sim: 0.3279402256011963\n",
      "     \"disclaimer\" -- sim: 0.32247552275657654\n",
      "     \"dawned\" -- sim: 0.30565589666366577\n",
      "     \"cast\" -- sim: 0.2974476218223572\n",
      "     \"mill\" -- sim: 0.29500746726989746\n",
      "     \"tan\" -- sim: 0.2875020503997803\n",
      "     \"this\" -- sim: 0.2817056179046631\n",
      "     \"drank\" -- sim: 0.2732568085193634\n",
      "     \"gently\" -- sim: 0.2725380063056946\n",
      "Most similar words to \"silver\":\n",
      "     \"miserable\" -- sim: 0.35244911909103394\n",
      "     \"tore\" -- sim: 0.31512022018432617\n",
      "     \"steal\" -- sim: 0.3009749948978424\n",
      "     \"lamp\" -- sim: 0.29763495922088623\n",
      "     \"train\" -- sim: 0.2955177128314972\n",
      "     \"easy\" -- sim: 0.2939110994338989\n",
      "     \"full\" -- sim: 0.2925071716308594\n",
      "     \"courage\" -- sim: 0.2877197861671448\n",
      "     \"liability\" -- sim: 0.28388547897338867\n",
      "     \"blackness\" -- sim: 0.28297221660614014\n",
      "Most similar words to \"window\":\n",
      "     \"western\" -- sim: 0.3311654329299927\n",
      "     \"gate\" -- sim: 0.32796308398246765\n",
      "     \"fight\" -- sim: 0.323650598526001\n",
      "     \"evidently\" -- sim: 0.32337504625320435\n",
      "     \"misfortune\" -- sim: 0.31650039553642273\n",
      "     \"gates\" -- sim: 0.31532394886016846\n",
      "     \"coal\" -- sim: 0.3103228211402893\n",
      "     \"tailor\" -- sim: 0.30743905901908875\n",
      "     \"door\" -- sim: 0.3063105344772339\n",
      "     \"driver\" -- sim: 0.30608415603637695\n",
      "Most similar words to \"thank\":\n",
      "     \"worse\" -- sim: 0.3455258011817932\n",
      "     \"terrified\" -- sim: 0.3395785391330719\n",
      "     \"shake\" -- sim: 0.3296877443790436\n",
      "     \"watched\" -- sim: 0.32260483503341675\n",
      "     \"kill\" -- sim: 0.3071005642414093\n",
      "     \"thumb\" -- sim: 0.30508142709732056\n",
      "     \"yards\" -- sim: 0.304368257522583\n",
      "     \"asked\" -- sim: 0.2967797815799713\n",
      "     \"invited\" -- sim: 0.296440452337265\n",
      "     \"store\" -- sim: 0.29516512155532837\n",
      "Most similar words to \"copyright\":\n",
      "     \"grimm\" -- sim: 0.3771515488624573\n",
      "     \"derivative\" -- sim: 0.3685777187347412\n",
      "     \"united\" -- sim: 0.36767101287841797\n",
      "     \"electronic\" -- sim: 0.35008448362350464\n",
      "     \"e\" -- sim: 0.3366979956626892\n",
      "     \"soul\" -- sim: 0.33073437213897705\n",
      "     \"favour\" -- sim: 0.33063602447509766\n",
      "     \"associated\" -- sim: 0.32867226004600525\n",
      "     \"lot\" -- sim: 0.3273012042045593\n",
      "     \"complying\" -- sim: 0.325133740901947\n",
      "Most similar words to \"bow\":\n",
      "     \"general\" -- sim: 0.31275075674057007\n",
      "     \"dresses\" -- sim: 0.30330994725227356\n",
      "     \"cord\" -- sim: 0.28771159052848816\n",
      "     \"barrel\" -- sim: 0.2874527871608734\n",
      "     \"clambered\" -- sim: 0.28742629289627075\n",
      "     \"wren\" -- sim: 0.28574901819229126\n",
      "     \"start\" -- sim: 0.2798890173435211\n",
      "     \"format\" -- sim: 0.27425888180732727\n",
      "     \"concussion\" -- sim: 0.272775262594223\n",
      "     \"quarters\" -- sim: 0.27040284872055054\n",
      "Most similar words to \"splendid\":\n",
      "     \"judged\" -- sim: 0.3075942099094391\n",
      "     \"beautiful\" -- sim: 0.30681896209716797\n",
      "     \"shore\" -- sim: 0.293620228767395\n",
      "     \"whole\" -- sim: 0.29031211137771606\n",
      "     \"rampion\" -- sim: 0.2898063659667969\n",
      "     \"sprang\" -- sim: 0.2895924746990204\n",
      "     \"quarter\" -- sim: 0.2878200113773346\n",
      "     \"simple\" -- sim: 0.28625041246414185\n",
      "     \"main\" -- sim: 0.2838241457939148\n",
      "     \"including\" -- sim: 0.2808419466018677\n",
      "Most similar words to \"death\":\n",
      "     \"purpose\" -- sim: 0.3678639233112335\n",
      "     \"wonder\" -- sim: 0.2999487817287445\n",
      "     \"pieces\" -- sim: 0.29393818974494934\n",
      "     \"journey\" -- sim: 0.2926340103149414\n",
      "     \"resumed\" -- sim: 0.2918499708175659\n",
      "     \"everything\" -- sim: 0.2902819812297821\n",
      "     \"mist\" -- sim: 0.2801641821861267\n",
      "     \"gift\" -- sim: 0.2771993577480316\n",
      "     \"frog\" -- sim: 0.2751310467720032\n",
      "     \"fate\" -- sim: 0.273884117603302\n",
      "Most similar words to \"dragging\":\n",
      "     \"striding\" -- sim: 0.35275232791900635\n",
      "     \"likewise\" -- sim: 0.33015865087509155\n",
      "     \"slipped\" -- sim: 0.29084259271621704\n",
      "     \"instant\" -- sim: 0.2852481007575989\n",
      "     \"lazy\" -- sim: 0.27592700719833374\n",
      "     \"no\" -- sim: 0.27261489629745483\n",
      "     \"little\" -- sim: 0.2675169110298157\n",
      "     \"bound\" -- sim: 0.26414671540260315\n",
      "     \"wild\" -- sim: 0.2629639506340027\n",
      "     \"surely\" -- sim: 0.26231589913368225\n",
      "Most similar words to \"seven\":\n",
      "     \"striding\" -- sim: 0.33395957946777344\n",
      "     \"vain\" -- sim: 0.3280385732650757\n",
      "     \"three\" -- sim: 0.32418787479400635\n",
      "     \"many\" -- sim: 0.3127896189689636\n",
      "     \"thick\" -- sim: 0.30273425579071045\n",
      "     \"displaying\" -- sim: 0.30183467268943787\n",
      "     \"lay\" -- sim: 0.28900811076164246\n",
      "     \"breezes\" -- sim: 0.28658151626586914\n",
      "     \"floor\" -- sim: 0.28532686829566956\n",
      "     \"opening\" -- sim: 0.2842869758605957\n"
     ]
    }
   ],
   "source": [
    "similarity(15,100,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"bell\":\n",
      "     \"planted\" -- sim: 0.3346071243286133\n",
      "     \"waiting\" -- sim: 0.29823920130729675\n",
      "     \"party\" -- sim: 0.29563140869140625\n",
      "     \"drop\" -- sim: 0.2951899766921997\n",
      "     \"support\" -- sim: 0.29239869117736816\n",
      "     \"curdken\" -- sim: 0.28694701194763184\n",
      "     \"everyone\" -- sim: 0.28688186407089233\n",
      "     \"breakfast\" -- sim: 0.2795769274234772\n",
      "     \"steak\" -- sim: 0.2774592339992523\n",
      "     \"dawned\" -- sim: 0.27737024426460266\n",
      "Most similar words to \"silver\":\n",
      "     \"smart\" -- sim: 0.32704150676727295\n",
      "     \"sense\" -- sim: 0.3109772205352783\n",
      "     \"treasure\" -- sim: 0.30615323781967163\n",
      "     \"your\" -- sim: 0.30246713757514954\n",
      "     \"steak\" -- sim: 0.29568785429000854\n",
      "     \"buttons\" -- sim: 0.2930767238140106\n",
      "     \"locks\" -- sim: 0.28750860691070557\n",
      "     \"yard\" -- sim: 0.2864888310432434\n",
      "     \"incredible\" -- sim: 0.2850775718688965\n",
      "     \"torn\" -- sim: 0.28429627418518066\n",
      "Most similar words to \"window\":\n",
      "     \"house\" -- sim: 0.4080429971218109\n",
      "     \"gates\" -- sim: 0.36660557985305786\n",
      "     \"door\" -- sim: 0.35188916325569153\n",
      "     \"gate\" -- sim: 0.3399267792701721\n",
      "     \"circle\" -- sim: 0.3367270529270172\n",
      "     \"room\" -- sim: 0.3339301645755768\n",
      "     \"news\" -- sim: 0.3269195556640625\n",
      "     \"object\" -- sim: 0.32338809967041016\n",
      "     \"evidently\" -- sim: 0.30536702275276184\n",
      "     \"coal\" -- sim: 0.2996499538421631\n",
      "Most similar words to \"thank\":\n",
      "     \"worse\" -- sim: 0.38240036368370056\n",
      "     \"do\" -- sim: 0.3618098199367523\n",
      "     \"additional\" -- sim: 0.348079115152359\n",
      "     \"tomorrow\" -- sim: 0.3379906117916107\n",
      "     \"unless\" -- sim: 0.33697691559791565\n",
      "     \"kerchief\" -- sim: 0.3354807496070862\n",
      "     \"cost\" -- sim: 0.33174413442611694\n",
      "     \"tell\" -- sim: 0.3310144543647766\n",
      "     \"kindness\" -- sim: 0.32725825905799866\n",
      "     \"does\" -- sim: 0.31929683685302734\n",
      "Most similar words to \"copyright\":\n",
      "     \"foundation\" -- sim: 0.4326537549495697\n",
      "     \"state\" -- sim: 0.42840734124183655\n",
      "     \"included\" -- sim: 0.38246041536331177\n",
      "     \"domain\" -- sim: 0.37425315380096436\n",
      "     \"united\" -- sim: 0.3703467845916748\n",
      "     \"tm\" -- sim: 0.366292268037796\n",
      "     \"terms\" -- sim: 0.3648574948310852\n",
      "     \"complying\" -- sim: 0.3647604286670685\n",
      "     \"writing\" -- sim: 0.3606833815574646\n",
      "     \"defect\" -- sim: 0.3586543798446655\n",
      "Most similar words to \"bow\":\n",
      "     \"wren\" -- sim: 0.3379477858543396\n",
      "     \"chain\" -- sim: 0.3357863128185272\n",
      "     \"heads\" -- sim: 0.32833370566368103\n",
      "     \"points\" -- sim: 0.3063388466835022\n",
      "     \"prize\" -- sim: 0.30563870072364807\n",
      "     \"wheel\" -- sim: 0.3007568418979645\n",
      "     \"heart\" -- sim: 0.2957151234149933\n",
      "     \"load\" -- sim: 0.29117828607559204\n",
      "     \"sing\" -- sim: 0.28682368993759155\n",
      "     \"turnip\" -- sim: 0.28476542234420776\n",
      "Most similar words to \"splendid\":\n",
      "     \"familiar\" -- sim: 0.3862461447715759\n",
      "     \"beautiful\" -- sim: 0.33914047479629517\n",
      "     \"setting\" -- sim: 0.3207703232765198\n",
      "     \"clearly\" -- sim: 0.31212639808654785\n",
      "     \"jorindel\" -- sim: 0.3072367310523987\n",
      "     \"golden\" -- sim: 0.3042844533920288\n",
      "     \"whirl\" -- sim: 0.2982524037361145\n",
      "     \"song\" -- sim: 0.29726749658584595\n",
      "     \"incredible\" -- sim: 0.2909998595714569\n",
      "     \"jorinda\" -- sim: 0.28467822074890137\n",
      "Most similar words to \"death\":\n",
      "     \"notice\" -- sim: 0.36114537715911865\n",
      "     \"purpose\" -- sim: 0.33892497420310974\n",
      "     \"beginning\" -- sim: 0.3350093364715576\n",
      "     \"impossible\" -- sim: 0.3304945230484009\n",
      "     \"energy\" -- sim: 0.3077414631843567\n",
      "     \"guest\" -- sim: 0.3026273250579834\n",
      "     \"pieces\" -- sim: 0.2990110218524933\n",
      "     \"face\" -- sim: 0.2794652581214905\n",
      "     \"die\" -- sim: 0.2792832553386688\n",
      "     \"martian\" -- sim: 0.2751954197883606\n",
      "Most similar words to \"dragging\":\n",
      "     \"striding\" -- sim: 0.31783202290534973\n",
      "     \"send\" -- sim: 0.3053951561450958\n",
      "     \"lazy\" -- sim: 0.3017961382865906\n",
      "     \"curdken\" -- sim: 0.2738468647003174\n",
      "     \"proved\" -- sim: 0.2613256573677063\n",
      "     \"glass\" -- sim: 0.2444239854812622\n",
      "     \"pushing\" -- sim: 0.24174532294273376\n",
      "     \"sexton\" -- sim: 0.24135150015354156\n",
      "     \"left\" -- sim: 0.2341112494468689\n",
      "     \"returned\" -- sim: 0.23246118426322937\n",
      "Most similar words to \"seven\":\n",
      "     \"three\" -- sim: 0.35593241453170776\n",
      "     \"four\" -- sim: 0.3251788914203644\n",
      "     \"nuts\" -- sim: 0.30396580696105957\n",
      "     \"thieves\" -- sim: 0.29481297731399536\n",
      "     \"countless\" -- sim: 0.2885582447052002\n",
      "     \"nine\" -- sim: 0.28811076283454895\n",
      "     \"twelve\" -- sim: 0.2852471172809601\n",
      "     \"floor\" -- sim: 0.28377678990364075\n",
      "     \"looking\" -- sim: 0.2749313712120056\n",
      "     \"standing\" -- sim: 0.2684139609336853\n"
     ]
    }
   ],
   "source": [
    "#Step 9(c (ii))\n",
    "\n",
    "similarity(5,100,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"bell\":\n",
      "     \"party\" -- sim: 0.33197125792503357\n",
      "     \"dawned\" -- sim: 0.32194608449935913\n",
      "     \"f\" -- sim: 0.31368133425712585\n",
      "     \"curdken\" -- sim: 0.30459749698638916\n",
      "     \"cry\" -- sim: 0.30387431383132935\n",
      "     \"drop\" -- sim: 0.2950055003166199\n",
      "     \"women\" -- sim: 0.29164060950279236\n",
      "     \"stable\" -- sim: 0.2907787561416626\n",
      "     \"talers\" -- sim: 0.2869738042354584\n",
      "     \"planted\" -- sim: 0.2802666425704956\n",
      "Most similar words to \"silver\":\n",
      "     \"sense\" -- sim: 0.4013127088546753\n",
      "     \"torn\" -- sim: 0.3047332167625427\n",
      "     \"jewels\" -- sim: 0.3018363416194916\n",
      "     \"dancing\" -- sim: 0.30131450295448303\n",
      "     \"imagination\" -- sim: 0.29833486676216125\n",
      "     \"nice\" -- sim: 0.29233577847480774\n",
      "     \"bulk\" -- sim: 0.2922404408454895\n",
      "     \"lamp\" -- sim: 0.28793588280677795\n",
      "     \"sun\" -- sim: 0.28562644124031067\n",
      "     \"smart\" -- sim: 0.2854771018028259\n",
      "Most similar words to \"window\":\n",
      "     \"house\" -- sim: 0.4288135766983032\n",
      "     \"room\" -- sim: 0.38202357292175293\n",
      "     \"door\" -- sim: 0.3384537398815155\n",
      "     \"gates\" -- sim: 0.3371467888355255\n",
      "     \"drain\" -- sim: 0.32348620891571045\n",
      "     \"chaise\" -- sim: 0.3207918703556061\n",
      "     \"lamp\" -- sim: 0.31859612464904785\n",
      "     \"circle\" -- sim: 0.313333123922348\n",
      "     \"evidently\" -- sim: 0.29884567856788635\n",
      "     \"street\" -- sim: 0.2971826195716858\n",
      "Most similar words to \"thank\":\n",
      "     \"additional\" -- sim: 0.373271107673645\n",
      "     \"tell\" -- sim: 0.3519635796546936\n",
      "     \"cost\" -- sim: 0.3385348618030548\n",
      "     \"kindness\" -- sim: 0.33205699920654297\n",
      "     \"tomorrow\" -- sim: 0.3298366665840149\n",
      "     \"do\" -- sim: 0.3276771903038025\n",
      "     \"does\" -- sim: 0.3268163800239563\n",
      "     \"kerchief\" -- sim: 0.3167853355407715\n",
      "     \"whatsoever\" -- sim: 0.31336095929145813\n",
      "     \"overtake\" -- sim: 0.3041332960128784\n",
      "Most similar words to \"copyright\":\n",
      "     \"state\" -- sim: 0.4719471335411072\n",
      "     \"foundation\" -- sim: 0.3910365700721741\n",
      "     \"complying\" -- sim: 0.3644167184829712\n",
      "     \"terms\" -- sim: 0.36381417512893677\n",
      "     \"defect\" -- sim: 0.36091184616088867\n",
      "     \"domain\" -- sim: 0.35729801654815674\n",
      "     \"united\" -- sim: 0.3561286926269531\n",
      "     \"using\" -- sim: 0.3527420163154602\n",
      "     \"tm\" -- sim: 0.3493290841579437\n",
      "     \"necessity\" -- sim: 0.3473963141441345\n",
      "Most similar words to \"bow\":\n",
      "     \"wren\" -- sim: 0.3248600363731384\n",
      "     \"turnip\" -- sim: 0.30571210384368896\n",
      "     \"load\" -- sim: 0.29691052436828613\n",
      "     \"joy\" -- sim: 0.29482540488243103\n",
      "     \"chain\" -- sim: 0.29470914602279663\n",
      "     \"shoot\" -- sim: 0.2887517809867859\n",
      "     \"permission\" -- sim: 0.2822754681110382\n",
      "     \"stomach\" -- sim: 0.27816838026046753\n",
      "     \"enter\" -- sim: 0.27690380811691284\n",
      "     \"start\" -- sim: 0.2758226990699768\n",
      "Most similar words to \"splendid\":\n",
      "     \"beautiful\" -- sim: 0.4149046838283539\n",
      "     \"golden\" -- sim: 0.3391544818878174\n",
      "     \"jorindel\" -- sim: 0.3270842730998993\n",
      "     \"familiar\" -- sim: 0.32170888781547546\n",
      "     \"queer\" -- sim: 0.31100475788116455\n",
      "     \"whirl\" -- sim: 0.29970240592956543\n",
      "     \"jet\" -- sim: 0.29730352759361267\n",
      "     \"changed\" -- sim: 0.2936178147792816\n",
      "     \"older\" -- sim: 0.2865578532218933\n",
      "     \"rising\" -- sim: 0.2853854298591614\n",
      "Most similar words to \"death\":\n",
      "     \"pieces\" -- sim: 0.3884814381599426\n",
      "     \"notice\" -- sim: 0.3503526449203491\n",
      "     \"purpose\" -- sim: 0.33953166007995605\n",
      "     \"within\" -- sim: 0.32515034079551697\n",
      "     \"immediate\" -- sim: 0.29964062571525574\n",
      "     \"save\" -- sim: 0.2932485342025757\n",
      "     \"guest\" -- sim: 0.29085519909858704\n",
      "     \"sick\" -- sim: 0.27524662017822266\n",
      "     \"impossible\" -- sim: 0.2736234962940216\n",
      "     \"beginning\" -- sim: 0.2679639756679535\n",
      "Most similar words to \"dragging\":\n",
      "     \"striding\" -- sim: 0.3738609850406647\n",
      "     \"send\" -- sim: 0.33991628885269165\n",
      "     \"proved\" -- sim: 0.32265201210975647\n",
      "     \"lazy\" -- sim: 0.290715754032135\n",
      "     \"servant\" -- sim: 0.28457003831863403\n",
      "     \"trunk\" -- sim: 0.2803387939929962\n",
      "     \"peering\" -- sim: 0.2736127972602844\n",
      "     \"especially\" -- sim: 0.256462037563324\n",
      "     \"shining\" -- sim: 0.24909517168998718\n",
      "     \"king\" -- sim: 0.2476641833782196\n",
      "Most similar words to \"seven\":\n",
      "     \"three\" -- sim: 0.3524351119995117\n",
      "     \"nine\" -- sim: 0.31893041729927063\n",
      "     \"five\" -- sim: 0.3157254457473755\n",
      "     \"thieves\" -- sim: 0.2941713333129883\n",
      "     \"talked\" -- sim: 0.293901264667511\n",
      "     \"four\" -- sim: 0.2871842384338379\n",
      "     \"project\" -- sim: 0.28590816259384155\n",
      "     \"nuts\" -- sim: 0.2798556387424469\n",
      "     \"nice\" -- sim: 0.27597057819366455\n",
      "     \"looking\" -- sim: 0.2712301015853882\n"
     ]
    }
   ],
   "source": [
    "similarity(5,100,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"bell\":\n",
      "     \"hunting\" -- sim: 0.3386894762516022\n",
      "     \"immediate\" -- sim: 0.31322407722473145\n",
      "     \"dawned\" -- sim: 0.2904212474822998\n",
      "     \"disclaimer\" -- sim: 0.2821723222732544\n",
      "     \"happening\" -- sim: 0.27011704444885254\n",
      "     \"f\" -- sim: 0.26597440242767334\n",
      "     \"grow\" -- sim: 0.264279305934906\n",
      "     \"party\" -- sim: 0.2610413134098053\n",
      "     \"mr\" -- sim: 0.2588898539543152\n",
      "     \"ringing\" -- sim: 0.25521984696388245\n",
      "Most similar words to \"silver\":\n",
      "     \"miserable\" -- sim: 0.37720009684562683\n",
      "     \"ear\" -- sim: 0.33412835001945496\n",
      "     \"treasure\" -- sim: 0.3237326741218567\n",
      "     \"flock\" -- sim: 0.3183053731918335\n",
      "     \"advance\" -- sim: 0.3124491572380066\n",
      "     \"howling\" -- sim: 0.3085130751132965\n",
      "     \"easy\" -- sim: 0.3064780831336975\n",
      "     \"whatsoever\" -- sim: 0.3033634424209595\n",
      "     \"train\" -- sim: 0.2900930643081665\n",
      "     \"sense\" -- sim: 0.28422147035598755\n",
      "Most similar words to \"window\":\n",
      "     \"house\" -- sim: 0.39663082361221313\n",
      "     \"evidently\" -- sim: 0.3287639617919922\n",
      "     \"gates\" -- sim: 0.32823213934898376\n",
      "     \"fight\" -- sim: 0.32584595680236816\n",
      "     \"folks\" -- sim: 0.3182070255279541\n",
      "     \"wall\" -- sim: 0.315400630235672\n",
      "     \"flames\" -- sim: 0.3142639398574829\n",
      "     \"pebbles\" -- sim: 0.31387102603912354\n",
      "     \"doves\" -- sim: 0.3117634654045105\n",
      "     \"alarmed\" -- sim: 0.3111545741558075\n",
      "Most similar words to \"thank\":\n",
      "     \"lies\" -- sim: 0.38467174768447876\n",
      "     \"wants\" -- sim: 0.35380274057388306\n",
      "     \"worse\" -- sim: 0.33031654357910156\n",
      "     \"dame\" -- sim: 0.32997506856918335\n",
      "     \"tell\" -- sim: 0.3299087882041931\n",
      "     \"bedroom\" -- sim: 0.3178217113018036\n",
      "     \"lead\" -- sim: 0.31689453125\n",
      "     \"do\" -- sim: 0.30729445815086365\n",
      "     \"send\" -- sim: 0.3028296232223511\n",
      "     \"knock\" -- sim: 0.2996225655078888\n",
      "Most similar words to \"copyright\":\n",
      "     \"e\" -- sim: 0.39786335825920105\n",
      "     \"united\" -- sim: 0.38577738404273987\n",
      "     \"grimm\" -- sim: 0.3806186020374298\n",
      "     \"distribute\" -- sim: 0.3599703311920166\n",
      "     \"domain\" -- sim: 0.34511032700538635\n",
      "     \"freely\" -- sim: 0.343134343624115\n",
      "     \"derivative\" -- sim: 0.34222787618637085\n",
      "     \"complying\" -- sim: 0.33558380603790283\n",
      "     \"electronic\" -- sim: 0.33515745401382446\n",
      "     \"ebook\" -- sim: 0.32613930106163025\n",
      "Most similar words to \"bow\":\n",
      "     \"wren\" -- sim: 0.3426697850227356\n",
      "     \"barrel\" -- sim: 0.3293496370315552\n",
      "     \"branches\" -- sim: 0.31771570444107056\n",
      "     \"worked\" -- sim: 0.31527191400527954\n",
      "     \"turnip\" -- sim: 0.3049994707107544\n",
      "     \"permission\" -- sim: 0.29921936988830566\n",
      "     \"summoned\" -- sim: 0.2877844572067261\n",
      "     \"general\" -- sim: 0.28544217348098755\n",
      "     \"dresses\" -- sim: 0.2794944941997528\n",
      "     \"quarters\" -- sim: 0.2707768678665161\n",
      "Most similar words to \"splendid\":\n",
      "     \"beautiful\" -- sim: 0.3678961396217346\n",
      "     \"tasted\" -- sim: 0.33179348707199097\n",
      "     \"curdken\" -- sim: 0.32323911786079407\n",
      "     \"shore\" -- sim: 0.29423075914382935\n",
      "     \"jorinda\" -- sim: 0.29109030961990356\n",
      "     \"shaped\" -- sim: 0.29022300243377686\n",
      "     \"site\" -- sim: 0.28888338804244995\n",
      "     \"setting\" -- sim: 0.2882590889930725\n",
      "     \"juniper\" -- sim: 0.2867860794067383\n",
      "     \"bare\" -- sim: 0.2798224687576294\n",
      "Most similar words to \"death\":\n",
      "     \"pieces\" -- sim: 0.3832632899284363\n",
      "     \"purpose\" -- sim: 0.3298809230327606\n",
      "     \"clothes\" -- sim: 0.32935208082199097\n",
      "     \"happen\" -- sim: 0.32739222049713135\n",
      "     \"probably\" -- sim: 0.32259389758110046\n",
      "     \"notice\" -- sim: 0.2954379916191101\n",
      "     \"impossible\" -- sim: 0.29080331325531006\n",
      "     \"sick\" -- sim: 0.2875380218029022\n",
      "     \"slain\" -- sim: 0.2863563597202301\n",
      "     \"wonder\" -- sim: 0.2807299494743347\n",
      "Most similar words to \"dragging\":\n",
      "     \"striding\" -- sim: 0.40368586778640747\n",
      "     \"little\" -- sim: 0.27769580483436584\n",
      "     \"likewise\" -- sim: 0.2732122242450714\n",
      "     \"printed\" -- sim: 0.25885260105133057\n",
      "     \"no\" -- sim: 0.25814008712768555\n",
      "     \"younger\" -- sim: 0.2550694942474365\n",
      "     \"beheld\" -- sim: 0.2535722553730011\n",
      "     \"passing\" -- sim: 0.25068429112434387\n",
      "     \"hardly\" -- sim: 0.24769321084022522\n",
      "     \"send\" -- sim: 0.2427583932876587\n",
      "Most similar words to \"seven\":\n",
      "     \"displaying\" -- sim: 0.3368123769760132\n",
      "     \"vain\" -- sim: 0.30971071124076843\n",
      "     \"floor\" -- sim: 0.3090997040271759\n",
      "     \"many\" -- sim: 0.3080742061138153\n",
      "     \"armour\" -- sim: 0.302297979593277\n",
      "     \"breezes\" -- sim: 0.29415205121040344\n",
      "     \"stayed\" -- sim: 0.2930357754230499\n",
      "     \"joy\" -- sim: 0.28899911046028137\n",
      "     \"three\" -- sim: 0.28602224588394165\n",
      "     \"opening\" -- sim: 0.2849712073802948\n"
     ]
    }
   ],
   "source": [
    "#Step 9(c (iii))\n",
    "\n",
    "similarity(10,100,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 9***\n",
    "\n",
    "(γ)\n",
    "(i) Γνωρίζουμε ότι όταν το μοντέλο μας πραγματοποιεί το training με μέγεθος παραθύρου ίσο με 3 τότε βρίσκουμε λέξεις που έχουν παρόμοια σύνταξη ενώ με μεγάλο παράθυρο βρίσκουμε λέξεις με παρόμοια σημασιολογία.  \n",
    "Κάνοντας training λοιπόν το μοντέλο μας με παράθυρο ίσο με 10 και 15 θα έπρεπε να παρατηρήσουμε βελτίωση των αποτελεσμάτων για κάθε λέξη. Κάτι τέτοιο όμως δεν συμβαίνει. Υποθέτουμε ότι αυτό οφείλεται στο μικρού μεγέθους dataset στο οποίο γίνεται train το μοντέλο μας. \n",
    "\n",
    "(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
