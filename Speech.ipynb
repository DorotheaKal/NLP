{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Επεξεργασία Φωνής και Φυσικής Γλώσσας</h1> \n",
    "<h2 align = \"center\">1η Εργασία</h2> \n",
    "<h3 align = \"center\"> Θεoδωρόπουλος Νικήτας -03115185</h3>\n",
    "<h3 align = \"center\"> Καλλιώρα Δωροθέα - 03115176</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Σκοπός \n",
    "\n",
    "Στόχος της εργαστηριακής άσκησης είναι η δημιουργία ενός απλού ορθογράφου με χρήση μηχανών πεπερασμένων καταστάσεων (**fst**) με τη βοήθεια της βιβλιοθήκης openfst (v1.6.1). Για την εκπαίδευση του μοντέλου χρησιμοποιούμε corpus απο δημόσια διαθέσιμα βιβλία απο τα οποία με κατάλληλο tokenization γίνεται εξαγωγή λέξεων και σχηματισμός λεξικού. Για κάθε λέξη προς διόρθωση υπολογίζουμε την απόσταση Levenshtein πάνω στο λεξικό, η λέξη με την ελάχιστη απόσταση είναι η πρόβλεψη του μοντέλο μας.\n",
    "\n",
    "Τέλος θα γίνει εισαγωγή σε αναπαραστάσεις **word2vec**. Ενα σύνολο μοντέλων που χρησιμοποιούνται για παραγωγή αναπαραστάσεων λέξεων (embeddigns) σε έναν d-διάσταστο διανυσματικό χώρο $\\mathbb{R}^d$, έτσι ώστε λέξεις με κοντινή σημασία να βρίσκονται κοντά και στον διανυσματικό χώρο. Η βασική υπόθεση είναι ότι λέξεις με κοινή κατανομή στο κείμενο θα έχουν και κοινή σημασία. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 1\n",
    "\n",
    "Κατασκευάζουμε το corpus με σύμπτηξη plain text βιβλίων, δημόσια διαθέσιμα στο Project Gutenberg. Θα χρησιμοποιήσουμε corpus των δυο παρακάτω βιβλίων:  \n",
    "<br>\n",
    "<div class=\"image123\">\n",
    "    <div class=\"imgContainer\"  Style = \"float:left\">\n",
    "        <p>The War of the Worlds by H. G. Wells</p>\n",
    "        <img src=\"./img/book36.jpg\" height=auto width=\"250\"/>\n",
    "    </div>\n",
    "    <div class=\"imgContainer\" Style = \"float:right\">\n",
    "        <p>Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm</p>\n",
    "        <img class=\"middle-img\" src=\"./img/book2591.jpg\"/ height=auto width=\"250\"/>\n",
    "    </div>\n",
    "    <div class=\"imgContainer\" Style = \"float:middle\">\n",
    "        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "            Pride and Prejudice by Jane Austen </p>\n",
    "        <img src=\"./img/book1342.jpg\"/ height=auto width=\"250\" align=\"center\"/>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(β) Κατα την κατασκευή γλωσσικών μοντέλων ειναι κοινή πρακτική η σύμπτηξη πολλών βιβλίων για την δημιουργία ενός ενιαίου corpus προς επεξεργασία. Προφανώς για οποιοδήποτε μοντέλο η αύξηση των δεδομένων εκπαίδευσης οδηγεί σε μεγαλύτερη ικανοτητα γενίκευσης. \n",
    "\n",
    "* Για το μοντέλο μας η επιλογή μιας μόνο πηγής δεδομένων εισάγει μεγάλη προκατάληψη (bias) καθώς περιοριζόμαστε στο λεξιλόγιο ενός μόνο συγγραφέα μιας συγκεκριμένης εποχής ή και του context του βιβλίου (όπως η κοινωνική τάξη των χαρακτήρων αν είναι αφηγηματικό). Για να μπορεί να διορθώσει σωστά μια λέξη το μοντέλο μας θα πρέπει πρώτα να την γνωρίζει, συνεπώς η ποικιλία στο λεξιλιλόγιο είναι ενας καθοριστικός παράγωντας για την επιτυχία του μοντέλου. \n",
    "\n",
    "* Σε αναπαραστάσεις που βασίζονται στα συμφραζόμενα, όπως το word2vec, απαιτείται μεγάλο πλήθος δεδομένων έτσι ώστε να προσδιοριστεί σωστά η σημασία μιας λέξης. Αυτο συμβαίνει γιατί πρέπει να αναλυθεί η χρήση της και να αναγνωριστεί η θέση της σε διαφορετικά γλωσσικά περιβάλλοντα και ισχύει ακόμα και για σταθερό λεξιλόγιο. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for step 1\n",
    "\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/36/36-0.txt\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/2591/2591-0.txt\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/1342/1342-0.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2 \n",
    "\n",
    "Για την προεπεξεργασία του αρχέιου εισόδου υλοποιούμε κατάλληλο tokenizer ο οποίος αγνοεί τα σημεία στίξης, τους αριθμούς και οποιουσδήποτε άλλους μη λεκτικούς χαρακτήρες. Διαβάζουμε το αρχείο γραμμή προς γραμμή εφαρμόζοντας την συνάρτηση και προκύπτει μια λίστα απο lowercase λέξεις."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OqT0WsSYSLb"
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "def identity_preprocess(str):\n",
    "  return str\n",
    "\n",
    "def readfile(path,preprocess=identity_preprocess):\n",
    "  processed_txt=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    processed_txt = processed_txt + preprocess(line)\n",
    "  return processed_txt \n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.strip().lower()\n",
    "    s = ''.join(c if c.isalpha() else ' ' for c in s)\n",
    "    # We replace all non-alpha characters with ' '\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(δ)\n",
    "Ο tokenizer που υλοποιούμε ειναι πολυ απλός και αναγνωρίζει ως tokens μόνο τις απλές λέξεις. Επίσης δεν έχουμε λάβει υπόψην μας ιδιαίτερα γραμματικά φαινόμενα όπως ή σύντμηση ( \"did not $\\rightarrow$ didn't\"). Στην βιλιοθήκη nltk υπάρχουν αρκετά πιο ακριβείς και εκλεπτυσμένοι tokenizers. Παρασουσιάζουμε ενδεικτικά παρακάτω τα αποτέλσματα για την πρόταση:\n",
    "\n",
    "_\"At eight o'clock on Thursday morning....Arthur didn't feel very good. He quickly rushed to the Doctor!\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom :\n",
      " ['at', 'eight', 'o', 'clock', 'on', 'thursday', 'morning', 'arthur', 'didn', 't', 'feel', 'very', 'good', 'he', 'quickly', 'rushed', 'to', 'the', 'doctor'] \n",
      "\n",
      "nltk punkt :\n",
      " ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', '.Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'He', 'quickly', 'rushed', 'to', 'the', 'Doctor', '!'] \n",
      "\n",
      "sentece detector :\n",
      " [\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\", 'He quickly rushed to the Doctor!'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 2 (d)\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\n",
    "              He quickly rushed to the Doctor!\"\"\"\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizers = [tokenize,nltk.word_tokenize,sent_detector.tokenize]\n",
    "names = [\"custom\",\"nltk punkt\",\"sentece detector\"]\n",
    "for tokenizer,name in zip(tokenizers,names):\n",
    "    print(name,\":\\n\",tokenizer(sentence),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Περιληπτικά αναλύουμε τους tokenizers.\n",
    "\n",
    "<b>Sentence Tokenizer</b>: Αυτός ο tokenizer χωρίζει το κείμενο σε μία λίστα από προτάσεις χρησιμοποιώντας unsupervised learning για να αναγνωρίσει επιτυχώς τα διαχωριστηκά σημεία στίξης, και να αγνοείσει όπως η απόστροφος σε μία λέξη.  Ο αλγόριθμος αυτός πρέπει να εκπαιδευτεί σε ένα μεγάλο σύνολο δεδομένων πριν μπορέσει να χρησιμοποιηθεί αποτελεσματικά. Το NLTK data package περιλαμβάνει ένα προ-εκπαιδευμένο Punkt tokenizer για την αγγλική γλώσσα.\n",
    "\n",
    "<b>Word Τokenizer</b>: Αυτός ο tokenizer εντοπίζει τις λέξεις σε ένα string και τις αποθηκεύει σε μία λίστα. Από τα σημεία στίξης κρατάει μόνο εκείνα που διαχωρίζουν προτάσεις. Όπως και ο Sentence Tokenizer χρησιμοποιεί ένα προ-εκπαιδευμένο Punkt tokenizer για την αγγλική γλώσσα. \n",
    "\n",
    "Η δικία μας απλή εκδοχή του tokenizer είναι αρκετά αποτελεσματική και υστερεί μόνο όταν σημεία στίξης αποτελούν μέρος της λέξης.  \n",
    "\n",
    "Η βιβλιοθήκη nltk διαθέτει αρκετά εξεζητημένους tokenizers, ενδεικτικά αναφέρουμε τον tweet tokenizer που διατηρεί τα σημεία στίξης και τα emoji ενω αναγνωρίζει και τα hashtags. Ολα αυτά διαθέτουν μεγάλη πληροφορία, χρήσιμη για εφαρμογές ανάλυσης tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 3\n",
    "Με βάση τα tokens που προκύπτουν απο την προηγούμενη ανάλυηση βρίσκουμε τά μοναδικά tokens (λεξιλόγιο) και τους μοναδικούς χαρακτήρες (αλφάβητο) στο corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1574168112504,
     "user": {
      "displayName": "Dorothea Kalliora",
      "photoUrl": "",
      "userId": "17412342597228224634"
     },
     "user_tz": -120
    },
    "id": "YXntX-DgfvSF",
    "outputId": "19b454d9-ca2c-4be2-cb8c-bfe654403163",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295373 words overall\n",
      "12128 unique words in corpus\n",
      "31 symbols in alphabet:\n",
      "['g', 'ç', 't', 'ê', 'z', 'm', 'i', 'æ', 'c', 'h', 'v', 'p', 'u', 'à', 'n', 'é', 'x', 'f', 'k', 'e', 'w', 'j', 'r', 'a', 'q', 'y', 'd', 'l', 'b', 'o', 's']\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "\n",
    "\n",
    "text_1 = readfile('./data/36-0.txt',tokenize)\n",
    "text_2 = readfile('./data/2591-0.txt',tokenize)\n",
    "text_3 = readfile('./data/1342-0.txt',tokenize)\n",
    "\n",
    "text = text_1 + text_2 + text_3\n",
    "\n",
    "print(len(text),\"words overall\")\n",
    "word_corpus = list(set(text))\n",
    "print(len(word_corpus),\"unique words in corpus\")\n",
    "\n",
    "# Convert list of words to list of chars, get unique chars with set\n",
    "alphabet = list(set([c for word in word_corpus for c in word]))\n",
    "print(len(alphabet),\"symbols in alphabet:\")\n",
    "print(alphabet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε οτι το αλφάβητο είναι επαυξημένο με παραλλαγές γραμμάτων και έχει μέγεθος 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 4\n",
    "\n",
    "Δημιουργούμε τον πίνακα συμβόλων με βάση το αλφάβητο που υπολογίσαμε στο προηγούμενο βήμα. Το $\\epsilon$ αντιστοιχίζεται στο $0$, για τις υπολοιπες αντιστοιχίες ξεκινάμε απο εναν αυθαίρετο αριθμό. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "def create_syms(alphabet):\n",
    "    f = open(\"./chars.syms\",\"w+\")\n",
    "    f.write(f'<epsilon>     0\\n')  #for <epsilon> index 0\n",
    "    for i in range(len(alphabet)):\n",
    "        f.write(f'{alphabet[i]}     {i+50}\\n')  #for other characters index i+50\n",
    "        \n",
    "create_syms(alphabet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 5 \n",
    "Για την δημιουργία ενός ορθογράφου απαιτείται κατάλληλη μετρική για την απόσταση δύο λέξεων. \n",
    "\n",
    "θα χρησιμοποιήσουμε την απόσταση Levenshtein (ή edit distance). Η απόσταση μπορέι να υπολογιστεί αναδρομικά με χρήση dynamic programming (DP). Ανάμεσα σε δύο λέξεις μπορούν να γίνουν τρείς τύποι μετατροπών: εισαγωγή ενός χαρακτήρα ($ \\epsilon \\rightarrow a$), μετατροπή ενός χαρακτήρα σε έναν άλλο ($ a \\rightarrow b$) και διαγραφή ενός χαρακτήρα ($a \\rightarrow \\epsilon$). Θεωρούμε αρχικά ίδιο κόστος για όλες τις μετατροπές.\n",
    "\n",
    "(α) Δημιουργούμε κατάλληλο fst με μία κατάσταση, αντιστοιχίζοντας κάθε χαρακτήρα σε κάθε άλλον και στο $\\epsilon$, και το $\\epsilon$ σε κάθε χαρακτήρα με βάρος 1. Αντιστοιχίζουμε ακόμα τον χαρακτήρα στον εαυτό του με βάρος 0. Αν πάρουμε το shortest path τότε η έξοδος θα είναι ή ίδια η λέξη εισόδου, αφού το ελάχιστο βάρος προκύπτει αν δεν γίνει καμία μετατροπή. \n",
    "\n",
    "(β) Στην υλοποίηση που έγινε για το ερώτημα 5 έχουμε υποθέσει ότι όλα τα edits έχουν ίσο βάρος. Αυτό ουσιαστικά σημαίνει ότι για το μοντέλο μας οποιοδήποτε λάθος σε μία λέξη έχει την ίδια πιθανότητα εμφάνισης. Με βάση την διαίσθηση μας μια τέτοια θεώρηση δεν ανταποκρίνεται στα πραγματικά λάθη που συναντάμε σε κείμενα και προκύπτουν απο τον άνθρωπο. Για παράδειγμα μια λέξη που ξεκινά απο $b$ είναι σχεδόν αδύνατον να γραφεί λανθασμένα με $c$. Ιδανικά θα θέλαμε να γνωρίζουμε την κατανομή του λάθους για κάθε σύμβολο στο αλφάβητο η οποία εδώ έχουμε υποθέσει οτι ειναι ομοιόμορφη. \n",
    "\n",
    "Για να γίνει πειραματικός υπολογισμός θα θέλαμε ενα σύνολο δεδομένων train data της μορφής: $(original~word, wrong~spelling)$. Απο αυτό μπορούμε να υπολογίσουμε κάθε φορά την διόρθωση που απαιτείται. Η πιθανότητα θα προκύψει:\n",
    "\n",
    "$$ Pr[ a\\rightarrow b] = \\frac{|error = b \\rightarrow a|}{|train~samples|}, \\forall a,b \\in \\{A+\\epsilon\\} $$, όπου $A$ το αλφάβητο. \n",
    "\n",
    "Αυτές εινα οι a priori πιθανότητες για κάθε διόρθωση με βάση το training data. Μπορούν τώρα να χρησιμοποιηθούν για να υπολογιστούν τα βάρη στο μοντέλο αναγνώρισης μας με fst. Το βάρος για μια συγκεκριμένη διόρθωση Θα πρέπει να είναι αντιστρόφως ανάλογο της πιθανότητας εμφανίσης του αντίστοιχου λάθους. Η καθιερωμένη συνάρτηση αντιστοίχισης είναι η $ -log(p(X)) $ όπου $p(X)$ η συνάρτηση κατανομής πιθανοτήτων.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "\n",
    "# create a line corresponding to fst edge.\n",
    "def format_arc(src,dst,src_sym,dst_sym,w,f):\n",
    "    f.write(\"{} {} {} {} {} \\n\".format(src,dst,src_sym,dst_sym,w))\n",
    "\n",
    "\n",
    "def create_lev(file,alphabet,w1):\n",
    "    f = open(file,\"w+\")\n",
    "    for i in range(0, len(alphabet)):\n",
    "        format_arc(src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=alphabet[i], w = w1,f=f)\n",
    "        format_arc(src=0, dst=0, src_sym=alphabet[i], dst_sym=alphabet[i], w=0,f=f)\n",
    "        format_arc(src=0, dst=0, src_sym=alphabet[i], dst_sym=\"<epsilon>\", w = w1,f=f)\n",
    "        for j in range(0, len(alphabet)):\n",
    "            if(j!=i):\n",
    "                format_arc(src=0, dst=0, src_sym=alphabet[i], dst_sym=alphabet[j], w = w1,f=f)   \n",
    "    f.write('0\\n')\n",
    "    f.close()\n",
    "\n",
    "create_lev('lev.fst',alphabet,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev.fst lev.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 6 \n",
    "\n",
    "Ένας <b>αποδοχέας (acceptor)</b> είναι ένα fst όπου κάθε μετάβαση έχει ενα label (και προεραιτικά βάρος). Στην βιβλιοθήκη openfst1-6-1 μπορέι να θεωρηθεί ως μετατροπέας(transducer) με ίδιο input και output label. \n",
    "\n",
    "Σε αυτό το ερώτημα κατασκευάζουμε εναν αποδοχέα που αποδέχεται κάθε λέξη του λεξικού. Ο αποδοχέας έχει μία κοινή αρχική κατάσταση και απο εκεί κάθε λέξη επεκτείνεται σε καταστάσεις ανεξάρτητα απο τις άλλες. Δεν έχουμε βάρος στις μεταβάσεις (w = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "\n",
    "def create_acceptor(word_corpus,file,weights = {},model = 'Default'):\n",
    "    \n",
    "    ''' Create a suitable acceptor\n",
    "        word_corpus = dictionary of unique words\n",
    "        file = destination file\n",
    "        weights = dictionary with weights per word/letter/bigram respectively.\n",
    "                  In the case of bigrams it is a tuple.\n",
    "        model = language model type, one of: {Default,Word,Unigram,Bigram}\n",
    "    '''\n",
    "    \n",
    "    f = open(file,\"w+\")\n",
    "    s = 0\n",
    "    final_states = []\n",
    "    for word in word_corpus:\n",
    "        \n",
    "        if model == 'Default':\n",
    "            w1 = 0\n",
    "            w2 = 0\n",
    "        elif model == 'Word':\n",
    "            w1 = weights[word]\n",
    "            w2 = 0\n",
    "        elif model == 'Unigram':\n",
    "            w1 = 0\n",
    "        elif model == 'Bigram':\n",
    "            w1 = 0\n",
    "            prev_letter = ' '\n",
    "            \n",
    "        format_arc(0,s+1,\"<epsilon>\",\"<epsilon>\",w1,f)\n",
    "        s += 1\n",
    "        \n",
    "        for letter in word[0:]:\n",
    "            \n",
    "            if model == 'Unigram':\n",
    "                w2 = weights[letter]\n",
    "            \n",
    "            if model == 'Bigram':\n",
    "                w2 = weights[(prev_letter,letter)]\n",
    "            format_arc(s,s+1,letter, letter,w2,f)\n",
    "            s += 1\n",
    "            prev_letter = letter \n",
    "        final_states.append(s)\n",
    "\n",
    "    for state in final_states:\n",
    "        f.write(f'{state}\\n')\n",
    "    f.close()\n",
    "\n",
    "create_acceptor(word_corpus,\"acceptor.fst\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(β) Καλούμε παρακάτω τις συναρτήσεις $fstrmepsilon, fstdeterminze, fstminimize$ που βελτιώνουν το μοντέλο μας. Επεξηγούμε συνοπτικά την λειτουργία τους:\n",
    "\n",
    "**fstdetermize**: Δέχεται ως είσοδο έναν μετροπέα (transducer) και το αποτέλεσμα είναι ένα ισοδύναμο fst με την ιδιότητα ότι δεν υπάρχει κατάσταση όπου δύο μεταβάσεις έχουν το ίδιο σύμβολο εισόδου. Το αυτόματο γίνεται δηλαδή ντετερμινιστικό ως προς την είσοδο. Η συνάρτηση έχει το μειονέκτημα οτι χρησιμοποιεί το $\\epsilon$ σεμεταβάσεις, θεωρώντας το στοιχείου του αλφαβήτου. Ακόμα αν το αρχικό αυτόματο περιέχει $\\epsilon$-μεταβάσεις μπορεί το αποτέλεσμα να μην είναι ντετερμινιστικό. \n",
    "\n",
    "**fstrmepsilon**: Αφαιρεί απο ένα αυτόματο όλες τις $\\epsilon$-μεταβάσεις (όταν δηλαδή input = output = $\\epsilon$). \n",
    "\n",
    "**fstminimize**: Για εναν acceptor η συνάρτηση παράγει το ελάχιστο ισοδύναμο αυτόματο. Για εναν transducer η ελαχιστότητα δεν μπορεί να επιτευχθεί με την αυστηρή έννοια διότι κάτι τέτοιο θα απαιτούσε output labels με την μορφή συμβολοσειρών που δεν υποστηρίζεται απο την fst. Κάθε τέτοια μετάβαση ειναι ανταυτού μια ακολουθία απο μεταβάσεις με έξοδο χαρακτήρα. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6b\n",
    "\n",
    "!fstdeterminize acceptor.bin.fst acceptor.bin.fst\n",
    "!fstrmepsilon acceptor.bin.fst acceptor.bin.fst\n",
    "!fstminimize acceptor.bin.fst acceptor.bin.fst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 7\n",
    "\n",
    "Για να υλοποιήσουμε τον ορθογράφο ελάχιστης απόστασης (min edit distance spell checker) θα συνθέσουμε τον Levenshtein transducer με τον αποδοχέα του λεξικού που υλοποιήσαμε σε προηγούμενο ερώτημα. Το αποτέλεσμα είναι ένας transducer που διορθώνει τις λέξεις μόνο με κριτήριο τις ελάχιστες δυνατές μετατροπές που απαιτούνται, χωρίς να λαμβάνει υπόψη του καμία γλωσσική πληροφορία. \n",
    "\n",
    "α) \n",
    "\n",
    "* Για ίσα βάρη στα edits όπως αναλύσαμε και σε προηγούμενα ερωτήματα, όλες οι μετατροπές μεταξύ γραμμάτων έχουν ίση πιθανοτηα και αρα οι αντίστοιχες ακμές ίσο βάρος στο fst. Δεν υπάρχει συνεπώς κάποιο bias προς μια συγκεκριμένη κατέυθυνση και η επιλογή γίνεται καθαρά με την ελάχιστη edit distance. Αυτο μπορεί να οδηγήσει σε λάθη παρόλο που η λέξη προς διόρθωση μπορεί να είναι γνωστή. \n",
    "\n",
    "\n",
    "*  Για διαφορετικά βάρη των edits το fst είναι προδιαθετιμένο κάθε φορά να ακολουθήσει ένα συγκεκριμένο μονοπάτι μεταβάσεων. Αυτη η προδιάθεση μειώνει την τυχαιότητα στην εκτέλεση του μοντέλου, καθώς πολυ συχνά η λέξη με την ελάχιστη απόσταση θα είναι μοναδική. Η εισαγωγή bias με προσεκτική επιλογή των βαρών μπορεί να οδηγήσει σε πολυ καλά αποτελέσματα. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "\n",
    "#Step 7(a)\n",
    "!fstarcsort --sort_type=olabel lev.bin.fst lev.bin.fst\n",
    "!fstarcsort --sort_type=ilabel acceptor.bin.fst acceptor.bin.fst\n",
    "!fstcompose  lev.bin.fst acceptor.bin.fst spell_checker.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance prediction for ['cit'] is: \n",
      "it"
     ]
    }
   ],
   "source": [
    "#Step 7b\n",
    "word = ['cit']\n",
    "create_acceptor(word,\"in.fst\")\n",
    "\n",
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms   in.fst in.bin.fst\n",
    "!fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst \n",
    "print(f\"Min distance prediction for {word} is: \")\n",
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=10   \\\n",
    "| fstrmepsilon |  fsttopsort \\\n",
    "| fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait  | dot -Tjpg > ./img/min_cit.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(β) \n",
    "Παρουσιάζουμε σε μορφή διαγράμματος 10 πιθανές έλάχιστες προβλέψεις για την λέξη _\"cit\"_. Παρατηρούμε ότι λόγω των ίσων βαρών υπάρχουν πολλές διαφορετικές προβλέψεις ελάχιστης απόστασης. Συνεπώς κατω απο αυτές τις συνθήκες είναι αδύνατον το μοντέλο μας να εκτελεί με συνέπεια καλές προβλέψεις. \n",
    "\n",
    "Οι πιθανές προβλέψεις προκύπτουν: \n",
    "|\n",
    "$$\\{ fit, hit, cut, sit, city, it, lit, cat, pit, bit\\}$$ \n",
    "![predictions-cit](./img/min_cit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 8 \n",
    "\n",
    "Θα αξιολογήσουμε την επίδοση του ορθογράφου μας πάνω σε ένα σύνολο δεδομένων για evaluation. Η εκτίμηση μας είναι η λέξη του λεξικού με την ελάχιστη απόσταση απο την λέξη προς διόρθωση. Όπως αναφέραμε η λέξη αυτή δεν είναι μοναδική, και αρα το αποτέλεσμα εμπεριέχει τυχαιότητα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 8:\n",
    "import random\n",
    "random.seed()\n",
    "\n",
    "file = open('./data/spell_checker_test_set',\"r\")\n",
    "y_test = []\n",
    "X_test = []\n",
    "for line in file:\n",
    "    [y,X] = line.split(':')\n",
    "    X = X.split()\n",
    "    for word in X:\n",
    "        X_test.append(word)\n",
    "        y_test.append(y)\n",
    "# Get 20 random words from test set, along with their labels\n",
    "idxs = random.sample(range(0, len(y_test)), 20)\n",
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def predict(Y,X,spell_checker,Show = True):\n",
    "    correct_pred=0\n",
    "    for y,x in zip(Y,X):\n",
    "        create_acceptor([x],\"input.fst\")\n",
    "        !fstcompile  --isymbols=chars.syms --osymbols=chars.syms input.fst input.bin.fst\n",
    "        !fstarcsort --sort_type=olabel input.bin.fst input.bin.fst \n",
    "        command = f'''fstcompose input.bin.fst {spell_checker} |fstshortestpath --nshortest=1 \\\n",
    "        | fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "        | cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n' \n",
    "        '''\n",
    "        prediction = os.popen(command).read()\n",
    "        if Show:\n",
    "            print(\"Input:\",x,\"\\n  --Correct:   \",y,\"\\n  --Prediction:\",prediction,\"\\n\")\n",
    "        if y == prediction:\n",
    "            correct_pred+=1\n",
    "    print(f\"{spell_checker}-accuracy:{correct_pred/len(Y)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: protend \n",
      "  --Correct:    pretend \n",
      "  --Prediction: pretend \n",
      "\n",
      "Input: biscutes \n",
      "  --Correct:    biscuits \n",
      "  --Prediction: disputes \n",
      "\n",
      "Input: poetre \n",
      "  --Correct:    poetry \n",
      "  --Prediction: poetry \n",
      "\n",
      "Input: levals \n",
      "  --Correct:    levels \n",
      "  --Prediction: level \n",
      "\n",
      "Input: recipt \n",
      "  --Correct:    receipt \n",
      "  --Prediction: receipt \n",
      "\n",
      "Input: failes \n",
      "  --Correct:    fails \n",
      "  --Prediction: files \n",
      "\n",
      "Input: dirven \n",
      "  --Correct:    driven \n",
      "  --Prediction: siren \n",
      "\n",
      "Input: acomodation \n",
      "  --Correct:    accommodation \n",
      "  --Prediction: condition \n",
      "\n",
      "Input: scarely \n",
      "  --Correct:    scarcely \n",
      "  --Prediction: scarcely \n",
      "\n",
      "Input: oppasite \n",
      "  --Correct:    opposite \n",
      "  --Prediction: opposite \n",
      "\n",
      "Input: parrallel \n",
      "  --Correct:    parallel \n",
      "  --Prediction: parallel \n",
      "\n",
      "Input: parrallell \n",
      "  --Correct:    parallel \n",
      "  --Prediction: parallel \n",
      "\n",
      "Input: contende \n",
      "  --Correct:    contented \n",
      "  --Prediction: content \n",
      "\n",
      "Input: juise \n",
      "  --Correct:    juice \n",
      "  --Prediction: use \n",
      "\n",
      "Input: magnificant \n",
      "  --Correct:    magnificent \n",
      "  --Prediction: magnificent \n",
      "\n",
      "Input: sissors \n",
      "  --Correct:    scissors \n",
      "  --Prediction: scissors \n",
      "\n",
      "Input: oppisit \n",
      "  --Correct:    opposite \n",
      "  --Prediction: opposite \n",
      "\n",
      "Input: magificent \n",
      "  --Correct:    magnificent \n",
      "  --Prediction: magnificent \n",
      "\n",
      "Input: uneque \n",
      "  --Correct:    unique \n",
      "  --Prediction: unequal \n",
      "\n",
      "Input: pritend \n",
      "  --Correct:    pretend \n",
      "  --Prediction: pretend \n",
      "\n",
      "spell_checker.bin.fst-accuracy:0.6%\n"
     ]
    }
   ],
   "source": [
    "predict(Y_rand,X_rand,\"spell_checker.bin.fst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Διορθώνουμε 20 τυχαίες λέξεις στο test set και μετράμε το accuracy.\n",
    "\n",
    "Για διαφορετικές επαναλήψεις παρατηρήσαμε οτι η ακρίβεια προκύπτει απο 0.4% εώς 0.7%. Για λέξεις με μεγάλο μήκος που απέχουν πολύ απο άλλες λέξεις στο λεξιλόγιο το μοντέλο έχει καλή απόδοση και τις αναγνωρίζει με επιτυχία. Για παράδειγμα οι λέξεις: $\\{ biscuits, independent, bicycle, southern, scissors, visitors \\}$ αναγνωρίζονται σωστά. Λέξεις όμως όπως τα: $\\{ poems, cake , awful \\}$ αναγνωρίζονται δυσκολότερα γιατί ειναι μικρές και έχουν κοινά προθέματα και επιθέματα με άλλες λέξεις. \n",
    "\n",
    "Δύο είναι οι κύριοι παράγοντες προς βελτίωση που επηρεάζουν την απόδοση του μοντέλου:\n",
    "\n",
    "* Το μικρό σύνολο εκπαίδευσης. Εάν το μοντέλο δεν γνωρίζει μια λέξη δεν μπορεί να την αναγνωρίσει και αρα η διόρθωση σε αυτή θα προκύπτει πάντα λάθος. Το corpus απο ένωση 2 βιβλίων δεν ειναι αρκετό για να εξαλείψει σε ικανοποιητικό βαθμό αυτον τον τύπο λάθους.\n",
    "\n",
    "* Τα ίσα βάρη στις μετατροπές. Σε πολλές περιπτώσεις ακόμα και αν το μοντέλο γνωρίζει μια λέξη δεν καταφέρνει να διορθώσει σε αυτήν γιατί υπάρχουν ακόμα πολλές λέξεις με την ίδια ελάχιστη απόσταση. Με την \"δίκαιη\" αυτή αντιμετώπιση εισάγεται τυχαιότητα στην απόδοση του μοντέλου καθώς το αν θα προκύψει η σωστή λέξη απο αυτές με την ελάχιστη απόσταση ειναι κατα βάση τυχαίο. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 9\n",
    "\n",
    "Στο ερώτημα αυτο θα ασχοληθούμε με αναπαραστάσεις word2vec. Έχουμε ήδη μιλήσει για διανυσματικές αναπαραστάσεις λέξεων στην εισαγωγή. Στόχος είναι η αναπαράσταση λέξεων στον $\\mathbb{R}^d$ έτσι ώστε να βρίσκονται σημασιολογικά κοντά. Το training γίνεται με βάση την θέση τους στο κείμενο με κυλιόμενο παράθυρο. Οι 2 βασικές προσεγγίσεις είναι Continuous Bag of Words (η θέση στο παράθυρο δεν εχει σημασία) και continuous skip gram ( η λέξη χρησιμοποείται για πρόβλεψη των γειτονικών). \n",
    "\n",
    "Διαβάζουμε αρχικά το corpus σε μία λίστα απο προτάσεις με το tokenization που είχαμε υλοποιείσει σε προηγούμενο ερώτημα. Στην συνέχεια εκπαιδεύουμε 100 διάστατα word2vec embeddings με βάση τις προτάσεις που προκύπτουν. Χρησιμοποιούμε $window=5$ και $epochs=100$.\n",
    "\n",
    "Για 10 τυχαίες λέξεις θα δείξουμε τις σημασιολογικά κοντινότερες τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 9(a)\n",
    "\n",
    "def tokenized_list(path,preprocess=identity_preprocess):\n",
    "  list_of_sentences=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    l = preprocess(line)\n",
    "    if l:\n",
    "        list_of_sentences.append(l)\n",
    "  return list_of_sentences\n",
    "\n",
    "\n",
    "list1 = tokenized_list('./data/36-0.txt',tokenize)\n",
    "list2 = tokenized_list('./data/2591-0.txt',tokenize)\n",
    "list3 = tokenized_list('./data/1342-0.txt',tokenize)\n",
    "final_list = list1 + list2 + list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Step 9(b,c)\\n\\n# Initialize word2vec. Context is taken as the 2 previous and 2 next words\\nmodel = Word2Vec(final_list, window=5, size=100, workers=4)\\nmodel.train(final_list, total_examples=len(final_list), epochs=1000)\\n# get ordered vocabulary list\\nvoc = model.wv.index2word\\n# get vector size\\ndim = model.vector_size\\n#pick 10 random words from the dictionary\\nidxs = random.sample(range(0, len(voc)), 10)\\nrand_words = [voc[i] for i in idxs]\\nfor word in rand_words:\\n    print(f\\'Most similar words to \"{word}\":\\')\\n    for word,sim in model.wv.most_similar(word):\\n        print(f\\'     \"{word}\" -- sim: {sim}\\')\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Step 9(b,c)\n",
    "\n",
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "model = Word2Vec(final_list, window=5, size=100, workers=4)\n",
    "model.train(final_list, total_examples=len(final_list), epochs=1000)\n",
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "# get vector size\n",
    "dim = model.vector_size\n",
    "#pick 10 random words from the dictionary\n",
    "idxs = random.sample(range(0, len(voc)), 10)\n",
    "rand_words = [voc[i] for i in idxs]\n",
    "for word in rand_words:\n",
    "    print(f'Most similar words to \"{word}\":')\n",
    "    for word,sim in model.wv.most_similar(word):\n",
    "        print(f'     \"{word}\" -- sim: {sim}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(γ) Τα αποτελέσματα του μοντέλου δεν ειναι ικανοποιητικα. Οι λέξεις δεν έχουν επι το πλείστον σημασιολογική συσχέτιση, εκτός ενδεχομένως απο κάποιο κοινό θέμα.´Όπως οι μέρες της εβδομάδας, οι αριθμοί, ανθρώπινα ονόματα ή κάποια ανθρώπινη δράση. \n",
    "\n",
    "Θα προσπαθήσουμε τώρα να βελτιώσουμε το αποτέλεσμα του μοντέλου αλλάζωντας τις εξής παραμέτρους:\n",
    "* Αυξάνουμε το μέγεθος του παραθύρου context κρατώντας τον αριθμό εποχών σταθερό\n",
    "\n",
    "* Αυξάνουμε τον αριθμό των εποχών κρατώντας το μέγεθος του παραθύρου σταθερό\n",
    "\n",
    "* Αυξάνουμε και τον αριθμό εποχών και το μέγεθος του παραθύρου\n",
    "\n",
    "Παρακάτω παραθέτουμε τα αποτελέσματα του similarity 20 τυχαίων λέξεων για κάθε μία από τις τρεις περιπτώσεις. Για μέγεθος παραθύρου κοντά στο 1 έχουμε πληροφορία σύνταξης. Για μεγαλύτερο παράθυρο αντιστοιχίζουμε λέξεις με βάση την σημασιολογία. \n",
    "\n",
    "Για μεγαλύτερο μέγεθος παραθύρου θα περιμέναμε λοιπόν καλύτερα αποτελέσματα, εφόσων αναζητούμε σημασιολογικά κοντινές λέξεις. Κάτι τέτοιο δεν συμβαίνει στην πράξη. Εικάζουμε οτι αυτο οφείλεται στο μικρό σύνολο δεδομένων εκπαίδευσης που δεν επιτρέπουν στο μοντέλο να έχει μεγάλη εκφραστικότητα. Συγκεκριμένα για ακριβείς αναπαραστάσεις word2vec θέλουμε εκαττομύρια λέξεις και όχι της ταξής των 3000 που προκύπτουν απο το corpus μας. Παρατηρήσαμε οτι το word2vec όπως ειναι υλοποιημένο στην βιβλιοθήκη αγνοεί σπάνιες λέξεις, για αυτο και υπήρξε μείωση στο αρχικό vocabulary που εξάγαμε. Συγκεκριμένα η παράμετρος _min_count (int, optional)_ καθορίζει πόσο μικρή συχνότητα πρέπει να έχει μια λέξη για να θεωρηθεί οτι δεν μπορεί να δώσει με ακρίβεια πληροφορία, και να αγνοηθεί.\n",
    "\n",
    "Για αύξηση των αριθμών των epochs επίσης δεν έχουμε καλύτερο αποτέλεσμα. Για να επηρεάσει ουσιαστικά ο αριθμός εποχών θα πρέπει να έχουμε ενα αρκετά μεγάλο σύνολο δεδομένων ώστε ο αλγόριθμος να μην κανει πρόωρα converge. Για εμάς και μικρός αριθμός εποχών ειναι αρκετος. \n",
    "\n",
    "Συμπερασματικά  ο πιο καθοριστικός παράγοντας για σωστή εξαγωγή αναπαραστάσεων ειναι ο αριθμός των δεδομένων εκπαίδευσης.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9(c (i))\n",
    "\n",
    "def similarity(w,s,e):\n",
    "    model = Word2Vec(final_list, window=w, size=s, workers=4)\n",
    "    model.train(final_list, total_examples=len(final_list), epochs=e)\n",
    "    voc = model.wv.index2word\n",
    "    dim = model.vector_size\n",
    "    rand_words = [voc[i] for i in idxs]\n",
    "    for word in rand_words:\n",
    "        print(f'Most similar words to \"{word}\":')\n",
    "        for word,sim in model.wv.most_similar(word)[0:n]:\n",
    "            print(f'     \"{word}\" -- sim: {sim}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity(10,100,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity(15,100,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9(c (ii))\n",
    "\n",
    "#similarity(5,100,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity(5,100,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9(c (iii))\n",
    "\n",
    "#similarity(10,100,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\" >Μέρος 1 </h1>\n",
    "<h1 align = \"center\" >Ορθογράφος </h1>\n",
    "\n",
    "Στο πρώτο μέρος ενισχύουμε τον ορθογράφο που έχουμε ήδη υλοποιήσει χρησιμοποιώντας character level και word level unigram γλωσσικά μοντέλα, ενώ θα γίνει πειραματισμός και με bigram γλωσσικά μοντέλα. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 10\n",
    "Για να βελτιώσουμε την απόδοση του ορθογράφου μας θα πρέπει να πετύχουμε την μέγιστη αξιοποίηση του συνόλου εκπαίδευσης. Για τον σκοπό αυτό εξάγουμε στατιστικά χαρακτηριστικά απο τα δεδομένα και ενσωματόνουμε την πληροφορία αυτή αλλάζοντας τα βάρη του μοντέλου. Οι πηγές στατιστικών θα είναι:\n",
    "* word level: εξάγουμε την πιθανότητα εμφάνισης κάθε λέξης\n",
    "* character level: εξάγουμε την πιθανότητα εμφάνισης κάθε χαρακτήρα\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable word in the dictionary: 'the' with probability: 0.05669103134003446\n",
      "Least probable word in the dictionary: 'inhabited' with probability: 3.38554979635918e-06\n"
     ]
    }
   ],
   "source": [
    "#Step 10\n",
    "from collections import Counter\n",
    "\n",
    "word_prob = Counter(text) # Counter returns a dictionary {word: freq} in a fast way\n",
    "word_prob = {word:prob/len(text) for word,prob in word_prob.items()}\n",
    "\n",
    "chars = [char for word in text for char in word]\n",
    "char_prob = Counter(chars)\n",
    "char_prob = {char:prob/len(chars) for char,prob in char_prob.items()}\n",
    "max_prob_word = max(word_prob, key=word_prob.get)\n",
    "min_prob_word = min(word_prob, key=word_prob.get)\n",
    "print(f\"The most probable word in the dictionary: '{max_prob_word}' with probability: {word_prob[max_prob_word]}\")\n",
    "print(f\"Least probable word in the dictionary: '{min_prob_word}' with probability: {word_prob[min_prob_word]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 11\n",
    "Έχουμε ήδη δουλέψει με την απόσταση **Levenshtein** (ή edit distance). Χρησιμοποιούμε 3 τύπους απο edits: \n",
    "* Εισαγωγή χαρακτήρα \n",
    "* Διαγραφή χαρακτηρα \n",
    "* Αντικατάσταση χαρακτήρα\n",
    "α) Υπολογίζουμε την μέση τιμή των βαρών του word level μοντέλου δηλαδή:\n",
    "\n",
    "$$ W_{word}^{average} = \\sum_{i} \\cdot -log(~p(word_i)~) / |words|$$\n",
    "\n",
    "Τα βάρη δίνονται απο την συνάρτηση $-log(p(x))$ , η οποία διαισθητικά κωδικοποιεί σωστά την πληροφορία, δίνοντας μεγαλύτερο βάρος στα λιγότερο συχνά ενδεχόμενα.\n",
    "Το κόστος των edits για το word-level μοντέλο είναι η μέση τιμή $w  = \\overline{W} $. \n",
    "\n",
    "Εναλλακτικά μπορούμε να υπολογίσουμε την πιθανοτική μέση τιμή των βαρών αν θεωρήσουμε τυχαία μεταβλητή με τιμές τα βάρη και πιθανότητες την πιθανότητα της αντίστοιχης λέξης. Τότε η μέση τιμή των βαρών είναι επίσης η *εντροπία* της κατανομής των λέξεων $p(x), x \\in word~corpus$. \n",
    "\n",
    "$$ \\mathbb{E}[W_{word}] = -\\sum_{i} p(word_i) \\cdot log(~p(word_i)~) $$\n",
    "\n",
    "(β) Κατασκευάζουμε έναν μετατροπέα με μία κατάσταση που υλοποιεί την απόσταση Levenshtein. Για κάθε edit το κόστος είναι $w$, εκτός απο την αντικατάσταση ενός γράμματος με τον εαυτό του που έχει κόστος 0. \n",
    "\n",
    "(γ) Επαναλαμβάνουμε για το unigram γλωσσικό μοντέλο.\n",
    "\n",
    "(δ) Όπως έχουμε αναφέρει αυτός ο τρόπος υπολογισμού των βαρών δεν κωδικοποιεί σημαντική πληροφορία και δεν βελτιώνει την απόδοση του μοντέλου μας.\n",
    "\n",
    "Ιδανικά θα θέλαμε ένα σύνολο labeled δεδομένων της μορφής (original word, wrong spelling). Με αυτό τον τρόπο μπορούμε να εξάγουμε σημαντική πληροφορία βρίσκοντας την πιθανότητα ενός συγκεκριμένου edit. \n",
    "\n",
    "Διαισθητικά δεν είναι όλες οι μετατροπές το ίδιο πιθανές. Για παράδειγμα μια λέξη που ξεκινά απο $a$ είναι σχεδόν αδύνατον να γραφεί λανθασμένα με $z$ (θεωρώντας ρεαλιστικά ανθρώπινα δεδομένα και όχι τυχαίο θωρυβώδες dataset). Θα θέλαμε λοιπόν να υπολογίσουμε την _a priori_ πιθανότητα κάθε edit και αυτήν να κωδικοποιήσουμε στα βάρη μας:\n",
    "\n",
    "$$ Pr[ a\\rightarrow b] = \\frac{|error = b \\rightarrow a|}{|train~samples|}, \\forall a,b \\in \\{A+\\epsilon\\} $$, όπου $A$ το αλφάβητο. \n",
    "\n",
    "Το βάρος για μια συγκεκριμένη διόρθωση Θα πρέπει να είναι αντιστρόφως ανάλογο της πιθανότητας εμφανίσης του αντίστοιχου λάθους. Η ιδιότητα μπορεί να επιτευχθεί με την συνάρτηση  $ -log(p(X)) $ όπου $p(X)$ η συνάρτηση κατανομής πιθανοτήτων.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average weight for word-level model: 16.366455555062522\n",
      "Average weight for unigram model: 7.803325647790889\n"
     ]
    }
   ],
   "source": [
    "#Step 11(a)\n",
    "import math\n",
    "\n",
    "# We create the weight dictionaries with an elegant dict comprehension..\n",
    "weight_words = {word: -math.log(prob,2) for word,prob in word_prob.items()}\n",
    "weight_chars = {char: -math.log(prob,2) for char,prob in char_prob.items()}\n",
    "\n",
    "avg_weight_words = sum(list(weight_words.values()))/len(list(weight_words.values()))\n",
    "print(f\"Average weight for word-level model: {avg_weight_words}\")\n",
    "\n",
    "avg_weight_chars = sum(list(weight_chars.values()))/len(list(weight_chars.values()))\n",
    "print(f\"Average weight for unigram model: {avg_weight_chars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 11(b)\n",
    "\n",
    "create_lev(\"lev_word.fst\",alphabet,avg_weight_words)\n",
    "create_lev(\"lev_unigram.fst\",alphabet,avg_weight_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev_word.fst lev_word.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev_unigram.fst lev_unigram.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 12\n",
    "Κατασκευάζουμε έναν αποδοχέα που αποδέχεται κάθε λέξη του corpus. Για βάρος χρησιμοποιούμε το $-log(P(word))$, δίνοντας στο μοντέλο περισσότερη πληροφορία και βελτιώνωντας το κριτήριο επιλογής λέξης. \n",
    "\n",
    "Ακολουθούμε την διαδικασία τόσο για το unigram όσο και για το word level γλωσσικό μοντέλο. \n",
    "Έχουμε τροποποθήσει την συνάρτηση δημιουργίας αποδοχέα στο βήμα 6 έτσι ώστε να περιλαμβάνει περιπτώσεις για τους διαφορετικούς τύπους μοντέλων: \n",
    "* Simple acceptor (zero weights)\n",
    "* Word level \n",
    "* Unigram\n",
    "* Bigram\n",
    "\n",
    "Στην συνέχεια βελτιστοποιούμε τα μοντέλα με τις _fstdeterminize, fstrmepsilon, fstminimize_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_acceptor(word_corpus,\"acceptor_word.fst\",weight_words,model='Word')\n",
    "create_acceptor(word_corpus,\"acceptor_unigram.fst\",weight_chars,model='Unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor_word.fst acceptor_word.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_word.bin.fst acceptor_word.bin.fst\n",
    "!fstrmepsilon acceptor_word.bin.fst acceptor_word.bin.fst\n",
    "!fstminimize acceptor_word.bin.fst acceptor_word.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor_unigram.fst acceptor_unigram.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_unigram.bin.fst acceptor_unigram.bin.fst\n",
    "!fstrmepsilon acceptor_unigram.bin.fst acceptor_unigram.bin.fst\n",
    "!fstminimize acceptor_unigram.bin.fst acceptor_unigram.bin.fst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 13\n",
    "Ακολουθώντας την διαδικασία του ερωτήματος 7 θα κατασκευάσουμε ορθογράφο με το word-level γλωσσικό μοντέλο και μετατροπέα, και αντίστοιχα με το unigram μοντέλο και μετατροπέα.\n",
    "\n",
    "Περιμένουμε οι ορθογράφοι αυτοί να έχουν καλύτερη απόδοση απο τον απλοϊκό ορθογράφο του βήματος 7, καθώς κωδικοποιούν πληροφορία και στα βάρη τους. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13(a)\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev_word.bin.fst lev_word.bin.fst\n",
    "!fstcompose  lev_word.bin.fst acceptor_word.bin.fst spell_checker_word.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13(b)\n",
    "\n",
    "\n",
    "!fstcompose  lev_word.bin.fst acceptor_unigram.bin.fst spell_checker_unigram.bin.fst\n",
    "#!fstarcsort --sort_type=olabel lev_unigram.bin.fst lev_unigram.bin.fst\n",
    "#!fstcompose  lev_unigram.bin.fst acceptor_unigram.bin.fst spell_checker_unigram.bin.fst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance prediction for ['cit'] is: \n",
      "it"
     ]
    }
   ],
   "source": [
    "#Step 13(c)\n",
    "# We repeat the process in step 7.. \n",
    "\n",
    "\n",
    "word = ['cit']\n",
    "create_acceptor(word,\"in.fst\")\n",
    "\n",
    "# Word-level model\n",
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms in.fst in.bin.fst\n",
    "!fstarcsort --sort_type=ilabel spell_checker_word.bin.fst spell_checker_word.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst \n",
    "\n",
    "print(f\"Min distance prediction for {word} is: \")\n",
    "!fstcompose in.bin.fst spell_checker_word.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance prediction for ['cit'] is: \n",
      "it"
     ]
    }
   ],
   "source": [
    "# Unigram model\n",
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms   in.fst in.bin.fst\n",
    "!fstarcsort --sort_type=ilabel spell_checker_unigram.bin.fst spell_checker_unigram.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst \n",
    "\n",
    "print(f\"Min distance prediction for {word} is: \")\n",
    "!fstcompose in.bin.fst spell_checker_unigram.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(γ) Οι δύο ορθογράφοι που δημιουργήσαμε έχουν ίδια αρχή λειτουργίας και αντιμετωπίζουν παρόμοια προβλήματα. Συγκεκριμένα η εκτίμηση τους είναι η λέξη με την ελάχιστη Levenhstein απόσταση απο την λέξη εισόδου. Η διαφορά είναι οτι στο word-level μοντέλο η επιλογή σταθμίζεται απο το βάρος της λέξης που τελικά επιλέγουμε, ενώ στο unigram μοντέλο σταθμίζεται αντιστοιχα κάθε επιλογή χαρακτήρα.\n",
    "\n",
    "Το μοντέλο επηρεάζεται σημαντικά απο το περιορισμένο λεξιλόγιο του, έτσι αν δεν γνωρίζει την ύπαρξη μίας λέξης δεν μπορεί να διορθώσει σε αυτή. Αυτός είναι ένας λογικός περιορισμός και διορθώνεται με την αύξηση των train δεδομένων. \n",
    "\n",
    "Το δεύτερο σημαντικό ελάττωμα του μοντέλου είναι οτι δεν διαθέτει αρκετά καλο κριτήριο για την επιλογή λέξεων σε περίπτωση ισοπαλίας. Αυτό έχει σε έναν βαθμό διορθωθεί με την χρήση στατιστικών στοιχείων στα παραπάνω 2 μοντέλα. \n",
    "\n",
    "Η αμφισημία προκύπτει γιατί παρά τα βάρη υπάρχουν λέξεις με ίδιο κόστος διόρθωσης και έτσι το μοντέλο πρέπει να επιλέξει τυχαία. (?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 14\n",
    "Θα αξοιλογήσουμε τους δύο ορθογράφους που δημιουργήσαμε πάνω στο spell checker test set που είχαμε δεί στο βήμα 8. Θα χρησιμοποιήσουμε την συνάρτηση που ήδη έχουμε γράψει στο βήμα 8. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spell_checker_unigram.bin.fst-accuracy:0.5370370370370371%\n"
     ]
    }
   ],
   "source": [
    "#Too slow, need help :'( \n",
    "predict(y_test,X_test,\"spell_checker.bin.fst\",Show = False)\n",
    "predict(y_test,X_test,\"spell_checker_word.bin.fst\",Show = False)\n",
    "predict(y_test,X_test,\"spell_checker_unigram.bin.fst\",Show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 15\n",
    "Θα εκτελέσουμε τα προηγούμενα βήματα για ενα Bigram γλωσσικό μοντέλο.\n",
    "Το bigram γλωσσικό μοντέλο ανήκει στην ευρύτερη κλάση των n-gram μοντέλων και προκύπτει για n = 2. Με τον όρο n-gram αναφερόμαστε σε μία συνεχή ακολουθία n αντικειμένων απο ένα δείγμα φωνής ή κειμένου Συγκεκριμένα στο πλάισιο των λέξεων ένα n-gram γλωσσικό μοντέλο χρησιμοποιεί ακολουθίες n χαρακτήρων για να προβλέψει το επόμενο γράμμα. Η πρόβλεψη βασίζεται σε (n-1)-order αλυσίδα markov (δηλαδή ισχύει η μαρκοβιανή ιδιότητα αλλα η εξάρτηση σταματάει στα n προηγούμενα δείγματα). \n",
    "\n",
    "$$ \\mathbb{P}[x_i|x_{i-1},x_{i-2},...,x_0] = \\mathbb{P}[x_i|x_{i-1},x_{i-2},...,x_{i-(n-1)}] $$ \n",
    "\n",
    "Δυο πλεονεκτήματα τους ειναι:\n",
    "* Η απλότητα\n",
    "* Η κλιμακωσιμότητα\n",
    "\n",
    "Για το bigram μοντέλο αρκεί να υπολογίσουμε τις πιθανότητες:\n",
    "\n",
    "$$ \\mathbb{P}[x_i|x_{i-1}] $$\n",
    "για κάθε ζεύγος στο αλφάβητο μας (συν το $\\epsilon$).\n",
    "\n",
    "\n",
    "Στο παρακάτω κελί υπολογίζουμε με παρόμοιο τρόπο τις πιθανότητες εμφάνισης για κάθε bigram. Συγκεκριμένα έχουμε επαυξήσει κάθε λέξη του συνόλου ώστε να αρχίζει απο το κενό, αυτή είναι η πιθανότητα να επιλεγεί το γράμμα απο το οποίο αρχίζει η λέξη χωρίς να έχει προηγηθεί κάποιο άλλο. Έπειτα υπολογίζουμε τα κατάλληλα βάρη και υλοποιούμε τον μετατροπέα και τον αποδοχέα για να τους συνδυάσουμε στον bigram spell checker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extended_text = [ \" \" + word for word in text] # \" \" symbolizes epsilon, for bigrams like (<epsilon>,a)\"\n",
    "bigrams = [(char1,char2) for word in extended_text for char1,char2 in zip(word,word[1:])]\n",
    "bigram_prob = Counter(bigrams)\n",
    "bigram_prob = {bigram:prob/len(bigrams) for bigram,prob in bigram_prob.items()}\n",
    "\n",
    "weight_bigrams =  {bigram: -math.log(prob,2) for bigram,prob in bigram_prob.items()}\n",
    "avg_weight_bigrams = sum(list(weight_bigrams.values()))/len(list(weight_bigrams.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spell_checker_bigram.bin.fst-accuracy:0.3814814814814815%\n"
     ]
    }
   ],
   "source": [
    "create_lev(\"lev_bigram.fst\",alphabet,avg_weight_bigrams)\n",
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev_bigram.fst lev_bigram.bin.fst\n",
    "\n",
    "create_acceptor(word_corpus,\"acceptor_bigram.fst\",weight_bigrams,model='Bigram')\n",
    "\n",
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor_bigram.fst acceptor_bigram.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstrmepsilon acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstminimize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev_word.bin.fst lev_word.bin.fst\n",
    "#!fstarcsort --sort_type=olabel lev_bigram.bin.fst lev_bigram.bin.fst\n",
    "\n",
    "!fstcompose  lev_word.bin.fst acceptor_bigram.bin.fst spell_checker_bigram.bin.fst\n",
    "#!fstcompose  lev_bigram.bin.fst acceptor_bigram.bin.fst spell_checker_bigram.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstrmepsilon acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstminimize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "\n",
    "\n",
    "predict(y_test,X_test,\"spell_checker_bigram.bin.fst\",Show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε οτι με το bigram μοντέλο η ακρίβεια πάνω στο test set έπεσε στο **_0.185%_**. Αυτο επιβεβαιώνει την αντίληψη μας ότι μοντέλα βασισμένα σε χαρακτήρες δεν λειτουργούν καλά για το συγκεκριμένο πρόβλημα, και μία προσέγγιση με στοιχεία λέξεις θα έχει πολυ καλύτερα αποτελέσματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spell_checker.bin.fst-accuracy:0.8%\n",
      "spell_checker_word.bin.fst-accuracy:0.8%\n",
      "spell_checker_unigram.bin.fst-accuracy:0.75%\n",
      "spell_checker_bigram.bin.fst-accuracy:0.6%\n"
     ]
    }
   ],
   "source": [
    "idxs = random.sample(range(0, len(y_test)), 20)\n",
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]\n",
    "\n",
    "predict(Y_rand,X_rand,\"spell_checker.bin.fst\",Show = False)\n",
    "predict(Y_rand,X_rand,\"spell_checker_word.bin.fst\",Show = False)\n",
    "predict(Y_rand,X_rand,\"spell_checker_unigram.bin.fst\",Show = False)\n",
    "predict(Y_rand,X_rand,\"spell_checker_bigram.bin.fst\",Show = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">ΜΕΡΟΣ 2</h1>\n",
    "\n",
    "<h3 align = \"center\">Χρήση σημασιολογικών αναπαραστάσεων για ανάλυση συναισθήματος.</h3>\n",
    "\n",
    "Στο πρώτο μέρος της άσκησης ασχοληθήκαμε κυρίως με συντακτικά μοντέλα για την κατασκευή ενός ορθογράφου. Εδώ θα ασχοληθούμε με τη χρήση λεξικών αναπαραστάσεων για την κατασκευή ενός ταξινομητή συναισθήματος. Ως δεδομένα θα χρησιμοποιήσουμε σχόλια για ταινίες από την ιστοσελίδα IMDB και θα τα ταξινομήσουμε σε θετικά και αρνητικά ως προς το συναίσθημα."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
