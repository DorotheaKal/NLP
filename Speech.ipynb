{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Επεξεργασία Φωνής και Φυσικής Γλώσσας \n",
    "### 1η Προπαρασκευαστική Εργασία \n",
    "### Θεδωρόπουλος Νικήτας -03115185\n",
    "### Καλλιώρα Δωροθέα - 03115176 ? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της εργαστηριακής άσκησης είναι η δημιουργία ενός απλού ορθογράφου με χρήση μηχανών πεπερασμένων καταστάσεων (fst) της βιβλιοθήκης openfst (v1.6.1). Θα χρησιμοποιήσουμε corpus απο δημόσια διαθέσιμα βιβλία για να δημιουργήσουμε ενα γλωσσικό μοντέλο σε μορφή λεξικού. Τέλος θα γίνει εισαγωγή σε αναπαραστάσεις word2vec. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κατασκευάζουμε το corpus με σύμπτηξη plain text βιβλίων, δημόσια διαθέσημα στο Project Gutenberg. Θα χρησιμοποιήσουμε corpus 2 βιβλίων The War of the Worlds by H. G. Wells και Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm. \n",
    "\n",
    "Η τεχνικής της συνένωσης πολλών βιβλίων, εκτός απο το προφανές πλεονέκτημα του μεγαλύτερου train corpus, αυξάνει την αναγνωριστική ισχύ του μοντέλου αφού μπορεί να αντλήσει λεξιλόγειο απο διαφορετικές πηγές και άρα να έχει μεγαλύτερο corpus λέξεων.Ακόμα μειώνει το bias που προκύπτει απο ενα συγκεκριμένο βιβλίο ή απο έναν συγκεκριμένο συγγραφέα, που χρησιμοποιεί συγκεκριμένες λέξεις λόγω προτίμησης, εποχής συγγραφής και άλλων παραγόντων. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -P ./data/ http://www.gutenberg.org/files/36/36-0.txt\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/2591/2591-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OqT0WsSYSLb"
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "def identity_preprocess(str):\n",
    "  return str\n",
    "\n",
    "def readfile(path,preprocess=identity_preprocess):\n",
    "  processed_txt=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    processed_txt= processed_txt + preprocess(line)\n",
    "  return processed_txt \n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.strip().lower()\n",
    "    s = ''.join(c if c.isalpha() else ' ' for c in s)\n",
    "    # We replace all non-alpha characters with ' '\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.split()\n",
    "    return s\n",
    "  \n",
    "# TODO: experiment with ntk tokenizers \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1574168112504,
     "user": {
      "displayName": "Dorothea Kalliora",
      "photoUrl": "",
      "userId": "17412342597228224634"
     },
     "user_tz": -120
    },
    "id": "YXntX-DgfvSF",
    "outputId": "19b454d9-ca2c-4be2-cb8c-bfe654403163",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64183 words overall\n",
      "7072 unique words in corpus\n",
      "28 symbols in alphabet:\n",
      "['æ', 'n', 'i', 'q', 'e', 'j', 'k', 'o', 'f', 'w', 'd', 'x', 'u', 'z', 'v', 'b', 't', 'r', 'm', 'ç', 'g', 's', 'y', 'p', 'h', 'a', 'l', 'c']\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "\n",
    "text_1 = readfile('./data/36-0.txt',tokenize)\n",
    "text_2 = readfile('./data/2591-0.txt',tokenize)\n",
    "text = text_1 + text_2\n",
    "print(len(text),\"words overall\")\n",
    "word_corpus = list(set(text))\n",
    "print(len(word_corpus),\"unique words in corpus\")\n",
    "# Convert list of words to list of chars, get unique chars with set\n",
    "alphabet = list(set([c for word in word_corpus for c in word]))\n",
    "print(len(alphabet),\"symbols in alphabet:\")\n",
    "print(alphabet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με βάση το αλφάβητο που προέκυψε απο το word corpus δημιουργούμε το αρχείο συμβόλων εισόδου και εξόδου για τα fst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "def create_syms(alphabet):\n",
    "    f = open(\"./chars.syms\",\"w+\")\n",
    "    f.write(f'<epsilon>     0\\n')  #for <epsilon> index 0\n",
    "    for i in range(len(alphabet)):\n",
    "        f.write(f'{alphabet[i]}     {i+50}\\n')  #for other characters index i+50\n",
    "        \n",
    "create_syms(alphabet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ως μετρική για την απόσταση 2 λέξεων θα χρησιμοποιήσουμε την απόσταση Levenshtein (ή edit distance). Η απόσταση μπορέι να υπολογιστεί αναδρομικά με χρήση dynamic programming (DP). Ανάμεσα σε δύο λέξεις μπορούν να γίνουν τρείς τύποι μετατροπών: εισαγωγή ενός χαρακτήρα ($ \\epsilon \\rightarrow a$), μετατροπή ενός χαρακτήρα ($ a \\rightarrow b$) και διαγραφή ενός χαρακτήρα ($a \\rightarrow \\epsilon$). Θεωρούμε αρχικά ίδιο κόστος για όλες τις μετατροπές.\n",
    "\n",
    "Δημιουργούμε κατάλληλο fst με μία κατάσταση, αντιστοιχίζοντας κάθε χαρακτήρα σε κάθε άλλον και στο $\\epsilon$, και το $\\epsilon$ σε κάθε χαρακτήρα με βάρος 1. Αντιστοιχίζουμε ακόμα τον χαρακτήρα στον εαυτό του με βάρος 0. Αν πάρουμε το shortest path τότε η έξοδος θα είναι λέξη εισόδου, αφού το ελάχιστο βάρος προκύπτει αν δεν γίνει καμία μετατροπή. \n",
    "\n",
    "β) ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "\n",
    "# arc format: src dest ilabel olabel [weight]\n",
    "# final state format: state [weight]\n",
    "# lines may occur in any order except initial state must be first line\n",
    "# unspecified weights default to 0.0 (for the library-default Weight type) \n",
    "\n",
    "f = open('lev.fst',\"w+\")\n",
    "\n",
    "def format_arc(src,dst,src_sym,dst_sym,w,f,acceptor=False):\n",
    "    if not acceptor:\n",
    "        f.write(\"{} {} {} {} {} \\n\".format(src,dst,src_sym,dst_sym,w))\n",
    "    else:\n",
    "        f.write(\"{} {} {} \\n\".format(src,dst,src_sym))\n",
    "letters =  alphabet\n",
    "\n",
    "for i in range(0, len(letters)):\n",
    "    format_arc(src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=letters[i], w=1,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[i], w=0,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=\"<epsilon>\", w=1,f=f)\n",
    "    for j in range(0, len(letters)):\n",
    "        if(j!=i):\n",
    "            format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[j], w=1,f=f)   \n",
    "f.write('0\\n')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "#TODO: change the edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev.fst lev.bin.fst\n",
    "!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait lev.bin.fst | dot -Tjpg >lev.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ένας αποδοχέας είναι ένα fst όπου κάθε μετάβαση έχει ενα label (και προεραιτικά βάρος). Στην βιβλιοθήκη fst μπορέι να θεωρηθεί ως μετατροπέας με ίδιο input και output label. Κατασκευάζουμε εναν αποδοχέα που αποδέχεται κάθε λέξη του λεξικού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "\n",
    "\n",
    "def create_acceptor(words,file):\n",
    "    f = open(file,\"w+\")\n",
    "    s = 0\n",
    "    final_states = []\n",
    "    for word in words:\n",
    "        format_arc(0,s+1,word[0],word[0],0,f,acceptor=True)\n",
    "        s += 1\n",
    "        for letter in word[1:]:\n",
    "            format_arc(s,s+1,letter, letter, 0,f,acceptor=True)\n",
    "            s += 1\n",
    "        final_states.append(s)\n",
    "\n",
    "    for state in final_states:\n",
    "        f.write(f'{state}\\n')\n",
    "    f.close()    \n",
    "create_acceptor(word_corpus,\"acceptor.fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms --acceptor acceptor.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait rosebud.bin1.fst | dot -Tjpg >acceptor.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Τι κάνουν οι απο κατω συναρτήσεις απο την βιβλιοθήκη..... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6b\n",
    "\n",
    "!fstrmepsilon acceptor.bin.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait acceptor.bin.fst | dot -Tjpg >dfa_rmepsilon.jpg\n",
    "\n",
    "!fstdeterminize acceptor.bin.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait acceptor.bin.fst | dot -Tjpg >dfa_determinize.jpg\n",
    "\n",
    "!fstminimize acceptor.bin.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait acceptor.bin.fst | dot -Tjpg >dfa_minimize.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "#mini.fst - lev.fst\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev.bin.fst lev.bin.fst\n",
    "!fstarcsort --sort_type=ilabel acceptor.bin.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompose  lev.bin.fst acceptor.bin.fst spell_checker.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait spell_checker.bin.fst | dot -Tjpg >com.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_acceptor(['cit'],\"in.fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor in.fst in.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut"
     ]
    }
   ],
   "source": [
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tc\tc\r\n",
      "1\t2\ti\tu\t1\r\n",
      "2\t3\tt\tt\r\n",
      "3\r\n"
     ]
    }
   ],
   "source": [
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1   \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contented', 'contented', 'contented', 'contented']\n",
      "['contenpted', 'contende', 'contended', 'contentid']\n",
      "270\n",
      "270\n",
      "[133, 34, 43, 23, 138, 127, 260, 247, 29, 81, 221, 251, 188, 214, 130, 76, 1, 192, 110, 217]\n"
     ]
    }
   ],
   "source": [
    "# Step 8:\n",
    "import random\n",
    "random.seed()\n",
    "file = open('./data/spell_checker_test_set',\"r\")\n",
    "y_test = []\n",
    "X_test = []\n",
    "for line in file:\n",
    "    [y,X] = line.split(':')\n",
    "    X = X.split()\n",
    "    for word in X:\n",
    "        X_test.append(word)\n",
    "        y_test.append(y)\n",
    "\n",
    "print(y_test[0:4])\n",
    "print(X_test[0:4])\n",
    "print(len(y_test))\n",
    "print(len(X_test))\n",
    "idxs = random.sample(range(0, len(y_test)), 20)\n",
    "print(idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Y,X):\n",
    "    correct_pred=0\n",
    "    for y,x in zip(Y,X):\n",
    "        create_acceptor([x],\"input.fst\")\n",
    "\n",
    "        !fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor input.fst input.bin.fst\n",
    "        !fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "        !fstarcsort --sort_type=olabel input.bin.fst input.bin.fst \n",
    "        prediction = !fstcompose input.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "        | fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "        | cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n' \n",
    "        print(\"Wrong = \",x,\"-- Correct = \",y,\"-- Prediction = \",prediction[0])\n",
    "        if y == prediction[0]:\n",
    "            correct_pred+=1\n",
    "    print(f\"Accuracy:{correct_pred/len(Y)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong =  protend -- Correct =  pretend -- Prediction =  pretend\n",
      "Wrong =  unexpcted -- Correct =  unexpected -- Prediction =  expected\n",
      "Wrong =  biscuts -- Correct =  biscuits -- Prediction =  biscuits\n",
      "Wrong =  independant -- Correct =  independent -- Prediction =  independent\n",
      "Wrong =  wote -- Correct =  wrote -- Prediction =  note\n",
      "Wrong =  chaphter -- Correct =  chapter -- Prediction =  chapter\n",
      "Wrong =  adres -- Correct =  address -- Prediction =  agree\n",
      "Wrong =  oppisit -- Correct =  opposite -- Prediction =  opposite\n",
      "Wrong =  poety -- Correct =  poetry -- Prediction =  piety\n",
      "Wrong =  totaly -- Correct =  totally -- Prediction =  coaly\n",
      "Wrong =  failes -- Correct =  fails -- Prediction =  files\n",
      "Wrong =  oppossitte -- Correct =  opposite -- Prediction =  opposite\n",
      "Wrong =  planed -- Correct =  planned -- Prediction =  plane\n",
      "Wrong =  embaras -- Correct =  embarrass -- Prediction =  mars\n",
      "Wrong =  vairious -- Correct =  various -- Prediction =  various\n",
      "Wrong =  arnt -- Correct =  aunt -- Prediction =  ant\n",
      "Wrong =  contende -- Correct =  contented -- Prediction =  continue\n",
      "Wrong =  aranging -- Correct =  arrangeing -- Prediction =  hanging\n",
      "Wrong =  acount -- Correct =  account -- Prediction =  account\n",
      "Wrong =  auxillary -- Correct =  auxiliary -- Prediction =  artillery\n",
      "Accuracy:0.4%\n"
     ]
    }
   ],
   "source": [
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]\n",
    "predict(Y_rand,X_rand)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
