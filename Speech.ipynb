{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Επεξεργασία Φωνής και Φυσικής Γλώσσας</h1> \n",
    "<h2 align = \"center\">1η Προπαρασκευαστική Εργασία</h2> \n",
    "<h3 align = \"center\"> Θεoδωρόπουλος Νικήτας -03115185</h3>\n",
    "<h3 align = \"center\"> Καλλιώρα Δωροθέα - 03115176</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Σκοπός:</h3>\n",
    "Σκοπός της εργαστηριακής άσκησης είναι η δημιουργία ενός απλού ορθογράφου με χρήση μηχανών πεπερασμένων καταστάσεων (fst) με τη βοήθεια της βιβλιοθήκης openfst (v1.6.1). Θα χρησιμοποιήσουμε corpus απο δημόσια διαθέσιμα βιβλία για να δημιουργήσουμε ενα γλωσσικό μοντέλο σε μορφή λεξικού. Τέλος θα γίνει εισαγωγή σε αναπαραστάσεις word2vec. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> TODO </h1> Dori\n",
    "\n",
    "***Βήμα 1:***\n",
    "(α)\n",
    "Κατασκευάζουμε το corpus με σύμπτηξη plain text βιβλίων, δημόσια διαθέσημα στο Project Gutenberg. Θα χρησιμοποιήσουμε corpus 2 βιβλίων The War of the Worlds by H. G. Wells και Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm. \n",
    "\n",
    "(β)\n",
    "Η τεχνικής της συνένωσης πολλών βιβλίων, εκτός απο το προφανές πλεονέκτημα του μεγαλύτερου train corpus, αυξάνει την αναγνωριστική ισχύ του μοντέλου αφού μπορεί να αντλήσει λεξιλόγιο απο διαφορετικές πηγές. Ακόμα μειώνει το bias, αφού ένας συγραφέας μπορεί να χρησιμοποιεί συγκεκριμένες λέξεις λόγω προτίμησης, εποχής συγγραφής και άλλων παραγόντων. \n",
    "\n",
    "![./img/book36.cover.jpg](./img/book36.jpg)\n",
    "![./img/book36.cover.jpg](./img/book2591.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -P ./data/ http://www.gutenberg.org/files/36/36-0.txt\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/2591/2591-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 2***  Niki\n",
    "ΔΙαβαζουμε αρχειο κτλπ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OqT0WsSYSLb"
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "def identity_preprocess(str):\n",
    "  return str\n",
    "\n",
    "def readfile(path,preprocess=identity_preprocess):\n",
    "  processed_txt=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    processed_txt = processed_txt + preprocess(line)\n",
    "  return processed_txt \n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.strip().lower()\n",
    "    s = ''.join(c if c.isalpha() else ' ' for c in s)\n",
    "    # We replace all non-alpha characters with ' '\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom :\n",
      " ['at', 'eight', 'o', 'clock', 'on', 'thursday', 'morning', 'arthur', 'didn', 't', 'feel', 'very', 'good', 'he', 'quickly', 'rushed', 'to', 'the', 'doctor'] \n",
      "\n",
      "nltk punkt :\n",
      " ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', '.Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'He', 'quickly', 'rushed', 'to', 'the', 'Doctor', '!'] \n",
      "\n",
      "nltk sentece detector :\n",
      " [\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\", 'He quickly rushed to the Doctor!'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/dorotheakal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Step 2 (d)\n",
    "\n",
    "#!pip install --user -U nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\n",
    "              He quickly rushed to the Doctor!\"\"\"\n",
    "tokenizers = [tokenize,nltk.word_tokenize,sent_detector.tokenize]\n",
    "names = [\"custom\",\"nltk punkt\",\"nltk sentece detector\"]\n",
    "for tokenizer,name in zip(tokenizers,names):\n",
    "    print(name,\":\\n\",tokenizer(sentence),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 2*** dori\n",
    "\n",
    "(δ) Συμπεράσματα και συγκρίσεις:\n",
    "\n",
    "Sentence Tokenizer\n",
    "\n",
    "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
    "\n",
    "Word punkt\n",
    "Tokenize a string to split off punctuation other than periods\n",
    "\n",
    "\n",
    "Tokenize() //diko mas\n",
    "na to grapsoume\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1574168112504,
     "user": {
      "displayName": "Dorothea Kalliora",
      "photoUrl": "",
      "userId": "17412342597228224634"
     },
     "user_tz": -120
    },
    "id": "YXntX-DgfvSF",
    "outputId": "19b454d9-ca2c-4be2-cb8c-bfe654403163",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169392 words overall\n",
      "3540\n",
      "9288 unique words in corpus\n",
      "28 symbols in alphabet:\n",
      "['y', 'w', 'l', 'ç', 'm', 'b', 'e', 'k', 'v', 't', 'r', 'o', 'x', 'd', 'æ', 'i', 'q', 'u', 'g', 'a', 'f', 'h', 'z', 'c', 's', 'n', 'j', 'p']\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "import random\n",
    "random.seed()\n",
    "\n",
    "text_1 = readfile('./data/36-0.txt',tokenize)\n",
    "text_2 = readfile('./data/2591-0.txt',tokenize)\n",
    "text = text_1 + text_2\n",
    "\n",
    "print(len(text),\"words overall\")\n",
    "word_corpus = list(set(text))\n",
    "print(len(word_corpus),\"unique words in corpus\")\n",
    "\n",
    "# Convert list of words to list of chars, get unique chars with set\n",
    "alphabet = list(set([c for word in word_corpus for c in word]))\n",
    "print(len(alphabet),\"symbols in alphabet:\")\n",
    "print(alphabet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με βάση το αλφάβητο που προέκυψε απο το word corpus δημιουργούμε το αρχείο συμβόλων εισόδου και εξόδου για τα fst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "def create_syms(alphabet):\n",
    "    f = open(\"./chars.syms\",\"w+\")\n",
    "    f.write(f'<epsilon>     0\\n')  #for <epsilon> index 0\n",
    "    for i in range(len(alphabet)):\n",
    "        f.write(f'{alphabet[i]}     {i+50}\\n')  #for other characters index i+50\n",
    "        \n",
    "create_syms(alphabet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 5*** niki\n",
    "\n",
    "(α)Ως μετρική για την απόσταση 2 λέξεων θα χρησιμοποιήσουμε την απόσταση Levenshtein (ή edit distance). Η απόσταση μπορέι να υπολογιστεί αναδρομικά με χρήση dynamic programming (DP). Ανάμεσα σε δύο λέξεις μπορούν να γίνουν τρείς τύποι μετατροπών: εισαγωγή ενός χαρακτήρα ($ \\epsilon \\rightarrow a$), μετατροπή ενός χαρακτήρα ($ a \\rightarrow b$) και διαγραφή ενός χαρακτήρα ($a \\rightarrow \\epsilon$). Θεωρούμε αρχικά ίδιο κόστος για όλες τις μετατροπές.\n",
    "\n",
    "Δημιουργούμε κατάλληλο fst με μία κατάσταση, αντιστοιχίζοντας κάθε χαρακτήρα σε κάθε άλλον και στο $\\epsilon$, και το $\\epsilon$ σε κάθε χαρακτήρα με βάρος 1. Αντιστοιχίζουμε ακόμα τον χαρακτήρα στον εαυτό του με βάρος 0. Αν πάρουμε το shortest path τότε η έξοδος θα είναι λέξη εισόδου, αφού το ελάχιστο βάρος προκύπτει αν δεν γίνει καμία μετατροπή. \n",
    "\n",
    "(β)Στην υλοποίηση που έγινε για το ερώτημα 5 έχουμε υποθέσει ότι όλα τα edits έχουν ίσο βάρος. Αυτή ειναι μια αφελής υπόθεση αφού κάθε είδος λάθους (παράλλειψη, επανάληψη ή λάθος τοποθέτηση γράμματος)  έχει την ίδια πιθανότητα εμφάνισης. Ιδανικά θα θέλαμε να γνωρίζουμε την κατανομή του λάθους, την οποία μπορούμε να εκτιμήσουμε απο ένα σύνολο δεδομένων. Για ενα ζεύγος (σωστή λέξη, λανθασμένη λέξη) μπορούμε να υπολογίσουμε τη διόρθωση που απαιτείται και η πιθανότητα θα προκύψει $ P[ a\\rightarrow b] = \\frac{|error = b \\rightarrow a|}{|errors|}, \\forall a,b \\in \\{A+\\epsilon\\} $, όπου $A$ το αλφάβητο. Αυτές εινα οι a priori πιθανότητες λαθών με βάση το training data. Αυτές μπορούν να χρησιμοποιηθούν για να υπολιστούν τα βάρη στο μοντέλο αναγνώρισης μας με fst, ώστε να είναι αντιστρόφως ανάλογα της πιθανότητας. Μια καλή συνάρτηση αντιστοίχησης που διατηρεί τις αναλογίες είναι η $\\frac{1}{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "\n",
    "# arc format: src dest ilabel olabel [weight]\n",
    "# final state format: state [weight]\n",
    "# lines may occur in any order except initial state must be first line\n",
    "# unspecified weights default to 0.0 (for the library-default Weight type) \n",
    "\n",
    "f = open('lev.fst',\"w+\")\n",
    "\n",
    "def format_arc(src,dst,src_sym,dst_sym,w,f,acceptor=False):\n",
    "    if not acceptor:\n",
    "        f.write(\"{} {} {} {} {} \\n\".format(src,dst,src_sym,dst_sym,w))\n",
    "    else:\n",
    "        f.write(\"{} {} {} \\n\".format(src,dst,src_sym))\n",
    "letters =  alphabet\n",
    "\n",
    "for i in range(0, len(letters)):\n",
    "    format_arc(src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=letters[i], w=1,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[i], w=0,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=\"<epsilon>\", w=1,f=f)\n",
    "    for j in range(0, len(letters)):\n",
    "        if(j!=i):\n",
    "            format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[j], w=1,f=f)   \n",
    "f.write('0\\n')\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev.fst lev.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait lev.bin.fst | dot -Tjpg >lev.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ένας αποδοχέας είναι ένα fst όπου κάθε μετάβαση έχει ενα label (και προεραιτικά βάρος). Στην βιβλιοθήκη fst μπορέι να θεωρηθεί ως μετατροπέας με ίδιο input και output label. Κατασκευάζουμε εναν αποδοχέα που αποδέχεται κάθε λέξη του λεξικού. dori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "\n",
    "def create_acceptor(words,file):\n",
    "    f = open(file,\"w+\")\n",
    "    s = 0\n",
    "    final_states = []\n",
    "    for word in words:\n",
    "        format_arc(0,s+1,word[0],word[0],0,f,acceptor=True)\n",
    "        s += 1\n",
    "        for letter in word[1:]:\n",
    "            format_arc(s,s+1,letter, letter, 0,f,acceptor=True)\n",
    "            s += 1\n",
    "        final_states.append(s)\n",
    "\n",
    "    for state in final_states:\n",
    "        f.write(f'{state}\\n')\n",
    "    f.close()    \n",
    "create_acceptor(word_corpus,\"acceptor.fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms --acceptor acceptor.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait rosebud.bin1.fst | dot -Tjpg >acceptor.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Τι κάνουν οι απο κατω συναρτήσεις απο την βιβλιοθήκη..... niki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6b\n",
    "\n",
    "!fstrmepsilon acceptor.bin.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait acceptor.bin.fst | dot -Tjpg >dfa_rmepsilon.jpg\n",
    "\n",
    "!fstdeterminize acceptor.bin.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait acceptor.bin.fst | dot -Tjpg >dfa_determinize.jpg\n",
    "\n",
    "!fstminimize acceptor.bin.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait acceptor.bin.fst | dot -Tjpg >dfa_minimize.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "#mini.fst - lev.fst\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev.bin.fst lev.bin.fst\n",
    "!fstarcsort --sort_type=ilabel acceptor.bin.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompose  lev.bin.fst acceptor.bin.fst spell_checker.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait spell_checker.bin.fst | dot -Tjpg >com.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_acceptor(['cit'],\"in.fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor in.fst in.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut"
     ]
    }
   ],
   "source": [
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tc\tc\r\n",
      "1\t2\ti\tu\t1\r\n",
      "2\t3\tt\tt\r\n",
      "3\r\n"
     ]
    }
   ],
   "source": [
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1   \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contented', 'contented', 'contented', 'contented']\n",
      "['contenpted', 'contende', 'contended', 'contentid']\n",
      "270\n",
      "270\n",
      "[133, 34, 43, 23, 138, 127, 260, 247, 29, 81, 221, 251, 188, 214, 130, 76, 1, 192, 110, 217]\n"
     ]
    }
   ],
   "source": [
    "# Step 8:\n",
    "\n",
    "file = open('./data/spell_checker_test_set',\"r\")\n",
    "y_test = []\n",
    "X_test = []\n",
    "for line in file:\n",
    "    [y,X] = line.split(':')\n",
    "    X = X.split()\n",
    "    for word in X:\n",
    "        X_test.append(word)\n",
    "        y_test.append(y)\n",
    "\n",
    "print(y_test[0:4])\n",
    "print(X_test[0:4])\n",
    "print(len(y_test))\n",
    "print(len(X_test))\n",
    "idxs = random.sample(range(0, len(y_test)), 20)\n",
    "print(idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Y,X):\n",
    "    correct_pred=0\n",
    "    for y,x in zip(Y,X):\n",
    "        create_acceptor([x],\"input.fst\")\n",
    "\n",
    "        !fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor input.fst input.bin.fst\n",
    "        !fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "        !fstarcsort --sort_type=olabel input.bin.fst input.bin.fst \n",
    "        prediction = !fstcompose input.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "        | fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "        | cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n' \n",
    "        print(\"Wrong = \",x,\"-- Correct = \",y,\"-- Prediction = \",prediction[0])\n",
    "        if y == prediction[0]:\n",
    "            correct_pred+=1\n",
    "    print(f\"Accuracy:{correct_pred/len(Y)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong =  protend -- Correct =  pretend -- Prediction =  pretend\n",
      "Wrong =  unexpcted -- Correct =  unexpected -- Prediction =  expected\n",
      "Wrong =  biscuts -- Correct =  biscuits -- Prediction =  biscuits\n",
      "Wrong =  independant -- Correct =  independent -- Prediction =  independent\n",
      "Wrong =  wote -- Correct =  wrote -- Prediction =  note\n",
      "Wrong =  chaphter -- Correct =  chapter -- Prediction =  chapter\n",
      "Wrong =  adres -- Correct =  address -- Prediction =  agree\n",
      "Wrong =  oppisit -- Correct =  opposite -- Prediction =  opposite\n",
      "Wrong =  poety -- Correct =  poetry -- Prediction =  piety\n",
      "Wrong =  totaly -- Correct =  totally -- Prediction =  coaly\n",
      "Wrong =  failes -- Correct =  fails -- Prediction =  files\n",
      "Wrong =  oppossitte -- Correct =  opposite -- Prediction =  opposite\n",
      "Wrong =  planed -- Correct =  planned -- Prediction =  plane\n",
      "Wrong =  embaras -- Correct =  embarrass -- Prediction =  mars\n",
      "Wrong =  vairious -- Correct =  various -- Prediction =  various\n",
      "Wrong =  arnt -- Correct =  aunt -- Prediction =  ant\n",
      "Wrong =  contende -- Correct =  contented -- Prediction =  continue\n",
      "Wrong =  aranging -- Correct =  arrangeing -- Prediction =  hanging\n",
      "Wrong =  acount -- Correct =  account -- Prediction =  account\n",
      "Wrong =  auxillary -- Correct =  auxiliary -- Prediction =  artillery\n",
      "Accuracy:0.4%\n"
     ]
    }
   ],
   "source": [
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]\n",
    "predict(Y_rand,X_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13945\n"
     ]
    }
   ],
   "source": [
    "#Step 9\n",
    "def tokenized_list(path,preprocess=identity_preprocess):\n",
    "  list_of_sentences=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    l = preprocess(line)\n",
    "    if l:\n",
    "        list_of_sentences.append(l)\n",
    "  return list_of_sentences\n",
    "\n",
    "\n",
    "list1 = tokenized_list('./data/36-0.txt',tokenize)\n",
    "list2 = tokenized_list('./data/2591-0.txt',tokenize)\n",
    "final_list = list1 + list2 \n",
    "print(len(final_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.10.25)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.25 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.25)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.25->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /home/dorotheakal/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.25->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "!pip install -U gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108772021, 169392000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(final_list, window=5, size=100, workers=4)\n",
    "model.train(final_list, total_examples=len(final_list), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2811\n",
      "100\n",
      "[('throwing', 0.3889895975589752), ('tore', 0.3638570308685303), ('threw', 0.35819464921951294), ('took', 0.34865516424179077), ('pay', 0.34183257818222046), ('set', 0.3053753972053528), ('gave', 0.3050578534603119), ('take', 0.3020142614841461), ('broke', 0.30198562145233154), ('bring', 0.2947586178779602)]\n",
      "Most similar words to \"greenish\":\n",
      "     \"vapour\" -- sim: 0.40728604793548584\n",
      "     \"violent\" -- sim: 0.3979704976081848\n",
      "     \"puffs\" -- sim: 0.39006245136260986\n",
      "     \"foam\" -- sim: 0.38760900497436523\n",
      "     \"whirling\" -- sim: 0.3635241389274597\n",
      "     \"haze\" -- sim: 0.3582707345485687\n",
      "     \"flag\" -- sim: 0.35365885496139526\n",
      "     \"gigantic\" -- sim: 0.3512701988220215\n",
      "     \"green\" -- sim: 0.3445907235145569\n",
      "     \"tumult\" -- sim: 0.341741681098938\n",
      "Most similar words to \"everybody\":\n",
      "     \"rubbish\" -- sim: 0.3829894959926605\n",
      "     \"health\" -- sim: 0.3715789318084717\n",
      "     \"defect\" -- sim: 0.35363760590553284\n",
      "     \"provide\" -- sim: 0.3510478138923645\n",
      "     \"poured\" -- sim: 0.3349335789680481\n",
      "     \"runs\" -- sim: 0.32348814606666565\n",
      "     \"yes\" -- sim: 0.3179360628128052\n",
      "     \"pyrford\" -- sim: 0.30697059631347656\n",
      "     \"memory\" -- sim: 0.3007999658584595\n",
      "     \"writhing\" -- sim: 0.2983589172363281\n",
      "Most similar words to \"paper\":\n",
      "     \"boat\" -- sim: 0.35685521364212036\n",
      "     \"emerged\" -- sim: 0.34712278842926025\n",
      "     \"sunset\" -- sim: 0.34368669986724854\n",
      "     \"destruction\" -- sim: 0.3403950333595276\n",
      "     \"violence\" -- sim: 0.3338928818702698\n",
      "     \"verge\" -- sim: 0.3328583836555481\n",
      "     \"several\" -- sim: 0.33161038160324097\n",
      "     \"friday\" -- sim: 0.3285578191280365\n",
      "     \"ordinary\" -- sim: 0.3208448886871338\n",
      "     \"floor\" -- sim: 0.31921952962875366\n",
      "Most similar words to \"rusty\":\n",
      "     \"ruddy\" -- sim: 0.4271893799304962\n",
      "     \"brightly\" -- sim: 0.3717888593673706\n",
      "     \"shone\" -- sim: 0.34140801429748535\n",
      "     \"sweeping\" -- sim: 0.33768147230148315\n",
      "     \"steamer\" -- sim: 0.32472744584083557\n",
      "     \"bulk\" -- sim: 0.3104298412799835\n",
      "     \"shell\" -- sim: 0.30353426933288574\n",
      "     \"whistled\" -- sim: 0.2968270182609558\n",
      "     \"looks\" -- sim: 0.2947840094566345\n",
      "     \"flames\" -- sim: 0.293163925409317\n",
      "Most similar words to \"hills\":\n",
      "     \"crest\" -- sim: 0.4007473886013031\n",
      "     \"meadows\" -- sim: 0.38260114192962646\n",
      "     \"batteries\" -- sim: 0.3821810185909271\n",
      "     \"fire\" -- sim: 0.37467968463897705\n",
      "     \"boats\" -- sim: 0.35955747961997986\n",
      "     \"abruptly\" -- sim: 0.3501310646533966\n",
      "     \"striding\" -- sim: 0.34885483980178833\n",
      "     \"dove\" -- sim: 0.34749922156333923\n",
      "     \"thames\" -- sim: 0.3460198938846588\n",
      "     \"dimly\" -- sim: 0.34056222438812256\n",
      "Most similar words to \"way\":\n",
      "     \"journey\" -- sim: 0.36014801263809204\n",
      "     \"brother\" -- sim: 0.33746975660324097\n",
      "     \"peas\" -- sim: 0.31522098183631897\n",
      "     \"step\" -- sim: 0.31197065114974976\n",
      "     \"tentacles\" -- sim: 0.3049921989440918\n",
      "     \"directly\" -- sim: 0.30411702394485474\n",
      "     \"break\" -- sim: 0.30044683814048767\n",
      "     \"steady\" -- sim: 0.2980971336364746\n",
      "     \"grass\" -- sim: 0.2979462742805481\n",
      "     \"passengers\" -- sim: 0.2902963161468506\n",
      "Most similar words to \"shook\":\n",
      "     \"swaying\" -- sim: 0.3138508200645447\n",
      "     \"therefore\" -- sim: 0.3031589984893799\n",
      "     \"crash\" -- sim: 0.30286312103271484\n",
      "     \"fetched\" -- sim: 0.301441490650177\n",
      "     \"drop\" -- sim: 0.30098050832748413\n",
      "     \"around\" -- sim: 0.2941461503505707\n",
      "     \"swallowed\" -- sim: 0.28200656175613403\n",
      "     \"planted\" -- sim: 0.2799552381038666\n",
      "     \"fresh\" -- sim: 0.27311521768569946\n",
      "     \"sank\" -- sim: 0.2722868025302887\n",
      "Most similar words to \"struck\":\n",
      "     \"swallowed\" -- sim: 0.34964466094970703\n",
      "     \"jumped\" -- sim: 0.3329952657222748\n",
      "     \"stroke\" -- sim: 0.32434993982315063\n",
      "     \"dragged\" -- sim: 0.32128098607063293\n",
      "     \"blinded\" -- sim: 0.31246933341026306\n",
      "     \"renewed\" -- sim: 0.3068656325340271\n",
      "     \"approach\" -- sim: 0.3040810227394104\n",
      "     \"knocked\" -- sim: 0.29959237575531006\n",
      "     \"touched\" -- sim: 0.2985570728778839\n",
      "     \"dressed\" -- sim: 0.29498207569122314\n",
      "Most similar words to \"warranties\":\n",
      "     \"limitation\" -- sim: 0.5495449304580688\n",
      "     \"status\" -- sim: 0.5390787124633789\n",
      "     \"copies\" -- sim: 0.48910728096961975\n",
      "     \"damages\" -- sim: 0.4863622784614563\n",
      "     \"owner\" -- sim: 0.46706753969192505\n",
      "     \"displaying\" -- sim: 0.44467657804489136\n",
      "     \"compliance\" -- sim: 0.4437751770019531\n",
      "     \"liability\" -- sim: 0.43868303298950195\n",
      "     \"tax\" -- sim: 0.4299894869327545\n",
      "     \"including\" -- sim: 0.42516273260116577\n",
      "Most similar words to \"fortune\":\n",
      "     \"opportunity\" -- sim: 0.38670429587364197\n",
      "     \"loss\" -- sim: 0.3740890324115753\n",
      "     \"luck\" -- sim: 0.37289485335350037\n",
      "     \"advice\" -- sim: 0.35160142183303833\n",
      "     \"sister\" -- sim: 0.3392103314399719\n",
      "     \"answer\" -- sim: 0.33167555928230286\n",
      "     \"mayor\" -- sim: 0.32358822226524353\n",
      "     \"pearls\" -- sim: 0.32050323486328125\n",
      "     \"use\" -- sim: 0.3174884021282196\n",
      "     \"loudly\" -- sim: 0.31318873167037964\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "print(len(voc))\n",
    "# get vector size\n",
    "dim = model.vector_size\n",
    "print(dim)\n",
    "# get most similar words\n",
    "sim = model.wv.most_similar('cut')\n",
    "print(sim)\n",
    "\n",
    "# Convert to numpy 2d array (n_vocab x vector_size)\n",
    "def to_embeddings_Matrix(model):  \n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n",
    "    word2idx = {}\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_matrix[i] = model.wv[model.wv.index2word[i]]\n",
    "        word2idx[model.wv.index2word[i]] = i\n",
    "    return embedding_matrix, model.wv.index2word, word2idx\n",
    "\n",
    "# word2idx : \"word\" -> index\n",
    "# embedding_matrix : index -> embedding \n",
    "# model.wv.index2word : index -> \"word\"\n",
    "\n",
    "idxs = random.sample(range(0, len(voc)), 10)\n",
    "rand_words = [voc[i] for i in idxs]\n",
    "for word in rand_words:\n",
    "    print(f'Most similar words to \"{word}\":')\n",
    "    for word,sim in model.wv.most_similar(word):\n",
    "        print(f'     \"{word}\" -- sim: {sim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#to_embeddings_Matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
