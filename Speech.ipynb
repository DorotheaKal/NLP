{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Επεξεργασία Φωνής και Φυσικής Γλώσσας \n",
    "### 1η Προπαρασκευαστική Εργασία \n",
    "### Θεδωρόπουλος Νικήτας -03115185\n",
    "### Καλλιώρα Δωροθέα - 03115176 ? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκοπός της εργαστηριακής άσκησης είναι η δημιουργία ενός απλού ορθογράφου με χρήση μηχανών πεπερασμένων καταστάσεων (fst) της βιβλιοθήκης openfst (v1.6.1). Θα χρησιμοποιήσουμε corpus απο δημόσια διαθέσιμα βιβλία για να δημιουργήσουμε ενα γλωσσικό μοντέλο σε μορφή λεξικού. Τέλος θα γίνει εισαγωγή σε αναπαραστάσεις word2vec. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κατασκευάζουμε το corpus με σύμπτηξη plain text βιβλίων, δημόσια διαθέσημα στο Project Gutenberg. Θα χρησιμοποιήσουμε corpus 2 βιβλίων The War of the Worlds by H. G. Wells και Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm. \n",
    "\n",
    "Η τεχνικής της συνένωσης πολλών βιβλίων, εκτός απο το προφανές πλεονέκτημα του μεγαλύτερου train corpus, αυξάνει την αναγνωριστική ισχύ του μοντέλου αφού μπορεί να αντλήσει λεξιλόγειο απο διαφορετικές πηγές και άρα να έχει μεγαλύτερο corpus λέξεων.Ακόμα μειώνει το bias που προκύπτει απο ενα συγκεκριμένο βιβλίο ή απο έναν συγκεκριμένο συγγραφέα, που χρησιμοποιεί συγκεκριμένες λέξεις λόγω προτίμησης, εποχής συγγραφής και άλλων παραγόντων. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-22 14:18:33--  http://www.gutenberg.org/files/36/36-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 366828 (358K) [text/plain]\n",
      "Saving to: ‘./data/36-0.txt.1’\n",
      "\n",
      "36-0.txt.1          100%[===================>] 358,23K   321KB/s    in 1,1s    \n",
      "\n",
      "2019-11-22 14:18:35 (321 KB/s) - ‘./data/36-0.txt.1’ saved [366828/366828]\n",
      "\n",
      "--2019-11-22 14:18:35--  http://www.gutenberg.org/files/2591/2591-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 560162 (547K) [text/plain]\n",
      "Saving to: ‘./data/2591-0.txt’\n",
      "\n",
      "2591-0.txt          100%[===================>] 547,03K   454KB/s    in 1,2s    \n",
      "\n",
      "2019-11-22 14:18:36 (454 KB/s) - ‘./data/2591-0.txt’ saved [560162/560162]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P ./data/ http://www.gutenberg.org/files/36/36-0.txt\n",
    "!wget -P ./data/ http://www.gutenberg.org/files/2591/2591-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OqT0WsSYSLb"
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "def identity_preprocess(str):\n",
    "  return str\n",
    "\n",
    "def readfile(path,preprocess=identity_preprocess):\n",
    "  processed_txt=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    processed_txt= processed_txt + preprocess(line)\n",
    "  return processed_txt \n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.strip().lower()\n",
    "    s = ''.join(c if c.isalpha() else ' ' for c in s)\n",
    "    # We replace all non-alpha characters with ' '\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.split()\n",
    "    return s\n",
    "  \n",
    "# TODO: experiment with ntk tokenizers \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1574168112504,
     "user": {
      "displayName": "Dorothea Kalliora",
      "photoUrl": "",
      "userId": "17412342597228224634"
     },
     "user_tz": -120
    },
    "id": "YXntX-DgfvSF",
    "outputId": "19b454d9-ca2c-4be2-cb8c-bfe654403163",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169392 words overall\n",
      "9288 unique words in corpus\n",
      "28 symbols in alphabet\n",
      "['æ', 'n', 'i', 'q', 'e', 'k', 'j', 'o', 'f', 'w', 'd', 'x', 'u', 'z', 'v', 'b', 't', 'r', 'm', 'ç', 'g', 's', 'y', 'p', 'h', 'a', 'l', 'c']\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "\n",
    "text_1 = readfile('./data/36-0.txt',tokenize)\n",
    "text_2 = readfile('./data/2591-0.txt',tokenize)\n",
    "text = text_1 + text_2\n",
    "print(len(text),\"words overall\")\n",
    "word_corpus = list(set(text))\n",
    "print(len(word_corpus),\"unique words in corpus\")\n",
    "# Convert list of words to list of chars, get unique chars with set\n",
    "alphabet = list(set([c for word in word_corpus for c in word]))\n",
    "print(len(alphabet),\"symbols in alphabet:\")\n",
    "print(alphabet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με βάση το αλφάβητο που προέκυψε απο το word corpus δημιουργούμε το αρχείο συμβόλων εισόδου και εξόδου για τα fst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "def create_syms(alphabet):\n",
    "    f = open(\"./chars.syms\",\"w+\")\n",
    "    f.write(f'<epsilon>     0\\n')  #for <epsilon> index 0\n",
    "    for i in range(len(alphabet)):\n",
    "        f.write(f'{alphabet[i]}     {i+50}\\n')  #for other characters index i+50\n",
    "        \n",
    "create_syms(alphabet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ως μετρική για την απόσταση 2 λέξεων θα χρησιμοποιήσουμε την απόσταση Levenshtein (ή edit distance). Η απόσταση μπορέι να υπολογιστεί αναδρομικά με χρήση dynamic programming (DP). Ανάμεσα σε δύο λέξεις μπορούν να γίνουν τρείς τύποι μετατροπών: εισαγωγή ενός χαρακτήρα ($ \\epsilon \\rightarrow a$), μετατροπή ενός χαρακτήρα ($ a \\rightarrow b$) και διαγραφή ενός χαρακτήρα ($a \\rightarrow \\epsilon$). Θεωρούμε αρχικά ίδιο κόστος για όλες τις μετατροπές.\n",
    "\n",
    "Δημιουργούμε κατάλληλο fst με μία κατάσταση, αντιστοιχίζοντας κάθε χαρακτήρα σε κάθε άλλον και στο $\\epsilon$, και το $\\epsilon$ σε κάθε χαρακτήρα με βάρος 1. Αντιστοιχίζουμε ακόμα τον χαρακτήρα στον εαυτό του με βάρος 0. Αν πάρουμε το shortest path τότε η έξοδος θα είναι λέξη εισόδου, αφού το ελάχιστο βάρος προκύπτει αν δεν γίνει καμία μετατροπή. \n",
    "\n",
    "β) ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "\n",
    "# arc format: src dest ilabel olabel [weight]\n",
    "# final state format: state [weight]\n",
    "# lines may occur in any order except initial state must be first line\n",
    "# unspecified weights default to 0.0 (for the library-default Weight type) \n",
    "\n",
    "f = open('lev.fst',\"w+\")\n",
    "\n",
    "def format_arc(src,dst,src_sym,dst_sym,w,f,acceptor=False):\n",
    "    if not acceptor:\n",
    "        f.write(\"{} {} {} {} {} \\n\".format(src,dst,src_sym,dst_sym,w))\n",
    "    else:\n",
    "        f.write(\"{} {} {} \\n\".format(src,dst,src_sym))\n",
    "letters =  alphabet\n",
    "\n",
    "for i in range(0, len(letters)):\n",
    "    format_arc(src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=letters[i], w=1,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[i], w=0,f=f)\n",
    "    format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=\"<epsilon>\", w=1,f=f)\n",
    "    for j in range(0, len(letters)):\n",
    "        if(j!=i):\n",
    "            format_arc(src=0, dst=0, src_sym=letters[i], dst_sym=letters[j], w=1,f=f)   \n",
    "f.write('0\\n')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "#TODO: change the edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev.fst lev.bin.fst\n",
    "!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait lev.bin.fst | dot -Tjpg >lev.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ένας αποδοχέας είναι ένα fst όπου κάθε μετάβαση έχει ενα label (και προεραιτικά βάρος). Στην βιβλιοθήκη fst μπορέι να θεωρηθεί ως μετατροπέας με ίδιο input και output label. Κατασκευάζουμε εναν αποδοχέα που αποδέχεται κάθε λέξη του λεξικού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "\n",
    "f = open(\"acceptor.fst\",\"w+\")\n",
    "s = 0\n",
    "final_states = []\n",
    "word_corpus_cut = word_corpus\n",
    "for word in word_corpus_cut:\n",
    "    format_arc(0,s+1,word[0],_,_,f,acceptor=True)\n",
    "    s += 1\n",
    "    for letter in word[1:]:\n",
    "        format_arc(s,s+1,letter,_, _,f,acceptor=True)\n",
    "        s += 1\n",
    "    final_states.append(s)\n",
    "    \n",
    "for state in final_states:\n",
    "    f.write(f'{state}\\n')\n",
    "    \n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms --acceptor acceptor.fst acceptor.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait rosebud.bin1.fst | dot -Tjpg >acceptor.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Τι κάνουν οι απο κατω συναρτήσεις απο την βιβλιοθήκη..... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6b\n",
    "\n",
    "!fstrmepsilon acc.bin.fst eps.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait eps.fst | dot -Tjpg >dfa_rmepsilon.jpg\n",
    "\n",
    "!fstdeterminize eps.fst det.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait det.fst | dot -Tjpg >dfa_determinize.jpg\n",
    "\n",
    "!fstminimize det.fst mini.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait mini.fst | dot -Tjpg >dfa_minimize.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "#mini.fst - lev.fst\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev.bin.fst lev_sorted.bin.fst\n",
    "!fstarcsort --sort_type=ilabel mini.fst mini.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompose  lev_sorted.bin.fst mini.fst composed.bin.fst\n",
    "#!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait composed.bin.fst | dot -Tjpg >com.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open(\"input.fst\",\"w+\")\n",
    "s = 0\n",
    "final_states = []\n",
    "word_corpus_cut = ['cit']\n",
    "for word in word_corpus_cut:\n",
    "    format_arc(0,s+1,word[0],word[0],0,f,acceptor=True)\n",
    "    s += 1\n",
    "    for letter in word[1:]:\n",
    "        format_arc(s,s+1,letter, letter, 0,f,acceptor=True)\n",
    "        s += 1\n",
    "    final_states.append(s)\n",
    "    \n",
    "for state in final_states:\n",
    "    f.write(f'{state}\\n')\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor input.fst I.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstarcsort --sort_type=ilabel composed.bin.fst s_composed.bin.fst \n",
    "!fstarcsort --sort_type=olabel I.bin.fst s_I.bin.fst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pit"
     ]
    }
   ],
   "source": [
    "!fstcompose s_I.bin.fst s_composed.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tc\tp\t1\r\n",
      "1\t2\ti\ti\r\n",
      "2\t3\tt\tt\r\n",
      "3\r\n"
     ]
    }
   ],
   "source": [
    "!fstcompose s_I.bin.fst s_composed.bin.fst |fstshortestpath --nshortest=1   \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contented', 'contented', 'contented', 'contented']\n",
      "['contenpted', 'contende', 'contended', 'contentid']\n",
      "270\n",
      "270\n",
      "[161, 251, 84, 82, 47, 119, 260, 257, 52, 232, 108, 222, 45, 100, 193, 143, 261, 177, 42, 121]\n"
     ]
    }
   ],
   "source": [
    "# Step 8:\n",
    "import random\n",
    "\n",
    "file = open('./data/spell_checker_test_set',\"r\")\n",
    "y_test = []\n",
    "X_test = []\n",
    "for line in file:\n",
    "    [y,X] = line.split(':')\n",
    "    X = X.split()\n",
    "    for word in X:\n",
    "        X_test.append(word)\n",
    "        y_test.append(y)\n",
    "\n",
    "print(y_test[0:4])\n",
    "print(X_test[0:4])\n",
    "print(len(y_test))\n",
    "print(len(X_test))\n",
    "idxs = random.sample(range(0, len(y_test)), 20)\n",
    "print(idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Y,X):\n",
    "    for y,x in zip(Y,X):\n",
    "        \n",
    "        f = open(\"input.fst\",\"w\")\n",
    "        s = 0\n",
    "        word = x\n",
    "        final_states = []\n",
    "        format_arc(0,s+1,word[0],word[0],0,f,acceptor=True)\n",
    "        s += 1\n",
    "        for letter in word[1:]:\n",
    "            format_arc(s,s+1,letter, letter, 0,f,acceptor=True)\n",
    "            s += 1\n",
    "        final_states.append(s)\n",
    "\n",
    "        for state in final_states:\n",
    "            f.write(f'{state}\\n')\n",
    "        f.close()    \n",
    "        !fstcompile  --isymbols=chars.syms --osymbols=chars.syms  --acceptor input.fst I.bin.fst\n",
    "        !fstarcsort --sort_type=ilabel composed.bin.fst s_composed.bin.fst \n",
    "        !fstarcsort --sort_type=olabel I.bin.fst s_I.bin.fst \n",
    "        prediction = !fstcompose s_I.bin.fst s_composed.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "        | fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "        | cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n' \n",
    "        print(\"Wrong = \",x,\"-- Correct = \",y,\"-- Prediction = \",prediction[0])\n",
    "        correct_pred=0\n",
    "        if y == prediction[0]:\n",
    "            correct_pred+=1\n",
    "    print(f\"Accuracy:{correct_pred/len(Y)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong =  perpul -- Correct =  purple -- Prediction =  peril\n",
      "Wrong =  oppossitte -- Correct =  opposite -- Prediction =  opposite\n",
      "Wrong =  southen -- Correct =  southern -- Prediction =  southern\n",
      "Wrong =  undersand -- Correct =  understand -- Prediction =  understand\n",
      "Wrong =  avaible -- Correct =  available -- Prediction =  audible\n",
      "Wrong =  latiest -- Correct =  latest -- Prediction =  latest\n",
      "Wrong =  adres -- Correct =  address -- Prediction =  are\n",
      "Wrong =  curtians -- Correct =  curtains -- Prediction =  martians\n",
      "Wrong =  necassary -- Correct =  necessary -- Prediction =  necessary\n",
      "Wrong =  extreamly -- Correct =  extremely -- Prediction =  extremely\n",
      "Wrong =  beetween -- Correct =  between -- Prediction =  between\n",
      "Wrong =  carrer -- Correct =  career -- Prediction =  carrier\n",
      "Wrong =  buiscits -- Correct =  biscuits -- Prediction =  biscuits\n",
      "Wrong =  desicate -- Correct =  desiccate -- Prediction =  delicate\n",
      "Wrong =  accesing -- Correct =  accessing -- Prediction =  accepting\n",
      "Wrong =  chalenges -- Correct =  challenges -- Prediction =  changes\n",
      "Wrong =  liaision -- Correct =  liaison -- Prediction =  passion\n",
      "Wrong =  scisors -- Correct =  scissors -- Prediction =  sailors\n",
      "Wrong =  biscutes -- Correct =  biscuits -- Prediction =  biscuits\n",
      "Wrong =  perhapse -- Correct =  perhaps -- Prediction =  perhaps\n",
      "Accuracy:0.05%\n"
     ]
    }
   ],
   "source": [
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]\n",
    "predict(Y_rand,X_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
