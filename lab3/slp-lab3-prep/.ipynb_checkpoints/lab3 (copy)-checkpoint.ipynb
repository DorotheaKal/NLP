{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Επεξεργασία Φωνής και Φυσικής Γλώσσας</h1>\n",
    "<h2 align = \"center\">3ο Εργαστήριο (Προπαρασκευή) </h2>\n",
    "<h3 align = \"center\">Sentiment Analysis </h3>\n",
    "<h3 align = \"center\"> Θεoδωρόπουλος Νικήτας -03115185</h3>\n",
    "<h3 align = \"center\"> Καλλιώρα Δωροθέα - 03115176</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Εισαγωγή\n",
    "Στην παρούσα εργαστηριακή άσκηση υλοποιούμε ένα μοντέλο για την επεξεργασία και την κατηγοριοποίηση κειμένων με χρήση Deep Neural Networks (**DNNs**), και την βιβλιοθήκη **PyTorch**. Για κάθε δείγμα εισόδου σε μορφή κειμένου δημιουργούμε διανυσματικές αναπαραστάσεις με χρήση pretrained word embeddings λέξεων. Συγκεκριμένα χρησιμοποιήσαμε [**GloVe**](https://nlp.stanford.edu/projects/glove/) αναπαραστάσεις $(50d,6B)$\n",
    "\n",
    "Στόχος είναι η εκπαίδευση μοντέλων για ανάλυση συναισθήματος (**sentiment analysis**) σε προτάσεις. Θα δουλέψουμε πάνω στα εξής δύο corpora:\n",
    "\n",
    "* **Sentence Polarity Dataset** [Pang and Lee, 2005](http://www.cs.cornell.edu/people/pabo/movie-review-data/). To dataset   \n",
    "  περιέχει 5331 θετικές\n",
    "  και 5331 αρνητικές κριτικές ταινιών, από το Rotten Tomatoes και   είναι binary-classification πρόβλημα (positive, negative).\n",
    "  \n",
    "  \n",
    "  \n",
    "* **Semeval 2017 Task4-A** [Rosenthal et al., 2017](http://alt.qcri.org/semeval2017/task4/index.php?id=data-and-tools). To dataset       αυτό περιέχει tweets τα οποία\n",
    "  είναι κατηγοριοποιημένα σε 3 κλάσεις (positive, negative,         neutral) με 49570 παραδείγματα\n",
    "  εκπαίδευσης και 12284 παραδείγματα αξιολόγησης.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κυρίως Μέρος\n",
    "\n",
    "#### 1: Προεπεξεργασία των δεδομένων: \n",
    "Αρχικά έγινε κατάλληλη προεπεξεργασία των δεδομένων με χρήση των κλάσεων Dataset, Dataloader. Τα labels αντιστοιχίζονται σε αριθμούς και στα κείμενα γίνεται κατάλληλο tokenization και αντιστοίχιση των λέξεων σε embeddings indexes. Χρησιμοποιηθήκε ο **NLTK** sentence tokenizer. \n",
    "\n",
    "Για να μπορέσει να γίνει επεξεργασία των δεδομένων θα πρέπει τα δέιγματα να έχουν το ίδιο μέγεθος για τον λόγο αυτό επιλέχθηκε το σταθερό μέγεθος **60** λέξεων, και έγινε κατάλληλο padding ή μείωση. Το πραγματικό μήκος αποθηκεύεται. Τα δεδομένα μετατρέπονται σε Tensors απο την κλάση Dataloader\n",
    "\n",
    "\n",
    "#### 2: Μοντέλο \n",
    "\n",
    "Σχεδιάζουμε την αρχιτεκτονική του Νευρωνικού Δικτύου. Με χρήση ενός **embedding layer** δημιουργούμε μια συνεχή διανυσματική αναπαράσταση για κάθε όρο της πρότασης, και για το δυνολικό δείγμα λαμβάνοντας τον μέσο όρο. Ο μέσος όρος προκύπτει διαιρώντας το άθροισμα με το πραγματικό μήκος του κειμένου, (τα embeddings των padded elements αντιστοιχίζονται στο μηδενικό διάνυσμα).\n",
    "\n",
    "Το embedding layer όπως έχουμε δει αντιστοιχίζει κοντα λέξεις που είναι σημασιολογικά κοντά. Τα embeddings αρχικοποιούνται με τα pretrained GloVe embeddings. Και έπειτα παγώνουν $(requires\\_grad=False)$.\n",
    "\n",
    "Τα διανύσματα προβάλονται απο το embedding layer μέσω μιας μη γραμμικής συνάρτησης ενεργοποίησης **(ReLU)** σε έναν ενδιάμεσο χώρο 100 διαστάσεων. Το τελευταίο layer προβάλει τα διανύσματα στον χώρο εξόδου. Για Binary Classification επιλέχθηκε output layer μεγέθους **1**, για compatibillity με το BCELoss. Για την κατηγοριοποίηση τριών κλάσεων έγινε αντιστοίχιση $\\mathbb{R}^{100} \\rightarrow \\mathbb{R}^3$.\n",
    "\n",
    "Υλοποιούμε το Forward pass εφαρμόζοτας σε κάθε mini-batch τους παραπάνω μετασχηματισμούς. \n",
    "\n",
    "\n",
    "#### 3: Εκπαίδευση\n",
    "\n",
    "Τα παραδέιγματα οργανώνονται σε mini-batches μεγέθους 4. Μεγαλύτερη τιμή θα ήταν προτιμότερη για το *Semeval2017A* dataset λόγω του μεγάλου αριθμού δεδομένων, χωρίς αυτό να επηρεάζει την απόδοση του μοντέλου μας. Εκτελούμε σε κάθε βήμα Stochasitc Gradient Descend. \n",
    "\n",
    "Για την εκπαίδευση χρησιμοποιούμε τα εξής:\n",
    "\n",
    "* Κριτήριο: **BCEWithLogitsLoss** για δεδομένα 2 κλάσεων. **CrossEntropyLoss** για δεδομένα 3 κλάσεων. \n",
    "\n",
    "\n",
    "* Παράμετροι: Το learning rate επιλέχθηκε $lr = 0.0001$, διότι παρατηρήσαμε οτι μεγαλύτερες τιμές οδηγούν σε μεγάλωσες ταλαντωσεις του loss και μειώνουν την απόδοση. Οι παράμετροι των μοντέλων με $requires\\_grad=True$ βελτιστοποιούνται με Gradient Descend\n",
    "\n",
    "\n",
    "* Optimizer: Επιλέγουμε **Adam** καθώς χρησιμοποιείται ευρέως στην βιβλιογραφία και έχει γενικά καλα αποτελέσματα. Επίσης προσαρμόζει αυτόματα την ταχύτητα ενημέρωσης των μαρών. \n",
    "\n",
    "\n",
    "Αξιολογούμε το μοντέλο στο τέλος κάθε εποχής, εκτυπώνοντας το train loss. Παρακάτω για τα δύο Data sets εκπαιδεύουμε τα μοντέλα και παρουσιάζουμε τα τελικά αποτελέσματα: \n",
    "* **Classification Report** (F_1_score (macro), recall (macro))\n",
    "* Διαγράμματα **Train loss, Test loss** άνα εποχή. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Polarity Dataset (MR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive']\n",
      "\n",
      "After LabelEncoder: [1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', '``', 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['the', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'the', 'lord', 'of', 'the', 'rings', '``', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer/director', 'peter', 'jackson', \"'s\", 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', 'tolkien', \"'s\", 'middle-earth', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['effective', 'but', 'too-tepid', 'biopic']\n",
      "\n",
      "Tokenized sample:\n",
      "['if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', ',', 'wasabi', 'is', 'a', 'good', 'place', 'to', 'start', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['emerges', 'as', 'something', 'rare', ',', 'an', 'issue', 'movie', 'that', \"'s\", 'so', 'honest', 'and', 'keenly', 'observed', 'that', 'it', 'does', \"n't\", 'feel', 'like', 'one', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['the', 'film', 'provides', 'some', 'great', 'insight', 'into', 'the', 'neurotic', 'mindset', 'of', 'all', 'comics', '--', 'even', 'those', 'who', 'have', 'reached', 'the', 'absolute', 'top', 'of', 'the', 'game', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['offers', 'that', 'rare', 'combination', 'of', 'entertainment', 'and', 'education', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['perhaps', 'no', 'picture', 'ever', 'made', 'has', 'more', 'literally', 'showed', 'that', 'the', 'road', 'to', 'hell', 'is', 'paved', 'with', 'good', 'intentions', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['steers', 'turns', 'in', 'a', 'snappy', 'screenplay', 'that', 'curls', 'at', 'the', 'edges', ';', 'it', \"'s\", 'so', 'clever', 'you', 'want', 'to', 'hate', 'it', '.', 'but', 'he', 'somehow', 'pulls', 'it', 'off', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['take', 'care', 'of', 'my', 'cat', 'offers', 'a', 'refreshingly', 'different', 'slice', 'of', 'asian', 'cinema', '.']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "an utterly compelling 'who wrote it' in which the reputation of the most famous author who ever lived comes into question .\n",
      "\n",
      "Transformed sample:\n",
      "(array([    30,  14306,   8538, 400001,    837,     21,     58,      7,\n",
      "           43,      1,   3148,      4,      1,     97,   1614,   1716,\n",
      "           39,    662,   1578,    935,     76,    996,      3,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 23)\n",
      "\n",
      "Original sample:\n",
      "illuminating if overly talky documentary .\n",
      "\n",
      "Transformed sample:\n",
      "(array([31742,    84, 11014, 98927,  3831,     3,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0]), 1, 6)\n",
      "\n",
      "Original sample:\n",
      "a masterpiece four years in the making .\n",
      "\n",
      "Transformed sample:\n",
      "(array([    8, 15024,   134,    83,     7,     1,   434,     3,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0]), 1, 8)\n",
      "\n",
      "Original sample:\n",
      "the movie's ripe , enrapturing beauty will tempt those willing to probe its inscrutable mysteries .\n",
      "\n",
      "Transformed sample:\n",
      "(array([     1,   1006,     10,  13712,      2, 400001,   4283,     44,\n",
      "        38664,    156,   2209,      5,   3616,     48,  59907,  14934,\n",
      "            3,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 17)\n",
      "\n",
      "Original sample:\n",
      "offers a breath of the fresh air of true sophistication .\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 1729,     8,  8354,     4,     1,  1904,   326,     4,  1447,\n",
      "       20664,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0]), 1, 11)\n",
      "\n",
      "BaselineDNN(\n",
      "  (embeddings): Embedding(400002, 50)\n",
      "  (lin1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lin2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 0.6883\n",
      " Epoch 1, Total loss: 0.6914\n",
      "\n",
      " [========================================] ...Epoch 2, Loss: 0.6893\n",
      " Epoch 2, Total loss: 0.6875\n",
      "\n",
      " [========================================] ...Epoch 3, Loss: 0.6739\n",
      " Epoch 3, Total loss: 0.6830\n",
      "\n",
      " [========================================] ...Epoch 4, Loss: 0.6855\n",
      " Epoch 4, Total loss: 0.6780\n",
      "\n",
      " [========================================] ...Epoch 5, Loss: 0.6702\n",
      " Epoch 5, Total loss: 0.6721\n",
      "\n",
      " [========================================] ...Epoch 6, Loss: 0.6392\n",
      " Epoch 6, Total loss: 0.6652\n",
      "\n",
      " [========================================] ...Epoch 7, Loss: 0.6472\n",
      " Epoch 7, Total loss: 0.6586\n",
      "\n",
      " [========================================] ...Epoch 8, Loss: 0.6465\n",
      " Epoch 8, Total loss: 0.6521\n",
      "\n",
      " [========================================] ...Epoch 9, Loss: 0.6458\n",
      " Epoch 9, Total loss: 0.6457\n",
      "\n",
      " [========================================] ...Epoch 10, Loss: 0.6832\n",
      " Epoch 10, Total loss: 0.6403\n",
      "\n",
      " [========================================] ...Epoch 11, Loss: 0.6373\n",
      " Epoch 11, Total loss: 0.6341\n",
      "\n",
      " [========================================] ...Epoch 12, Loss: 0.6498\n",
      " Epoch 12, Total loss: 0.6289\n",
      "\n",
      " [========================================] ...Epoch 13, Loss: 0.6128\n",
      " Epoch 13, Total loss: 0.6233\n",
      "\n",
      " [========================================] ...Epoch 14, Loss: 0.6325\n",
      " Epoch 14, Total loss: 0.6189\n",
      "\n",
      " [========================================] ...Epoch 15, Loss: 0.7139\n",
      " Epoch 15, Total loss: 0.6153\n",
      "\n",
      " [========================================] ...Epoch 16, Loss: 0.5696\n",
      " Epoch 16, Total loss: 0.6095\n",
      "\n",
      " [========================================] ...Epoch 17, Loss: 0.6168\n",
      " Epoch 17, Total loss: 0.6063\n",
      "\n",
      " [========================================] ...Epoch 18, Loss: 0.5011\n",
      " Epoch 18, Total loss: 0.6015\n",
      "\n",
      " [========================================] ...Epoch 19, Loss: 0.5331\n",
      " Epoch 19, Total loss: 0.5988\n",
      "\n",
      " [========================================] ...Epoch 20, Loss: 0.7787\n",
      " Epoch 20, Total loss: 0.5986\n",
      "\n",
      " [========================================] ...Epoch 21, Loss: 0.5826\n",
      " Epoch 21, Total loss: 0.5939\n",
      "\n",
      " [========================================] ...Epoch 22, Loss: 0.6633\n",
      " Epoch 22, Total loss: 0.5925\n",
      "\n",
      " [========================================] ...Epoch 23, Loss: 0.5568\n",
      " Epoch 23, Total loss: 0.5893\n",
      "\n",
      " [========================================] ...Epoch 24, Loss: 0.6678\n",
      " Epoch 24, Total loss: 0.5885\n",
      "\n",
      " [========================================] ...Epoch 25, Loss: 0.5481\n",
      " Epoch 25, Total loss: 0.5853\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [========================================] ...Epoch 26, Loss: 0.5050\n",
      " Epoch 26, Total loss: 0.5836\n",
      "\n",
      " [========================================] ...Epoch 27, Loss: 0.8168\n",
      " Epoch 27, Total loss: 0.5855\n",
      "\n",
      " [========================================] ...Epoch 28, Loss: 0.4789\n",
      " Epoch 28, Total loss: 0.5804\n",
      "\n",
      " [========================================] ...Epoch 29, Loss: 0.6688\n",
      " Epoch 29, Total loss: 0.5814\n",
      "\n",
      " [========================================] ...Epoch 30, Loss: 0.5524\n",
      " Epoch 30, Total loss: 0.5793\n",
      "\n",
      " [========================================] ...Epoch 31, Loss: 0.6453\n",
      " Epoch 31, Total loss: 0.5791\n",
      "\n",
      " [========================================] ...Epoch 32, Loss: 0.5405\n",
      " Epoch 32, Total loss: 0.5772\n",
      "\n",
      " [========================================] ...Epoch 33, Loss: 0.7291\n",
      " Epoch 33, Total loss: 0.5784\n",
      "\n",
      " [========================================] ...Epoch 34, Loss: 0.6267\n",
      " Epoch 34, Total loss: 0.5766\n",
      "\n",
      " [========================================] ...Epoch 35, Loss: 0.5916\n",
      " Epoch 35, Total loss: 0.5755\n",
      "\n",
      " [========================================] ...Epoch 36, Loss: 0.5715\n",
      " Epoch 36, Total loss: 0.5746\n",
      "\n",
      " [========================================] ...Epoch 37, Loss: 0.5790\n",
      " Epoch 37, Total loss: 0.5741\n",
      "\n",
      " [========================================] ...Epoch 38, Loss: 0.6333\n",
      " Epoch 38, Total loss: 0.5741\n",
      "\n",
      " [========================================] ...Epoch 39, Loss: 0.5229\n",
      " Epoch 39, Total loss: 0.5724\n",
      "\n",
      " [========================================] ...Epoch 40, Loss: 0.6744\n",
      " Epoch 40, Total loss: 0.5735\n",
      "\n",
      " [========================================] ...Epoch 41, Loss: 0.5450\n",
      " Epoch 41, Total loss: 0.5717\n",
      "\n",
      " [========================================] ...Epoch 42, Loss: 0.4819\n",
      " Epoch 42, Total loss: 0.5708\n",
      "\n",
      " [========================================] ...Epoch 43, Loss: 0.3995\n",
      " Epoch 43, Total loss: 0.5692\n",
      "\n",
      " [========================================] ...Epoch 44, Loss: 0.5639\n",
      " Epoch 44, Total loss: 0.5706\n",
      "\n",
      " [========================================] ...Epoch 45, Loss: 0.5362\n",
      " Epoch 45, Total loss: 0.5701\n",
      "\n",
      " [========================================] ...Epoch 46, Loss: 0.7595\n",
      " Epoch 46, Total loss: 0.5721\n",
      "\n",
      " [========================================] ...Epoch 47, Loss: 0.6608\n",
      " Epoch 47, Total loss: 0.5706\n",
      "\n",
      " [========================================] ...Epoch 48, Loss: 0.5392\n",
      " Epoch 48, Total loss: 0.5689\n",
      "\n",
      " [========================================] ...Epoch 49, Loss: 0.4879\n",
      " Epoch 49, Total loss: 0.5680\n",
      "\n",
      " [========================================] ...Epoch 50, Loss: 0.5131\n",
      " Epoch 50, Total loss: 0.5681\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 10, Classification Report:\u001b[0m\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.70      0.70       331\n",
      "         1.0       0.70      0.69      0.69       331\n",
      "\n",
      "    accuracy                           0.69       662\n",
      "   macro avg       0.69      0.69      0.69       662\n",
      "weighted avg       0.69      0.69      0.69       662\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 10, Plot:\u001b[0m\n",
      "\n",
      "Figure(800x800)\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-b1712973f994>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-b1712973f994>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <img src=\"./img/MR_50_loss.png?modified=12345678\" width = \"600\" height = auto;>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<img src=\"./img/MR_50_loss.png?modified=12345678\" width = \"600\" height = auto;>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semeval 2017 Task4-A (Semeval2017A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'SeeMonterey', 'LOST', '-', 'Sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'Early', 'Fri', 'morning', ',', 'Montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'REWARD', '!', 'Plz', 'RT', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'PersonaSoda', 'well', 'yeah', ',', 'that', \"'s\", 'third', 'parties', '.', 'Sony', 'itself', 'is', \"n't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', 'It', \"'s\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['Sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y.o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'fakethom', 'Have', 'android', 'tab', 'and', 'do', \"n't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'May', 'go', 'the', 'Sony', 'route', 'then', ':', '-', ')']\n",
      "\n",
      "Tokenized sample:\n",
      "['Finally', 'I', 'get', 'my', 'ps4', 'back', 'I', 'sent', 'it', 'to', 'Sony', 'cause', 'my', 'HDMI', 'was', 'mess', 'up', 'now', 'I', 'can', 'play', 'MG', \"'s\", 'Tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'AskPlayStation', 'Why', 'wo', \"n't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'Im', 'calling', 'Sony', 'tomorrow', '!', 'I', 'want', 'help', 'but', 'the', '``', 'support', 'team', \"''\", '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "['Sony', \"'s\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'Playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https', ':', '//t.co/xbisCRkPL4', '#', 'MistressSophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#', 'tv', 'Ind', 'vs', 'SL', '3rd', 'Test', 'Day', '3', ':', 'Cricket', 'live', 'score', 'and', 'Sony', 'Six', 'live', 'streaming', 'info', ':', 'Watch', 'the', 'live', 'teleca', '...', 'http', ':', '//t.co/mUlHw4cN00', '#', 'Sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'TruthInsider', '@', 'bertymufc', '@', 'gamerxone720', '@', 'PNF4LYFE', '@', 'Yanks2013', '@', 'VirtuaMe', 'Lol', 'it', \"'s\", 'all', 'about', 'Sony', 'Sony', 'Sony', ',', 'if', 'Sony', 'gave', 'Bj', \"'s\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'greencapt', 'Official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  23,  339, 4503,   44,  966,   31, 1515,   21,   11, 5116,  327,\n",
      "        522,  186,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,    415,    579,   4503,   3308,  16145,    223,\n",
      "         1516,   4003,    435,      3,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 12)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,  43896,      3,   4003,      3,   2473,    435,\n",
      "         4003,     10,     53,     62,   4503,   4574,     67,      1,\n",
      "          183,  16672,      4, 400001,  58977,      3,   3203,      2,\n",
      "          103,      8,  11128,    123,    806,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 2, 29)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([    29,  31511,    410,     46,      8,  14403,     10,    157,\n",
      "         8339,     11, 281436,      7,    194,    454,     14,    305,\n",
      "          520,      2,  15464,      2,   4503,    458, 400001,    994,\n",
      "          187,      7,      1,  27264,   1969,    435,     28,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 31)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,     44,     31,   4002,      5,    254,     84,\n",
      "         4503,   7803,      1,   1966,    616,      7,      1, 310983,\n",
      "           12,     21,    108,     34,    239,      8,    162,    205,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 0, 24)\n",
      "\n",
      "BaselineDNN(\n",
      "  (embeddings): Embedding(400002, 50)\n",
      "  (lin1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lin2): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 0.9090\n",
      " Epoch 1, Total loss: 1.0262\n",
      "\n",
      " [========================================] ...Epoch 2, Loss: 0.8558\n",
      " Epoch 2, Total loss: 0.9673\n",
      "\n",
      " [========================================] ...Epoch 3, Loss: 0.8061\n",
      " Epoch 3, Total loss: 0.9359\n",
      "\n",
      " [========================================] ...Epoch 4, Loss: 0.8899\n",
      " Epoch 4, Total loss: 0.9152\n",
      "\n",
      " [========================================] ...Epoch 5, Loss: 0.8402\n",
      " Epoch 5, Total loss: 0.9020\n",
      "\n",
      " [========================================] ...Epoch 6, Loss: 0.9897\n",
      " Epoch 6, Total loss: 0.8931\n",
      "\n",
      " [========================================] ...Epoch 7, Loss: 1.0749\n",
      " Epoch 7, Total loss: 0.8861\n",
      "\n",
      " [========================================] ...Epoch 8, Loss: 0.8507\n",
      " Epoch 8, Total loss: 0.8796\n",
      "\n",
      " [========================================] ...Epoch 9, Loss: 0.8053\n",
      " Epoch 9, Total loss: 0.8748\n",
      "\n",
      " [========================================] ...Epoch 10, Loss: 0.9165\n",
      " Epoch 10, Total loss: 0.8712\n",
      "\n",
      " [========================================] ...Epoch 11, Loss: 0.9712\n",
      " Epoch 11, Total loss: 0.8681\n",
      "\n",
      " [========================================] ...Epoch 12, Loss: 0.9605\n",
      " Epoch 12, Total loss: 0.8656\n",
      "\n",
      " [========================================] ...Epoch 13, Loss: 0.7857\n",
      " Epoch 13, Total loss: 0.8632\n",
      "\n",
      " [========================================] ...Epoch 14, Loss: 1.0039\n",
      " Epoch 14, Total loss: 0.8620\n",
      "\n",
      " [========================================] ...Epoch 15, Loss: 0.8481\n",
      " Epoch 15, Total loss: 0.8603\n",
      "\n",
      " [========================================] ...Epoch 16, Loss: 1.0001\n",
      " Epoch 16, Total loss: 0.8594\n",
      "\n",
      " [========================================] ...Epoch 17, Loss: 0.9842\n",
      " Epoch 17, Total loss: 0.8581\n",
      "\n",
      " [========================================] ...Epoch 18, Loss: 0.8939\n",
      " Epoch 18, Total loss: 0.8572\n",
      "\n",
      " [========================================] ...Epoch 19, Loss: 0.9119\n",
      " Epoch 19, Total loss: 0.8563\n",
      "\n",
      " [========================================] ...Epoch 20, Loss: 0.8517\n",
      " Epoch 20, Total loss: 0.8557\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [========================================] ...Epoch 21, Loss: 0.9307\n",
      " Epoch 21, Total loss: 0.8551\n",
      "\n",
      " [========================================] ...Epoch 22, Loss: 0.7839\n",
      " Epoch 22, Total loss: 0.8541\n",
      "\n",
      " [========================================] ...Epoch 23, Loss: 0.8351\n",
      " Epoch 23, Total loss: 0.8537\n",
      "\n",
      " [========================================] ...Epoch 24, Loss: 0.9504\n",
      " Epoch 24, Total loss: 0.8533\n",
      "\n",
      " [========================================] ...Epoch 25, Loss: 0.8268\n",
      " Epoch 25, Total loss: 0.8527\n",
      "\n",
      " [========================================] ...Epoch 26, Loss: 0.9115\n",
      " Epoch 26, Total loss: 0.8524\n",
      "\n",
      " [========================================] ...Epoch 27, Loss: 0.7382\n",
      " Epoch 27, Total loss: 0.8516\n",
      "\n",
      " [========================================] ...Epoch 28, Loss: 0.8788\n",
      " Epoch 28, Total loss: 0.8514\n",
      "\n",
      " [========================================] ...Epoch 29, Loss: 0.9362\n",
      " Epoch 29, Total loss: 0.8511\n",
      "\n",
      " [========================================] ...Epoch 30, Loss: 0.9424\n",
      " Epoch 30, Total loss: 0.8508\n",
      "\n",
      " [========================================] ...Epoch 31, Loss: 0.7842\n",
      " Epoch 31, Total loss: 0.8502\n",
      "\n",
      " [========================================] ...Epoch 32, Loss: 0.7432\n",
      " Epoch 32, Total loss: 0.8498\n",
      "\n",
      " [========================================] ...Epoch 33, Loss: 0.7314\n",
      " Epoch 33, Total loss: 0.8494\n",
      "\n",
      " [========================================] ...Epoch 34, Loss: 0.8680\n",
      " Epoch 34, Total loss: 0.8493\n",
      "\n",
      " [========================================] ...Epoch 35, Loss: 0.8140\n",
      " Epoch 35, Total loss: 0.8490\n",
      "\n",
      " [========================================] ...Epoch 36, Loss: 0.7795\n",
      " Epoch 36, Total loss: 0.8486\n",
      "\n",
      " [========================================] ...Epoch 37, Loss: 0.8903\n",
      " Epoch 37, Total loss: 0.8485\n",
      "\n",
      " [========================================] ...Epoch 38, Loss: 0.7953\n",
      " Epoch 38, Total loss: 0.8481\n",
      "\n",
      " [========================================] ...Epoch 39, Loss: 0.7781\n",
      " Epoch 39, Total loss: 0.8476\n",
      "\n",
      " [========================================] ...Epoch 40, Loss: 0.8643\n",
      " Epoch 40, Total loss: 0.8475\n",
      "\n",
      " [========================================] ...Epoch 41, Loss: 0.6749\n",
      " Epoch 41, Total loss: 0.8469\n",
      "\n",
      " [========================================] ...Epoch 42, Loss: 0.9241\n",
      " Epoch 42, Total loss: 0.8471\n",
      "\n",
      " [========================================] ...Epoch 43, Loss: 0.8000\n",
      " Epoch 43, Total loss: 0.8466\n",
      "\n",
      " [========================================] ...Epoch 44, Loss: 0.8232\n",
      " Epoch 44, Total loss: 0.8464\n",
      "\n",
      " [========================================] ...Epoch 45, Loss: 0.9461\n",
      " Epoch 45, Total loss: 0.8463\n",
      "\n",
      " [========================================] ...Epoch 46, Loss: 0.8217\n",
      " Epoch 46, Total loss: 0.8460\n",
      "\n",
      " [========================================] ...Epoch 47, Loss: 0.9838\n",
      " Epoch 47, Total loss: 0.8459\n",
      "\n",
      " [========================================] ...Epoch 48, Loss: 0.7217\n",
      " Epoch 48, Total loss: 0.8453\n",
      "\n",
      " [========================================] ...Epoch 49, Loss: 0.8635\n",
      " Epoch 49, Total loss: 0.8453\n",
      "\n",
      " [========================================] ...Epoch 50, Loss: 0.8779\n",
      " Epoch 50, Total loss: 0.8450\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 10, Classification Report:\u001b[0m\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.43      0.51      3972\n",
      "           1       0.59      0.70      0.64      5937\n",
      "           2       0.52      0.54      0.53      2375\n",
      "\n",
      "    accuracy                           0.58     12284\n",
      "   macro avg       0.58      0.56      0.56     12284\n",
      "weighted avg       0.59      0.58      0.58     12284\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 10, Plot:\u001b[0m\n",
      "\n",
      "Figure(800x800)\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Semeval2017A_50_loss.png?modified=12345678\" width = \"600\" height = auto;>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Απαντήσεις Ερωτημάτων:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 1***: Γιατί αρχικοποιούμε το embedding layer με τα προ-εκπαιδευμένα word embeddings;\n",
    "\n",
    "Για την αρχικοποίηση των βαρών του embedding layer είχαμε δύο επιλογές: να χρησιμοποιήσουμε βάρη απο pre-trained word embeddings ή να αρχικοποιήσουμε τα βάρη σε τυχαίες τιμές οι οποίες θα μαθαίνονταν κατά την εκπαίδευση του μοντέλου. Επιλέξαμε να χρησιμοποιήσουμε τα βάρη των pre-trained word embedding. \n",
    "\n",
    "\n",
    "Τα pre-trained embeddings αναπαριστούν τις λέξεις σε ένα διανυσματικό χώρο ώστε οι σημασιολογικά κοντινές λέξεις να έχουν μικρή απόσταση. Η εκπαίδευση τους έχει γίνει σε corpus εκατομμυρίων λέξεων, και αποτελούν ένα τεράστιο λεξικό αναπαραστάεων. \n",
    "\n",
    "Κείμενα με κοντινή αναπαράσταση στον embedded χώρο θα έχουν και κοντινό συναισθηματικό περιεχόμενο και άρα θα πρέπει να δώσουν κοινή έξοδο. Η πληροφορία αυτή ειναι ανεκτίμητη για έναν ταξινομητή συναισθήματος, και προφανώς δε μπορεί να συναχθεί απο τα δεδομένα εκπαίδευσης στον ίδιο βαθμό. Ακόμα οδηγούν σε καλύτερο generalization αφού λέξεις του test set μπορεί να διαθέτουν pre-trained embedding αλλα να μην γίνονται γνωστές απο το train set. \n",
    "\n",
    "Συμπερασματικά τα embeddings περιέχουν ήδη σημασιολογική πληροφορία  και το DNN θα μπορέσει να αξιοποιήσει τις αναπαραστάσεις για να έχει καλύτερη απόδοση και να συγκλίνει πιο γρήγορα. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 2***: Γιατί κρατάμε παγωμένα τα βάρη του embedding layer κατά την εκπαίδευση;\n",
    "\n",
    "Οι λόγοι που κρατάμε παγωμένα τα βάρη του embedding layer κατά την εκπαίδευση είναι οι εξής:\n",
    "\n",
    "* Τα βάρη που προκύπτουν από τα pre-trained word embeddings έχουν δημιουργηθεί από ένα μεγάλου μεγέθους corpus και δείχνουν μια συσχέτιση μεταξύ των λέξεων. Η συσχέτιση αυτή, μαθαίνεται μετά από την εκπαίδευση σε high-end μηχανές και χρησιμοποιώντας ειδικά υπολογισμένες παραμέτρους. Άρα, δεν χρειάζεται περαιτέρω ανανέωση των τιμών των βαρών με αποτέλεσμα να μειώνεται η υπολογιστή πολυπλοκότητα του αλγορίθμου.\n",
    "\n",
    "\n",
    "* Αν συνεχίσουμε να ανανεώνουμε τις τιμές των βαρών των pre-trained word embeddings κατά την εκπαίδευση, ενδέχεται να αλλάξουμε τις συσχετίσεις που δείχνουν αρχκά τα embeddings και να μην έχουμε την επιθυμητή συμπεριφορά από το emdedding layer. Συγκεκριμένα, μια ανεπιθύμητη συμπεριφορά που μπορεί να έχουμε είναι το overfitting του DNN που έχουμε δημιουργήσει. Αν συνεχιστεί η ανανέωση των βαρών, το DNN θα εκπαιδευτεί πολύ καλά πάνω στο training set που έχουμε και έτσι δεν θα έχει καλή απόδοση σε κάποιο άλλο dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 3***: Γιατί βάζουμε μία μη γραμμική συνάρτηση ενεργοποίησης στο προτελευταίο layer; Τι διαφορά θα είχε αν είχαμε 2 ή περισσότερους γραμμικούς μετασχηματισμούς στη σειρά;\n",
    "\n",
    "\n",
    "Οποιαδήποτε λειτουργία θέλουμε να κάνει το νευρωνικό δίκτυο που δημιουργούμε θέλουμε να την αναπαραστήσουμε σε μία υπολογιστική συνάρτηση.Για να πετύχουμε αυτό θα πρέπει να εφαρμόσουμε μια συνάρτηση ενεργοποίησης f(x) έτσι ώστε το δίκτυο να γίνει πιο ισχυρό, να έχει την ικανότητα να μαθαίνει κάτι περίπλοκο και πολύπλοκο από τα δεδομένα και να αναπαριστά μη γραμμικές σύνθετες και αυθαίρετες αντιστοιχίες μεταξύ εισόδου και εξόδου. Ως εκ τούτου, χρησιμοποιώντας μη γραμμική ενεργοποίηση, είμαστε σε θέση να παράγουμε μη γραμμικές απεικονίσεις από τις εισόδους στις εξόδους αφού οι μη γραμμικές συναρτήσεις έχουν βαθμό μεγαλύτερο από ένα και έχουν καμπυλότητα όταν τις σχεδιάζουμε. \n",
    "\n",
    "Αν δεν είχαμε χρησιμοποιήσει μη-γραμμική συνάρτηση ενεργοποίησης, όσα layers και να βάζαμε στο DNN μας, θα συμπεριφερόταν σαν ένα single-layer perceptron αφού αν αθροίζαμε όλα τα layers του θα παίρναμε μια συνολική γραμμική συνάρτηση. Άρα, το μοντέλο θα προσπαθούσε κάθε φορά να αντιστοιχίσει την είσοδο με την έξοδο γραμμικά \n",
    "και έτσι θα είχαμε μία πολύ απλή αναπαράσταση των δεδομένων.  \n",
    "\n",
    "***Also another important feature of a Activation function is that it should be differentiable. We need it to be this way so as to perform backpropogation optimization strategy while propogating backwards in the network to compute gradients of Error(loss) with respect to Weights and then accordingly optimize weights using Gradient descend or any other Optimization technique to reduce Error. https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 4***: Αν θεωρήσουμε ότι κάθε διάσταση του embedding χώρου αντιστοιχεί σε μία αφηρημένη έννοια, μπορείτε να δώσετε μία διαισθητική ερμηνεία για το τι περιγράφει η αναπαράσταση που φτιάξατε(κέντρο-βάρους)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 5***: Αναφέρετε πιθανές αδυναμίες της συγκεκριμένης προσέγγισης για να αναπαραστήσουμε κείμενα.\n",
    "\n",
    "* Στη συγκεκριμένη προσέγγιση δεν λαμβάνουμε υπόψιν τη σύνταξη της πρότασης. Κάθε λέξη μπορεί να έχει μια συγκεκριμένη ερμηνεία στην πρόταση, αλλά η αναπαράσταση της να εκφράζει μια διαφορετική ερμηνεία.\n",
    "\n",
    "* Ακόμα, δεν λαμβάνουμε υπόψιν τη θέση που έχει κάθε λέξη στο κείμενο. Η θέση μιας λέξης στην πρόταση μπορείαλλάζει τη σημασία της σε κάθε πρόταση.\n",
    "\n",
    "* Τέλος, τα σημεία στίξης του κειμένου δεν επηρεάζουν την αναπαράσταση. Τα σημεία στίξης προσδίδουν διαφορετικό νόημα σε κάθε λέξη και δείχνουν διαφορετικό συναίσθημα για κάθε πρόταση. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 6***: Τι συνέπειες έχουν τα μικρά και μεγάλα mini-batches στην εκπαίδευση των μοντέλων;\n",
    "\n",
    "Αν χρησιμοποιήσουμε μικρά mini-batches για την εκπαίδευση του DNN τότε τα δεδομένα μας κανονικοποιούνται και έχουμε ένα μικρότερο generalization error. Αυτό συμβαίνει γιατί τα μικρά mini-batches έχουν πολύ θόρυβο μέσα αφού αποτελείται από πολλά raw δεδομένα. \n",
    "\n",
    "https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/\n",
    "\n",
    "Αν χρησιμοποιήσουμε μεγάλα mini-batches \n",
    "\n",
    " The concept is that if your batch size is big enough, this will provide a stable enough estimate of what the gradient of the full dataset would be. By taking samples from your dataset, you estimate the gradient while reducing computational cost significantly. The lower you go, the less accurate your esttimate will be, however in some cases these noisy gradients can actually help escape local minima. When it is too low, your network weights can just jump around if your data is noisy and it might be unable to learn or it converges very slowly, thus negatively impacting total computation time.\n",
    "\n",
    "https://datascience.stackexchange.com/questions/12532/does-batch-size-in-keras-have-any-effects-in-results-quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 7***: Συνήθως ανακατεύουμε την σειρά των mini-batches στα δεδομένα εκπαίδευσης σε κάθε εποχή. Μπορείτε να εξηγήσετε γιατί;\n",
    "\n",
    "\n",
    "Θέλουμε να κάνουμε shuffle τη σειρά των mini-batches δεδομένα εκπαίδευσης για τους παρακάτω λόγους:\n",
    "\n",
    "* Το DNN που έχουμε δημιουργήσει μπορεί εκτός από τις συναρτήσεις ενεργοποίησης που συνδέουν την είσοδο με την έξοδο να μάθει και την σειρά με την οποία δίνονται σε αυτό τα δεδομένα για την εκπαίδευση του. Άρα, αν κάνουμε shuffle ανά εποχή, το νευρωνικό βλέπει κάθε φορά τα δεδομένα με διαφορετική σειρά και δεν μπορεί να κάνει κάποια αντιστοίχιση της εισόδου με την έξοδο βασισμένο αυτή.\n",
    "\n",
    "\n",
    "* Το shuffle των mini-batches ανά εποχή μπορεί επίσης να βοηθήσει το DNN να συγκλίνει πιο γρήγορα στο ολικό ελάχιστο κατά την εκτέλεση του stochastic gradient descent. Αυτό συμβαίνει γιατί αν σε μία εποχή ο αλγόριθμος έχει ‘’κολλήσει’’ σε τοπικό ελάχιστο, στην επόμενη εποχή έχει μεγάλη πιθανότητα να ‘’ξεκολλήσει’’ αφού τα δεδομένα θα δίνονται με διαφορετική σειρά στο νευρωνικό δίκτυο. \n",
    "\n",
    "\n",
    "* Αν κάνουμε shuffle τη σειρά εισάγουμε τυχαιότητα στη σειρά που δίνονται τα δεδομένα στο δίκτυο ανά εποχή και έτσι το loss που παίρνουμε σαν αποτέλεσμα είναι πιο αμερόληπτο.\n",
    "\n",
    "https://stats.stackexchange.com/questions/245502/shuffling-data-in-the-mini-batch-training-of-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 8:*** Αξιολόγηση του ζητούμενου 10\n",
    "\n",
    "**1ο Dataset**\n",
    "\n",
    "**2ο Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
