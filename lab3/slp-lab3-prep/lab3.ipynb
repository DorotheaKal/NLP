{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Επεξεργασία Φωνής και Φυσικής Γλώσσας</h1>\n",
    "<h2 align = \"center\">3ο Εργαστήριο (Προπαρασκευή) </h2>\n",
    "<h3 align = \"center\">Sentiment Analysis </h3>\n",
    "<h3 align = \"center\"> Θεoδωρόπουλος Νικήτας -03115185</h3>\n",
    "<h3 align = \"center\"> Καλλιώρα Δωροθέα - 03115176</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Εισαγωγή\n",
    "Στην παρούσα εργαστηριακή άσκηση υλοποιούμε ένα μοντέλο για την επεξεργασία και την κατηγοριοποίηση κειμένων με χρήση Deep Neural Networks (**DNNs**), και την βιβλιοθήκη **PyTorch**. Για κάθε δείγμα εισόδου σε μορφή κειμένου δημιουργούμε διανυσματικές αναπαραστάσεις με χρήση pretrained word embeddings λέξεων. Συγκεκριμένα χρησιμοποιήσαμε [**GloVe**](https://nlp.stanford.edu/projects/glove/) αναπαραστάσεις $(50d,6B)$\n",
    "\n",
    "Στόχος είναι η εκπαίδευση μοντέλων για ανάλυση συναισθήματος (**sentiment analysis**) σε προτάσεις. Θα δουλέψουμε πάνω στα εξής δύο corpora:\n",
    "\n",
    "* **Sentence Polarity Dataset** [Pang and Lee, 2005](http://www.cs.cornell.edu/people/pabo/movie-review-data/). To dataset   \n",
    "  περιέχει 5331 θετικές\n",
    "  και 5331 αρνητικές κριτικές ταινιών, από το Rotten Tomatoes και   είναι binary-classification πρόβλημα (positive, negative).\n",
    "  \n",
    "  \n",
    "  \n",
    "* **Semeval 2017 Task4-A** [Rosenthal et al., 2017](http://alt.qcri.org/semeval2017/task4/index.php?id=data-and-tools). To dataset       αυτό περιέχει tweets τα οποία\n",
    "  είναι κατηγοριοποιημένα σε 3 κλάσεις (positive, negative,         neutral) με 49570 παραδείγματα\n",
    "  εκπαίδευσης και 12284 παραδείγματα αξιολόγησης.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κυρίως Μέρος\n",
    "\n",
    "#### 1: Προεπεξεργασία των δεδομένων: \n",
    "Αρχικά έγινε κατάλληλη προεπεξεργασία των δεδομένων με χρήση των κλάσεων Dataset, Dataloader. Τα labels αντιστοιχίζονται σε αριθμούς και στα κείμενα γίνεται κατάλληλο tokenization και αντιστοίχιση των λέξεων σε embeddings indexes. Χρησιμοποιηθήκε ο **NLTK** sentence tokenizer. \n",
    "\n",
    "Για να μπορέσει να γίνει επεξεργασία των δεδομένων θα πρέπει τα δέιγματα να έχουν το ίδιο μέγεθος για τον λόγο αυτό επιλέχθηκε το σταθερό μέγεθος **60** λέξεων, και έγινε κατάλληλο padding ή μείωση. Το πραγματικό μήκος αποθηκεύεται. Τα δεδομένα μετατρέπονται σε Tensors απο την κλάση Dataloader\n",
    "\n",
    "\n",
    "#### 2: Μοντέλο \n",
    "\n",
    "Σχεδιάζουμε την αρχιτεκτονική του Νευρωνικού Δικτύου. Με χρήση ενός **embedding layer** δημιουργούμε μια συνεχή διανυσματική αναπαράσταση για κάθε όρο της πρότασης, και για το δυνολικό δείγμα λαμβάνοντας τον μέσο όρο. Ο μέσος όρος προκύπτει διαιρώντας το άθροισμα με το πραγματικό μήκος του κειμένου, (τα embeddings των padded elements αντιστοιχίζονται στο μηδενικό διάνυσμα).\n",
    "\n",
    "Το embedding layer όπως έχουμε δει αντιστοιχίζει κοντα λέξεις που είναι σημασιολογικά κοντά. Τα embeddings αρχικοποιούνται με τα pretrained GloVe embeddings. Και έπειτα παγώνουν $(requires\\_grad=False)$.\n",
    "\n",
    "Τα διανύσματα προβάλονται απο το embedding layer μέσω μιας μη γραμμικής συνάρτησης ενεργοποίησης **(ReLU)** σε έναν ενδιάμεσο χώρο 100 διαστάσεων. Το τελευταίο layer προβάλει τα διανύσματα στον χώρο εξόδου. Για Binary Classification επιλέχθηκε output layer μεγέθους **1**, για compatibillity με το BCELoss. Για την κατηγοριοποίηση τριών κλάσεων έγινε αντιστοίχιση $\\mathbb{R}^{100} \\rightarrow \\mathbb{R}^3$.\n",
    "\n",
    "Υλοποιούμε το Forward pass εφαρμόζοτας σε κάθε mini-batch τους παραπάνω μετασχηματισμούς. \n",
    "\n",
    "\n",
    "#### 3: Εκπαίδευση\n",
    "\n",
    "Τα παραδέιγματα οργανώνονται σε mini-batches μεγέθους 128. Μεγαλύτερη τιμή είναι προτιμότερη για το *Semeval2017A* dataset λόγω του μεγάλου αριθμού δεδομένων. Εκτελούμε σε κάθε βήμα Stochasitc Gradient Descend. \n",
    "\n",
    "Για την εκπαίδευση χρησιμοποιούμε τα εξής:\n",
    "\n",
    "* Κριτήριο: **BCEWithLogitsLoss** για δεδομένα 2 κλάσεων. **CrossEntropyLoss** για δεδομένα 3 κλάσεων. \n",
    "\n",
    "\n",
    "* Παράμετροι: Το learning rate επιλέχθηκε $lr = 0.0001$, διότι παρατηρήσαμε οτι μεγαλύτερες τιμές οδηγούν σε μεγάλωσες ταλαντωσεις του loss και μειώνουν την απόδοση. Οι παράμετροι των μοντέλων με $requires\\_grad=True$ βελτιστοποιούνται με Gradient Descend\n",
    "\n",
    "\n",
    "* Optimizer: Επιλέγουμε **Adam** καθώς χρησιμοποιείται ευρέως στην βιβλιογραφία και έχει γενικά καλα αποτελέσματα. Επίσης προσαρμόζει αυτόματα την ταχύτητα ενημέρωσης των μαρών. \n",
    "\n",
    "\n",
    "Αξιολογούμε το μοντέλο στο τέλος κάθε εποχής, εκτυπώνοντας το train loss. Παρακάτω για τα δύο Data sets εκπαιδεύουμε τα μοντέλα και παρουσιάζουμε τα τελικά αποτελέσματα: \n",
    "* **Classification Report** (F_1_score (macro), recall (macro))\n",
    "* Διαγράμματα **Train loss, Test loss** άνα εποχή. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Polarity Dataset (MR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/MR_50_loss.png?modified=12345678\" width = \"600\" height = auto;>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semeval 2017 Task4-A (Semeval2017A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Semeval2017A_50_loss.png?modified=12345678\" width = \"600\" height = auto;>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Απαντήσεις Ερωτημάτων:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 1***: Γιατί αρχικοποιούμε το embedding layer με τα προ-εκπαιδευμένα word embeddings;\n",
    "\n",
    "Για την αρχικοποίηση των βαρών του embedding layer είχαμε δύο επιλογές: να χρησιμοποιήσουμε βάρη απο pre-trained word embeddings ή να αρχικοποιήσουμε τα βάρη σε τυχαίες τιμές οι οποίες θα μαθαίνονταν κατά την εκπαίδευση του μοντέλου. Επιλέξαμε να χρησιμοποιήσουμε τα βάρη των pre-trained word embedding. \n",
    "\n",
    "\n",
    "Τα pre-trained embeddings αναπαριστούν τις λέξεις σε ένα διανυσματικό χώρο ώστε οι σημασιολογικά κοντινές λέξεις να έχουν μικρή απόσταση. Η εκπαίδευση τους έχει γίνει σε corpus εκατομμυρίων λέξεων, και αποτελούν ένα τεράστιο λεξικό αναπαραστάεων. \n",
    "\n",
    "Κείμενα με κοντινή αναπαράσταση στον embedded χώρο θα έχουν και κοντινό συναισθηματικό περιεχόμενο και άρα θα πρέπει να δώσουν κοινή έξοδο. Η πληροφορία αυτή ειναι ανεκτίμητη για έναν ταξινομητή συναισθήματος, και προφανώς δε μπορεί να συναχθεί απο τα δεδομένα εκπαίδευσης στον ίδιο βαθμό. Ακόμα οδηγούν σε καλύτερο generalization αφού λέξεις του test set μπορεί να διαθέτουν pre-trained embedding αλλα να μην γίνονται γνωστές απο το train set. \n",
    "\n",
    "Συμπερασματικά τα embeddings περιέχουν πλούσια σημασιολογική πληροφορία  και το DNN θα μπορέσει να αξιοποιήσει τις αναπαραστάσεις για να έχει καλύτερη απόδοση και να συγκλίνει πιο γρήγορα. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 2***: Γιατί κρατάμε παγωμένα τα βάρη του embedding layer κατά την εκπαίδευση;\n",
    "\n",
    "Οι λόγοι που κρατάμε παγωμένα τα βάρη του embedding layer κατά την εκπαίδευση είναι οι εξής:\n",
    "\n",
    "* Τα βάρη που προκύπτουν από τα pre-trained word embeddings έχουν δημιουργηθεί από ένα μεγάλου μεγέθους corpus και δείχνουν μια συσχέτιση μεταξύ των λέξεων. Η συσχέτιση αυτή, μαθαίνεται μετά από την εκπαίδευση σε high-end μηχανές και χρησιμοποιώντας βέλτιστες παραμέτρους. Άρα, δεν χρειάζεται περαιτέρω ανανέωση των τιμών των βαρών με αποτέλεσμα να μειώνεται η υπολογιστή πολυπλοκότητα του αλγορίθμου.\n",
    "\n",
    "\n",
    "* Αν συνεχίσουμε να ανανεώνουμε τις τιμές των βαρών των pre-trained word embeddings κατά την εκπαίδευση, ενδέχεται να αλλάξουμε τις συσχετίσεις που δείχνουν αρχκά τα embeddings και να μην έχουμε την επιθυμητή συμπεριφορά από το emdedding layer. Συγκεκριμένα, μια ανεπιθύμητη συμπεριφορά είναι το overfitting του DNN. Αν συνεχιστεί η ανανέωση των βαρών, το DNN θα εκπαιδευτεί πολύ καλά πάνω στο training set που έχουμε και έτσι δεν θα έχει καλή απόδοση σε κάποιο άλλο dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 3***: Γιατί βάζουμε μία μη γραμμική συνάρτηση ενεργοποίησης στο προτελευταίο layer; Τι διαφορά θα είχε αν είχαμε 2 ή περισσότερους γραμμικούς μετασχηματισμούς στη σειρά;\n",
    "\n",
    "\n",
    "Οποιαδήποτε λειτουργία θέλουμε να κάνει το νευρωνικό δίκτυο που δημιουργούμε θέλουμε να την αναπαραστήσουμε σε μία υπολογιστική συνάρτηση.Για να πετύχουμε αυτό θα πρέπει να εφαρμόσουμε μια συνάρτηση ενεργοποίησης $f(x)$ έτσι ώστε το δίκτυο να γίνει πιο ισχυρό, να έχει την ικανότητα να μαθαίνει κάτι περίπλοκο και πολύπλοκο από τα δεδομένα και να αναπαριστά μη γραμμικές σύνθετες και αυθαίρετες αντιστοιχίες μεταξύ εισόδου και εξόδου. Ως εκ τούτου, χρησιμοποιώντας μη γραμμική ενεργοποίηση, είμαστε σε θέση να παράγουμε μη γραμμικές απεικονίσεις από τις εισόδους στις εξόδους αφού οι μη γραμμικές συναρτήσεις έχουν βαθμό μεγαλύτερο από ένα και έχουν καμπυλότητα όταν τις σχεδιάζουμε. Για προβλήματα classification πρέπει να μπορούμε να υπολογίσουμε μη γραμμικά decision boundaries.\n",
    "\n",
    "Αν δεν είχαμε χρησιμοποιήσει μη-γραμμική συνάρτηση ενεργοποίησης, όσα layers και να βάζαμε στο DNN μας, θα συμπεριφερόταν σαν ένα single-layer perceptron αφού αν αθροίζαμε όλα τα layers του θα παίρναμε μια συνολική γραμμική συνάρτηση. Άρα, το μοντέλο θα προσπαθούσε κάθε φορά να αντιστοιχίσει την είσοδο με την έξοδο γραμμικά \n",
    "και έτσι θα είχαμε μία πολύ απλή αναπαράσταση των δεδομένων.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 4***: Αν θεωρήσουμε ότι κάθε διάσταση του embedding χώρου αντιστοιχεί σε μία αφηρημένη έννοια, μπορείτε να δώσετε μία διαισθητική ερμηνεία για το τι περιγράφει η αναπαράσταση που φτιάξατε(κέντρο-βάρους).\n",
    "\n",
    "Θεωρούμε έναν χώρο όπου κάθε διάσταση αντιστοιχίζεται και σε μία αφηρημένη έννοια. Τοτε για ένα διάνυσμα του χώρου $x = (x_1,x_2,...,x_n)$ η τιμή $x_i$ του διανύσματος εκφράζει την συσχέτιση με την έννοια της διάστασης $i$, πόσο κοντά δηλαδή σημασιολογικά είναι το κείμενο με την έννοια. Κάθε λέξη του κειμένου αποτελεί και ένα διάνυσμα που έχει μεγαλύτερη τιμή σε άλλες διάστασεις. Κάθε τιμή του μέσου όρου μας δίνει μια μετρική για το πόσο κοντά ειναι το κείμενο με την συγκεκριμένη έννοια. Για χαμηλές τιμές το κείμενο είναι ασυσχέτιστο με τις έννοιες και για υψηλές τιμές η συσχέτιση είναι μεγάλη. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 5***: Αναφέρετε πιθανές αδυναμίες της συγκεκριμένης προσέγγισης για να αναπαραστήσουμε κείμενα.\n",
    "\n",
    "* Στη συγκεκριμένη προσέγγιση δεν λαμβάνουμε υπόψιν την θέση της λέξης. Η σημασιολογία μίας λέξης αλλάζει ανάλογα με την θέση της στην πρόταση, και μια αρνητική λέξη μπορεί να μετατραπεί σε θετική σε διαφορετικά συμφραζόμενα. Ένας τρόπος να αντιμετοπιστεί αυτο το πρόβλημα είναι με εισαγωγή n-gram language models παράλληλα με τα embeddings. \n",
    "\n",
    "\n",
    "* Ακόμα, δεν λαμβάνουμε υπόψιν την σύνταξη της πρότασης, αφού αθροίζουμε απλά τις λέξεις χωρίς να ελέγχουμε για άλλα στοιχεία της πρότασης και τον  ρόλο λέξεων. \n",
    "\n",
    "\n",
    "* Τέλος, τα σημεία στίξης του κειμένου δεν επηρεάζουν την αναπαράσταση. Τα σημεία στίξης προσδίδουν διαφορετικό νόημα σε κάθε λέξη και δείχνουν διαφορετικό συναίσθημα για κάθε πρόταση. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 6***: Τι συνέπειες έχουν τα μικρά και μεγάλα mini-batches στην εκπαίδευση των μοντέλων;\n",
    "\n",
    " \n",
    "\n",
    "Για αρκετά μεγάλο batch size μπορούμε να πετύχουμε μια σταθερή εκτίμηση για το ποιο θα είναι το gradient για το συνολικό data set. Ακόμα ανεβαίνει σημαντικά η ταχύτητα εκπαίδευσης αφού το gradient υπολογίζεται αθροιστικά για ένα σύνολο δεδομένων και έπειτα πραγματοποιείται back propagation και gradient descend. Όσο πιο μικρό είναι το batch size τόσο λιγότερη ακριβής ειναι η εκτίμηση. Τα μικρά batches έχουν όμως θόρυβο αφού αποτελούνται από πολλά raw δεδομένα, το οποία μπορεί να βοηθήσει το μοντέλο να ξεφύγει απο τοπικά ελάχιστα και άρα να οδηγήσει σε καλύτερη βελτιστοποίηση. Όταν είναι πολύ μικρό όμως το μοντέλο παρουσιάζει έντονη ταλάντωση ή συγκλίνει πολυ αργά. Μικρά batch sizes μπορούν επίσης να μειώσουν το generalization error. \n",
    "\n",
    "Γενικά το σωστό μέγεθος batch είναι σημαντικό γιατί επηρεάζει την ταχύτητα σύγκλισης του μοντέλου, την υπολογιστικη πολυπλοκότητα αλλα και την τελική απόδοση. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 7***: Συνήθως ανακατεύουμε την σειρά των mini-batches στα δεδομένα εκπαίδευσης σε κάθε εποχή. Μπορείτε να εξηγήσετε γιατί;\n",
    "\n",
    "\n",
    "Θέλουμε να κάνουμε shuffle τη σειρά των mini-batches δεδομένα εκπαίδευσης για τους παρακάτω λόγους:\n",
    "\n",
    "* Το DNN που έχουμε δημιουργήσει μπορεί εκτός από τις συναρτήσεις ενεργοποίησης που συνδέουν την είσοδο με την έξοδο να μάθει και την σειρά με την οποία δίνονται σε αυτό τα δεδομένα για την εκπαίδευση του. Άρα, αν κάνουμε shuffle ανά εποχή, το νευρωνικό βλέπει κάθε φορά τα δεδομένα με διαφορετική σειρά και δεν μπορεί να κάνει κάποια αντιστοίχιση της εισόδου με την έξοδο βασισμένο αυτή.\n",
    "\n",
    "\n",
    "* Το shuffle των mini-batches ανά εποχή μπορεί επίσης να βοηθήσει το DNN να συγκλίνει πιο γρήγορα στο ολικό ελάχιστο κατά την εκτέλεση του stochastic gradient descent. Αυτό συμβαίνει γιατί αν σε μία εποχή ο αλγόριθμος έχει ‘’κολλήσει’’ σε τοπικό ελάχιστο, στην επόμενη εποχή έχει μεγάλη πιθανότητα να ‘’ξεκολλήσει’’ αφού τα δεδομένα θα δίνονται με διαφορετική σειρά στο νευρωνικό δίκτυο. \n",
    "\n",
    "\n",
    "* Αν κάνουμε shuffle τη σειρά εισάγουμε τυχαιότητα στη σειρά που δίνονται τα δεδομένα στο δίκτυο ανά εποχή και έτσι το loss που παίρνουμε σαν αποτέλεσμα είναι πιο αμερόληπτο.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ερώτηση 8:*** Αξιολόγηση του ζητούμενου 10\n",
    "\n",
    "Παραπάνω έχουμε τυπώσει **classification report** που περιλαμβάνει F1_score (macro) και recall (macro). Ακόμα παραθέτουμε τις καμπύλες εκπαίδευσης. \n",
    "\n",
    "Παρατηρούμε ότι οι καμπύλες έιναι φυσιολογικές με το loss να πέφτει και στα δύο σύνολα. Η τιμή learning rate που επιλέξαμε είναι καλή αλλιώς θα βλέπαμε απότομη αύξηση του loss. \n",
    "\n",
    "Στο **MR** dataset παρατηρούμε μεγάλη ταλάντωση του loss function στο test set που σημαίνει οτι δεν γίνεται καλο generalization και απαιτεί περαιτέρω διερεύνηση.  \n",
    "\n",
    "Στο **Semeval2017A** η καμπύλη εκπαίδευσης παραπέμπει σε καλά αποτελέσματα, με το loss να πέφτει σταθερά και στα δύο σύνολα. \n",
    "\n",
    "Γενικά δεν παρατηρείται overfitting όπως είναι αναμενόμενο λόγω της πολυπλοκότητας των δεδομένων και του προβλήματος, αλλα και τον μικρό αριθμό εποχών. Για μεγαλύτερο αριθμό εποχών αναμένουμε καλύτερα αποτελέσματα. \n",
    "\n",
    "Η ακρίβεια είναι **0.69** και **0.58** αντίστοιχα. Μεγαλύτερη στο **MR** όπως περιμένα αφού ειναι ευκολότερο πρόβλημα. Τα f1_score ως καλύτερες μετρικές απόδοσης ακολουθούν το accuracy. Αναφέρουμε οτι στο **Semeval2017A** η κλάση 1 (neutral) πάει σημαντικά καλύτερα απο τις άλλες (10% διαφορά στην ακρίβεια). Ακόμα η κλάση 0 (negative) έχει αρκετά χαμηλή τιμή recall **0.43**. Για το \n",
    "**MR** οι μετρικές είναι πολυ καλές.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "the\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'personasoda', 'well', 'yeah', ',', 'that', \"'s\", 'third', 'parties', '.', 'sony', 'itself', 'is', \"n't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', 'it', \"'s\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y.o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'fakethom', 'have', 'android', 'tab', 'and', 'do', \"n't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':', '-', ')']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', 'mg', \"'s\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'askplaystation', 'why', 'wo', \"n't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '``', 'support', 'team', \"''\", '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', \"'s\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https', ':', '//t.co/xbiscrkpl4', '#', 'mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#', 'tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http', ':', '//t.co/mulhw4cn00', '#', 'sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'truthinsider', '@', 'bertymufc', '@', 'gamerxone720', '@', 'pnf4lyfe', '@', 'yanks2013', '@', 'virtuame', 'lol', 'it', \"'s\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', 'bj', \"'s\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  23,  339, 4503,   44,  966,   31, 1515,   21,   11, 5116,  327,\n",
      "        522,  186,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,    415,    579,   4503,   3308,  16145,    223,\n",
      "         1516,   4003,    435,      3,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 12)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,  43896,      3,   4003,      3,   2473,    435,\n",
      "         4003,     10,     53,     62,   4503,   4574,     67,      1,\n",
      "          183,  16672,      4, 400001,  58977,      3,   3203,      2,\n",
      "          103,      8,  11128,    123,    806,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 2, 29)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([    29,  31511,    410,     46,      8,  14403,     10,    157,\n",
      "         8339,     11, 281436,      7,    194,    454,     14,    305,\n",
      "          520,      2,  15464,      2,   4503,    458, 400001,    994,\n",
      "          187,      7,      1,  27264,   1969,    435,     28,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 31)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,     44,     31,   4002,      5,    254,     84,\n",
      "         4503,   7803,      1,   1966,    616,      7,      1, 310983,\n",
      "           12,     21,    108,     34,    239,      8,    162,    205,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 0, 24)\n",
      "\n",
      "BaselineDNN(\n",
      "  (embeddings): Embedding(400002, 50)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (lin1): Linear(in_features=50, out_features=1024, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lin2): Linear(in_features=1024, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 0.7736\n",
      " Epoch 1, Train loss: 0.8642\n",
      "           Test loss: 0.9067\n",
      " [========================================] ...Epoch 2, Loss: 0.7094\n",
      " Epoch 2, Train loss: 0.8493\n",
      "           Test loss: 0.8767\n",
      " [========================================] ...Epoch 3, Loss: 0.8513\n",
      " Epoch 3, Train loss: 0.8455\n",
      "           Test loss: 0.8638\n",
      " [========================================] ...Epoch 4, Loss: 0.9258\n",
      " Epoch 4, Train loss: 0.8408\n",
      "           Test loss: 0.8563\n",
      " [========================================] ...Epoch 5, Loss: 0.8423\n",
      " Epoch 5, Train loss: 0.8480\n",
      "           Test loss: 0.8667\n",
      " [========================================] ...Epoch 6, Loss: 0.9207\n",
      " Epoch 6, Train loss: 0.8418\n",
      "           Test loss: 0.8814\n",
      " [========================================] ...Epoch 7, Loss: 0.7791\n",
      " Epoch 7, Train loss: 0.8350\n",
      "           Test loss: 0.8656\n",
      " [========================================] ...Epoch 8, Loss: 1.0053\n",
      " Epoch 8, Train loss: 0.8354\n",
      "           Test loss: 0.8630\n",
      " [========================================] ...Epoch 9, Loss: 0.8307\n",
      " Epoch 9, Train loss: 0.8325\n",
      "           Test loss: 0.8608\n",
      " [========================================] ...Epoch 10, Loss: 0.8474\n",
      " Epoch 10, Train loss: 0.8285\n",
      "           Test loss: 0.8454\n",
      " [========================================] ...Epoch 11, Loss: 0.7960\n",
      " Epoch 11, Train loss: 0.8250\n",
      "           Test loss: 0.8535\n",
      " [========================================] ...Epoch 12, Loss: 0.8119\n",
      " Epoch 12, Train loss: 0.8246\n",
      "           Test loss: 0.8477\n",
      " [========================================] ...Epoch 13, Loss: 0.8277\n",
      " Epoch 13, Train loss: 0.8238\n",
      "           Test loss: 0.8610\n",
      " [========================================] ...Epoch 14, Loss: 0.6255\n",
      " Epoch 14, Train loss: 0.8236\n",
      "           Test loss: 0.8511\n",
      " [========================================] ...Epoch 15, Loss: 0.7523\n",
      " Epoch 15, Train loss: 0.8223\n",
      "           Test loss: 0.8580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [========================================] ...Epoch 16, Loss: 1.0224\n",
      " Epoch 16, Train loss: 0.8189\n",
      "           Test loss: 0.8448\n",
      " [========================================] ...Epoch 17, Loss: 0.8647\n",
      " Epoch 17, Train loss: 0.8195\n",
      "           Test loss: 0.8541\n",
      " [========================================] ...Epoch 18, Loss: 0.8202\n",
      " Epoch 18, Train loss: 0.8157\n",
      "           Test loss: 0.8519\n",
      " [========================================] ...Epoch 19, Loss: 1.0188\n",
      " Epoch 19, Train loss: 0.8190\n",
      "           Test loss: 0.8428\n",
      " [========================================] ...Epoch 20, Loss: 0.8780\n",
      " Epoch 20, Train loss: 0.8155\n",
      "           Test loss: 0.8481\n",
      " [========================================] ...Epoch 21, Loss: 0.8144\n",
      " Epoch 21, Train loss: 0.8422\n",
      "           Test loss: 0.9086\n",
      " [========================================] ...Epoch 22, Loss: 0.8970\n",
      " Epoch 22, Train loss: 0.8173\n",
      "           Test loss: 0.8487\n",
      " [========================================] ...Epoch 23, Loss: 0.8410\n",
      " Epoch 23, Train loss: 0.8131\n",
      "           Test loss: 0.8540\n",
      " [========================================] ...Epoch 24, Loss: 0.7865\n",
      " Epoch 24, Train loss: 0.8133\n",
      "           Test loss: 0.8652\n",
      " [========================================] ...Epoch 25, Loss: 0.9972\n",
      " Epoch 25, Train loss: 0.8132\n",
      "           Test loss: 0.8648\n",
      " [========================================] ...Epoch 26, Loss: 0.7456\n",
      " Epoch 26, Train loss: 0.8136\n",
      "           Test loss: 0.8657\n",
      " [========================================] ...Epoch 27, Loss: 0.8147\n",
      " Epoch 27, Train loss: 0.8103\n",
      "           Test loss: 0.8449\n",
      " [========================================] ...Epoch 28, Loss: 0.8831\n",
      " Epoch 28, Train loss: 0.8153\n",
      "           Test loss: 0.8660\n",
      " [========================================] ...Epoch 29, Loss: 0.9814\n",
      " Epoch 29, Train loss: 0.8149\n",
      "           Test loss: 0.8395\n",
      " [========================================] ...Epoch 30, Loss: 0.7943\n",
      " Epoch 30, Train loss: 0.8207\n",
      "           Test loss: 0.8978\n",
      " [========================================] ...Epoch 31, Loss: 0.8112\n",
      " Epoch 31, Train loss: 0.8145\n",
      "           Test loss: 0.8467\n",
      " [========================================] ...Epoch 32, Loss: 0.9727\n",
      " Epoch 32, Train loss: 0.8043\n",
      "           Test loss: 0.8460\n",
      " [========================================] ...Epoch 33, Loss: 0.7176\n",
      " Epoch 33, Train loss: 0.8047\n",
      "           Test loss: 0.8537\n",
      " [========================================] ...Epoch 34, Loss: 0.7039\n",
      " Epoch 34, Train loss: 0.8094\n",
      "           Test loss: 0.8673\n",
      " [========================================] ...Epoch 35, Loss: 0.9568\n",
      " Epoch 35, Train loss: 0.8036\n",
      "           Test loss: 0.8441\n",
      " [========================================] ...Epoch 36, Loss: 0.7750\n",
      " Epoch 36, Train loss: 0.8013\n",
      "           Test loss: 0.8552\n",
      " [========================================] ...Epoch 37, Loss: 0.7331\n",
      " Epoch 37, Train loss: 0.8050\n",
      "           Test loss: 0.8778\n",
      " [========================================] ...Epoch 38, Loss: 0.7686\n",
      " Epoch 38, Train loss: 0.7996\n",
      "           Test loss: 0.8603\n",
      " [========================================] ...Epoch 39, Loss: 0.9584\n",
      " Epoch 39, Train loss: 0.8035\n",
      "           Test loss: 0.8543\n",
      " [========================================] ...Epoch 40, Loss: 0.6068\n",
      " Epoch 40, Train loss: 0.8012\n",
      "           Test loss: 0.8631\n",
      " [========================================] ...Epoch 41, Loss: 0.9320\n",
      " Epoch 41, Train loss: 0.7992\n",
      "           Test loss: 0.8623\n",
      " [========================================] ...Epoch 42, Loss: 0.6995\n",
      " Epoch 42, Train loss: 0.7991\n",
      "           Test loss: 0.8504\n",
      " [========================================] ...Epoch 43, Loss: 0.9353\n",
      " Epoch 43, Train loss: 0.8008\n",
      "           Test loss: 0.8791\n",
      " [========================================] ...Epoch 44, Loss: 0.7376\n",
      " Epoch 44, Train loss: 0.7990\n",
      "           Test loss: 0.8605\n",
      " [========================================] ...Epoch 45, Loss: 0.7698\n",
      " Epoch 45, Train loss: 0.8021\n",
      "           Test loss: 0.8713\n",
      " [========================================] ...Epoch 46, Loss: 0.9145\n",
      " Epoch 46, Train loss: 0.7961\n",
      "           Test loss: 0.8563\n",
      " [========================================] ...Epoch 47, Loss: 0.8243\n",
      " Epoch 47, Train loss: 0.7949\n",
      "           Test loss: 0.8525\n",
      " [========================================] ...Epoch 48, Loss: 0.9033\n",
      " Epoch 48, Train loss: 0.7937\n",
      "           Test loss: 0.8698\n",
      " [========================================] ...Epoch 49, Loss: 0.7716\n",
      " Epoch 49, Train loss: 0.7981\n",
      "           Test loss: 0.8771\n",
      " [========================================] ...Epoch 50, Loss: 0.9692\n",
      " Epoch 50, Train loss: 0.7943\n",
      "           Test loss: 0.8723\n",
      "\n",
      "\u001b[1mQuestion 10, Classification Report:\u001b[0m\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.50      0.55      3972\n",
      "           1       0.61      0.65      0.63      5937\n",
      "           2       0.50      0.59      0.54      2375\n",
      "\n",
      "    accuracy                           0.59     12284\n",
      "   macro avg       0.58      0.58      0.57     12284\n",
      "weighted avg       0.59      0.59      0.59     12284\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 10, Plot:\u001b[0m\n",
      "\n",
      "Figure(800x800)\n"
     ]
    }
   ],
   "source": [
    "!python main.py DNN_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6fd84cafa78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./checkpoints/{model_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test_gold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_gold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from models import BaselineDNN,BaseLSTM\n",
    "\n",
    "model = BaselineDNN(output_size=n_classes,  # EX8\n",
    "                    embeddings=embeddings,\n",
    "                    method = 'mean',\n",
    "                    attention_size=60,\n",
    "                    trainable_emb=EMB_TRAINABLE,tf_idf = TF_IDF)\n",
    "\n",
    "import torch\n",
    "model_name = 'DNN_mean'\n",
    "from training import train_dataset, eval_dataset\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "the\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'personasoda', 'well', 'yeah', ',', 'that', \"'s\", 'third', 'parties', '.', 'sony', 'itself', 'is', \"n't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', 'it', \"'s\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y.o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'fakethom', 'have', 'android', 'tab', 'and', 'do', \"n't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':', '-', ')']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', 'mg', \"'s\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'askplaystation', 'why', 'wo', \"n't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '``', 'support', 'team', \"''\", '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', \"'s\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https', ':', '//t.co/xbiscrkpl4', '#', 'mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#', 'tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http', ':', '//t.co/mulhw4cn00', '#', 'sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'truthinsider', '@', 'bertymufc', '@', 'gamerxone720', '@', 'pnf4lyfe', '@', 'yanks2013', '@', 'virtuame', 'lol', 'it', \"'s\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', 'bj', \"'s\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@', 'greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  23,  339, 4503,   44,  966,   31, 1515,   21,   11, 5116,  327,\n",
      "        522,  186,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,    415,    579,   4503,   3308,  16145,    223,\n",
      "         1516,   4003,    435,      3,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 12)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,  43896,      3,   4003,      3,   2473,    435,\n",
      "         4003,     10,     53,     62,   4503,   4574,     67,      1,\n",
      "          183,  16672,      4, 400001,  58977,      3,   3203,      2,\n",
      "          103,      8,  11128,    123,    806,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 2, 29)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([    29,  31511,    410,     46,      8,  14403,     10,    157,\n",
      "         8339,     11, 281436,      7,    194,    454,     14,    305,\n",
      "          520,      2,  15464,      2,   4503,    458, 400001,    994,\n",
      "          187,      7,      1,  27264,   1969,    435,     28,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 1, 31)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([ 17528, 400001,     44,     31,   4002,      5,    254,     84,\n",
      "         4503,   7803,      1,   1966,    616,      7,      1, 310983,\n",
      "           12,     21,    108,     34,    239,      8,    162,    205,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0]), 0, 24)\n",
      "\n",
      "BaselineDNN(\n",
      "  (embeddings): Embedding(400002, 50)\n",
      "  (lin1): Linear(in_features=100, out_features=1024, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lin2): Linear(in_features=1024, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 1.1325\n",
      " Epoch 1, Total loss: 0.9557\n",
      "\n",
      " [========================================] ...Epoch 2, Loss: 0.9825\n",
      " Epoch 2, Total loss: 0.9011\n",
      "\n",
      " [========================================] ...Epoch 3, Loss: 0.8424\n",
      " Epoch 3, Total loss: 0.8827\n",
      "\n",
      " [========================================] ...Epoch 4, Loss: 0.8006\n",
      " Epoch 4, Total loss: 0.8719\n",
      "\n",
      " [========================================] ...Epoch 5, Loss: 0.8108\n",
      " Epoch 5, Total loss: 0.8646\n",
      "\n",
      " [========================================] ...Epoch 6, Loss: 0.8711\n",
      " Epoch 6, Total loss: 0.8596\n",
      "\n",
      " [========================================] ...Epoch 7, Loss: 0.8216\n",
      " Epoch 7, Total loss: 0.8558\n",
      "\n",
      " [========================================] ...Epoch 8, Loss: 0.8287\n",
      " Epoch 8, Total loss: 0.8522\n",
      "\n",
      " [========================================] ...Epoch 9, Loss: 0.8970\n",
      " Epoch 9, Total loss: 0.8494\n",
      "\n",
      " [========================================] ...Epoch 10, Loss: 0.7413\n",
      " Epoch 10, Total loss: 0.8463\n",
      "\n",
      " [========================================] ...Epoch 11, Loss: 1.0144\n",
      " Epoch 11, Total loss: 0.8445\n",
      "\n",
      " [========================================] ...Epoch 12, Loss: 0.7043\n",
      " Epoch 12, Total loss: 0.8429\n",
      "\n",
      " [========================================] ...Epoch 13, Loss: 0.8748\n",
      " Epoch 13, Total loss: 0.8412\n",
      "\n",
      " [========================================] ...Epoch 14, Loss: 0.8843\n",
      " Epoch 14, Total loss: 0.8385\n",
      "\n",
      " [========================================] ...Epoch 15, Loss: 0.7844\n",
      " Epoch 15, Total loss: 0.8368\n",
      "\n",
      " [========================================] ...Epoch 16, Loss: 0.9612\n",
      " Epoch 16, Total loss: 0.8368\n",
      "\n",
      " [===========-----------------------------] ...Epoch 17, Loss: 0.8022^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 178, in <module>\n",
      "    train_dataset(epoch, train_loader, model, criterion, optimizer,n_classes)\n",
      "  File \"/home/nikitas/NLP/lab3/slp-lab3-prep/training.py\", line 70, in train_dataset\n",
      "    optimizer.step()\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\", line 103, in step\n",
      "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py DNN_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python main.py DNN_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/core/__init__.py\", line 24, in <module>\r\n",
      "    from . import multiarray\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/core/multiarray.py\", line 14, in <module>\r\n",
      "    from . import overrides\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/core/overrides.py\", line 7, in <module>\r\n",
      "    from numpy.core._multiarray_umath import (\r\n",
      "ImportError: PyCapsule_Import could not import module \"datetime\"\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"main.py\", line 6, in <module>\r\n",
      "    from sklearn.exceptions import UndefinedMetricWarning\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/__init__.py\", line 82, in <module>\r\n",
      "    from .base import clone\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 17, in <module>\r\n",
      "    import numpy as np\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/__init__.py\", line 142, in <module>\r\n",
      "    from . import core\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/core/__init__.py\", line 54, in <module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: \r\n",
      "\r\n",
      "IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\r\n",
      "\r\n",
      "Importing the numpy c-extensions failed.\r\n",
      "- Try uninstalling and reinstalling numpy.\r\n",
      "- If you have already done that, then:\r\n",
      "  1. Check that you expected to use Python3.7 from \"/home/nikitas/anaconda3/bin/python\",\r\n",
      "     and that you have no directories in your PATH or PYTHONPATH that can\r\n",
      "     interfere with the Python and numpy version \"1.18.1\" you're trying to use.\r\n",
      "  2. If (1) looks fine, you can open a new issue at\r\n",
      "     https://github.com/numpy/numpy/issues.  Please include details on:\r\n",
      "     - how you installed Python\r\n",
      "     - how you installed numpy\r\n",
      "     - your operating system\r\n",
      "     - whether or not you have multiple versions of Python installed\r\n",
      "     - if you built from source, your compiler versions and ideally a build log\r\n",
      "\r\n",
      "- If you're working with a numpy git repository, try `git clean -xdf`\r\n",
      "  (removes all files not under version control) and rebuild numpy.\r\n",
      "\r\n",
      "Note: this error has many possible causes, so please don't comment on\r\n",
      "an existing issue about this - open a new one instead.\r\n",
      "\r\n",
      "Original error was: PyCapsule_Import could not import module \"datetime\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"main.py\", line 6, in <module>\r\n",
      "    from sklearn.exceptions import UndefinedMetricWarning\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/__init__.py\", line 82, in <module>\r\n",
      "    from .base import clone\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 17, in <module>\r\n",
      "    import numpy as np\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/__init__.py\", line 152, in <module>\r\n",
      "    from . import random\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/random/__init__.py\", line 181, in <module>\r\n",
      "    from . import _pickle\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/numpy/random/_pickle.py\", line 1, in <module>\r\n",
      "    from .mtrand import RandomState\r\n",
      "  File \"_bit_generator.pxd\", line 14, in init numpy.random.mtrand\r\n",
      "  File \"_bit_generator.pyx\", line 40, in init numpy.random._bit_generator\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/secrets.py\", line 15, in <module>\r\n",
      "    import base64\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/base64.py\", line 11, in <module>\r\n",
      "    import binascii\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"main.py\", line 6, in <module>\r\n",
      "    from sklearn.exceptions import UndefinedMetricWarning\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/__init__.py\", line 82, in <module>\r\n",
      "    from .base import clone\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 20, in <module>\r\n",
      "    from .utils import _IS_32BIT\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\", line 27, in <module>\r\n",
      "    from .fixes import np_version\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/sklearn/utils/fixes.py\", line 18, in <module>\r\n",
      "    import scipy.stats\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/scipy/stats/__init__.py\", line 384, in <module>\r\n",
      "    from .stats import *\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py\", line 180, in <module>\r\n",
      "    from scipy.ndimage import measurements\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/scipy/ndimage/__init__.py\", line 153, in <module>\r\n",
      "    from .filters import *\r\n",
      "  File \"/home/nikitas/anaconda3/lib/python3.7/site-packages/scipy/ndimage/filters.py\", line 38, in <module>\r\n",
      "    from . import _ni_docstrings\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\r\n",
      "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_pooling_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_attention_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
