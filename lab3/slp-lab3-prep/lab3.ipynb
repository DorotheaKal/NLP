{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "<user>\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@personasoda', 'well', 'yeah', ',', \"that's\", 'third', 'parties', '.', 'sony', 'itself', \"isn't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', \"it's\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y', '.', 'o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@fakethom', 'have', 'android', 'tab', 'and', \"don't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':-)']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', \"mg's\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@askplaystation', 'why', \"won't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '\"', 'support', 'team', '\"', '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "[\"sony's\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https://t.co/xbiscrkpl4', '#mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http://t.co/mulhw4cn00', '#sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@truthinsider', '@bertymufc', '@gamerxone720', '@pnf4lyfe', '@yanks2013', '@virtuame', 'lol', \"it's\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', \"bj's\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  67, 1230, 5581,  129,  897,   57, 5442,   34,   38, 4476,  244,\n",
      "        621,  731,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     714,     417,    5581,   21025,   14331,     212,\n",
      "          3626,     329, 1193515,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 10)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,    2241,       2,     329,       2,     400, 1193515,\n",
      "       1193515,     895,      93,    5581,    5720,     100,      14,\n",
      "           412,   17807,      40,    1657,     235,   48173,       2,\n",
      "           194,       5,      87,      12,    1831,     126,      10,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 2, 28)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([     13,   75686, 1193515,       3,      12, 1193515,     625,\n",
      "         13723,      38, 1193515,      36,    2935,    1939,      47,\n",
      "          2632, 1193515,       5, 1193515,       5,    5581,    7161,\n",
      "       1193515,    3380,     354,      36,      14,    9702,     200,\n",
      "       1193515,      13,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 30)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     129,      57,    2555,      17,     164,      75,\n",
      "          5581,   45964,      14,    1903,    3949,      36,      14,\n",
      "       1193515,      29,      34,     531,      65,    1006,      12,\n",
      "           321,     577,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 0, 23)\n",
      "\n",
      "BaselineDNN(\n",
      "  (embeddings): Embedding(1193516, 50)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (lin1): Linear(in_features=100, out_features=1024, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lin2): Linear(in_features=1024, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 0.7441\n",
      " Epoch 1, Train loss: 0.8250\n",
      "           Test loss: 0.8330\n",
      " [========================================] ...Epoch 2, Loss: 0.8796\n",
      " Epoch 2, Train loss: 0.8127\n",
      "           Test loss: 0.8088\n",
      " [========================================] ...Epoch 3, Loss: 0.7695\n",
      " Epoch 3, Train loss: 0.8059\n",
      "           Test loss: 0.8173\n",
      " [========================================] ...Epoch 4, Loss: 0.7234\n",
      " Epoch 4, Train loss: 0.8037\n",
      "           Test loss: 0.8162\n",
      " [========================================] ...Epoch 5, Loss: 0.8581\n",
      " Epoch 5, Train loss: 0.7981\n",
      "           Test loss: 0.8041\n",
      " [========================================] ...Epoch 6, Loss: 0.8166\n",
      " Epoch 6, Train loss: 0.8054\n",
      "           Test loss: 0.8207\n",
      " [========================================] ...Epoch 7, Loss: 0.6361\n",
      " Epoch 7, Train loss: 0.7919\n",
      "           Test loss: 0.8003\n",
      " [========================================] ...Epoch 8, Loss: 0.6400\n",
      " Epoch 8, Train loss: 0.7948\n",
      "           Test loss: 0.8140\n",
      " [========================================] ...Epoch 9, Loss: 0.7164\n",
      " Epoch 9, Train loss: 0.8005\n",
      "           Test loss: 0.7898\n",
      " [========================================] ...Epoch 10, Loss: 0.8666\n",
      " Epoch 10, Train loss: 0.7885\n",
      "           Test loss: 0.8092\n",
      " [========================================] ...Epoch 11, Loss: 0.7300\n",
      " Epoch 11, Train loss: 0.7864\n",
      "           Test loss: 0.8066\n",
      " [========================================] ...Epoch 12, Loss: 0.7576\n",
      " Epoch 12, Train loss: 0.7884\n",
      "           Test loss: 0.8067\n",
      " [========================================] ...Epoch 13, Loss: 0.6756\n",
      " Epoch 13, Train loss: 0.7858\n",
      "           Test loss: 0.7979\n",
      " [========================================] ...Epoch 14, Loss: 0.7511\n",
      " Epoch 14, Train loss: 0.8102\n",
      "           Test loss: 0.8297\n",
      " [========================================] ...Epoch 15, Loss: 0.8838\n",
      " Epoch 15, Train loss: 0.7835\n",
      "           Test loss: 0.7822\n",
      " [========================================] ...Epoch 16, Loss: 0.7386\n",
      " Epoch 16, Train loss: 0.7798\n",
      "           Test loss: 0.8001\n",
      " [========================================] ...Epoch 17, Loss: 0.8563\n",
      " Epoch 17, Train loss: 0.7855\n",
      "           Test loss: 0.7877\n",
      " [========================================] ...Epoch 18, Loss: 0.8792\n",
      " Epoch 18, Train loss: 0.7824\n",
      "           Test loss: 0.8230\n",
      " [========================================] ...Epoch 19, Loss: 0.7895\n",
      " Epoch 19, Train loss: 0.7872\n",
      "           Test loss: 0.8071\n",
      " [========================================] ...Epoch 20, Loss: 0.6599\n",
      " Epoch 20, Train loss: 0.7740\n",
      "           Test loss: 0.7864\n",
      " [========================================] ...Epoch 21, Loss: 0.8570\n",
      " Epoch 21, Train loss: 0.7792\n",
      "           Test loss: 0.8220\n",
      " [========================================] ...Epoch 22, Loss: 0.6398\n",
      " Epoch 22, Train loss: 0.7966\n",
      "           Test loss: 0.8626\n",
      " [========================================] ...Epoch 23, Loss: 0.9683\n",
      " Epoch 23, Train loss: 0.7826\n",
      "           Test loss: 0.7847\n",
      " [========================================] ...Epoch 24, Loss: 0.9618\n",
      " Epoch 24, Train loss: 0.7734\n",
      "           Test loss: 0.7826\n",
      " [========================================] ...Epoch 25, Loss: 0.7640\n",
      " Epoch 25, Train loss: 0.7876\n",
      "           Test loss: 0.8359\n",
      " [========================================] ...Epoch 26, Loss: 0.7009\n",
      " Epoch 26, Train loss: 0.7798\n",
      "           Test loss: 0.8297\n",
      " [========================================] ...Epoch 27, Loss: 1.1020\n",
      " Epoch 27, Train loss: 0.7623\n",
      "           Test loss: 0.7951\n",
      " [========================================] ...Epoch 28, Loss: 0.7045\n",
      " Epoch 28, Train loss: 0.7649\n",
      "           Test loss: 0.8075\n",
      " [========================================] ...Epoch 29, Loss: 0.9039\n",
      " Epoch 29, Train loss: 0.7626\n",
      "           Test loss: 0.7927\n",
      " [========================================] ...Epoch 30, Loss: 0.7523\n",
      " Epoch 30, Train loss: 0.7636\n",
      "           Test loss: 0.7950\n",
      " [========================================] ...Epoch 31, Loss: 0.8067\n",
      " Epoch 31, Train loss: 0.7644\n",
      "           Test loss: 0.8057\n",
      " [========================================] ...Epoch 32, Loss: 0.6883\n",
      " Epoch 32, Train loss: 0.7637\n",
      "           Test loss: 0.7922\n",
      " [========================================] ...Epoch 33, Loss: 0.6321\n",
      " Epoch 33, Train loss: 0.7579\n",
      "           Test loss: 0.7932\n",
      " [========================================] ...Epoch 34, Loss: 0.7495\n",
      " Epoch 34, Train loss: 0.7613\n",
      "           Test loss: 0.7876\n",
      " [========================================] ...Epoch 35, Loss: 0.9228\n",
      " Epoch 35, Train loss: 0.7680\n",
      "           Test loss: 0.8226\n",
      " [========================================] ...Epoch 36, Loss: 0.8937\n",
      " Epoch 36, Train loss: 0.7671\n",
      "           Test loss: 0.8352\n",
      " [========================================] ...Epoch 37, Loss: 0.8411\n",
      " Epoch 37, Train loss: 0.7680\n",
      "           Test loss: 0.8407\n",
      " [========================================] ...Epoch 38, Loss: 0.6088\n",
      " Epoch 38, Train loss: 0.7541\n",
      "           Test loss: 0.7947\n",
      " [========================================] ...Epoch 39, Loss: 0.6389\n",
      " Epoch 39, Train loss: 0.7533\n",
      "           Test loss: 0.7923\n",
      " [========================================] ...Epoch 40, Loss: 0.9630\n",
      " Epoch 40, Train loss: 0.7532\n",
      "           Test loss: 0.7964\n",
      " [========================================] ...Epoch 41, Loss: 0.5774\n",
      " Epoch 41, Train loss: 0.7677\n",
      "           Test loss: 0.8302\n",
      " [========================================] ...Epoch 42, Loss: 0.9551\n",
      " Epoch 42, Train loss: 0.7528\n",
      "           Test loss: 0.8010\n",
      " [========================================] ...Epoch 43, Loss: 0.8496\n",
      " Epoch 43, Train loss: 0.7577\n",
      "           Test loss: 0.8072\n",
      " [========================================] ...Epoch 44, Loss: 0.7892\n",
      " Epoch 44, Train loss: 0.7502\n",
      "           Test loss: 0.8052\n",
      " [========================================] ...Epoch 45, Loss: 0.6146\n",
      " Epoch 45, Train loss: 0.7638\n",
      "           Test loss: 0.7920\n",
      " [========================================] ...Epoch 46, Loss: 0.9124\n",
      " Epoch 46, Train loss: 0.7485\n",
      "           Test loss: 0.7978\n",
      " [========================================] ...Epoch 47, Loss: 0.7297\n",
      " Epoch 47, Train loss: 0.7545\n",
      "           Test loss: 0.8160\n",
      " [========================================] ...Epoch 48, Loss: 0.7356\n",
      " Epoch 48, Train loss: 0.7464\n",
      "           Test loss: 0.7927\n",
      " [========================================] ...Epoch 49, Loss: 0.9087\n",
      " Epoch 49, Train loss: 0.7570\n",
      "           Test loss: 0.7935\n",
      " [========================================] ...Epoch 50, Loss: 0.6546\n",
      " Epoch 50, Train loss: 0.7455\n",
      "           Test loss: 0.8021\n",
      "Figure(800x800)\n"
     ]
    }
   ],
   "source": [
    "!python main.py DNN_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "<user>\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@personasoda', 'well', 'yeah', ',', \"that's\", 'third', 'parties', '.', 'sony', 'itself', \"isn't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', \"it's\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y', '.', 'o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@fakethom', 'have', 'android', 'tab', 'and', \"don't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':-)']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', \"mg's\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@askplaystation', 'why', \"won't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '\"', 'support', 'team', '\"', '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "[\"sony's\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https://t.co/xbiscrkpl4', '#mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http://t.co/mulhw4cn00', '#sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@truthinsider', '@bertymufc', '@gamerxone720', '@pnf4lyfe', '@yanks2013', '@virtuame', 'lol', \"it's\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', \"bj's\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  67, 1230, 5581,  129,  897,   57, 5442,   34,   38, 4476,  244,\n",
      "        621,  731,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     714,     417,    5581,   21025,   14331,     212,\n",
      "          3626,     329, 1193515,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 10)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,    2241,       2,     329,       2,     400, 1193515,\n",
      "       1193515,     895,      93,    5581,    5720,     100,      14,\n",
      "           412,   17807,      40,    1657,     235,   48173,       2,\n",
      "           194,       5,      87,      12,    1831,     126,      10,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 2, 28)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([     13,   75686, 1193515,       3,      12, 1193515,     625,\n",
      "         13723,      38, 1193515,      36,    2935,    1939,      47,\n",
      "          2632, 1193515,       5, 1193515,       5,    5581,    7161,\n",
      "       1193515,    3380,     354,      36,      14,    9702,     200,\n",
      "       1193515,      13,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 30)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     129,      57,    2555,      17,     164,      75,\n",
      "          5581,   45964,      14,    1903,    3949,      36,      14,\n",
      "       1193515,      29,      34,     531,      65,    1006,      12,\n",
      "           321,     577,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 0, 23)\n",
      "\n",
      "BaselineDNN(\n",
      "  (embeddings): Embedding(1193516, 50)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (lin1): Linear(in_features=50, out_features=1024, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lin2): Linear(in_features=1024, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 0.8223\n",
      " Epoch 1, Train loss: 0.8201\n",
      "           Test loss: 0.8253\n",
      " [========================================] ...Epoch 2, Loss: 0.7205\n",
      " Epoch 2, Train loss: 0.8055\n",
      "           Test loss: 0.8042\n",
      " [========================================] ...Epoch 3, Loss: 0.9604\n",
      " Epoch 3, Train loss: 0.8052\n",
      "           Test loss: 0.8191\n",
      " [========================================] ...Epoch 4, Loss: 0.9065\n",
      " Epoch 4, Train loss: 0.7977\n",
      "           Test loss: 0.8051\n",
      " [========================================] ...Epoch 5, Loss: 0.8383\n",
      " Epoch 5, Train loss: 0.7927\n",
      "           Test loss: 0.8053\n",
      " [========================================] ...Epoch 6, Loss: 0.9082\n",
      " Epoch 6, Train loss: 0.7947\n",
      "           Test loss: 0.7881\n",
      " [========================================] ...Epoch 7, Loss: 0.9718\n",
      " Epoch 7, Train loss: 0.7889\n",
      "           Test loss: 0.7961\n",
      " [========================================] ...Epoch 8, Loss: 0.8712\n",
      " Epoch 8, Train loss: 0.7858\n",
      "           Test loss: 0.7947\n",
      " [========================================] ...Epoch 9, Loss: 0.7703\n",
      " Epoch 9, Train loss: 0.7854\n",
      "           Test loss: 0.7860\n",
      " [========================================] ...Epoch 10, Loss: 0.8143\n",
      " Epoch 10, Train loss: 0.7824\n",
      "           Test loss: 0.7907\n",
      " [========================================] ...Epoch 11, Loss: 0.6398\n",
      " Epoch 11, Train loss: 0.7797\n",
      "           Test loss: 0.7972\n",
      " [========================================] ...Epoch 12, Loss: 0.7371\n",
      " Epoch 12, Train loss: 0.7780\n",
      "           Test loss: 0.8057\n",
      " [========================================] ...Epoch 13, Loss: 0.7962\n",
      " Epoch 13, Train loss: 0.7758\n",
      "           Test loss: 0.8003\n",
      " [========================================] ...Epoch 14, Loss: 0.9335\n",
      " Epoch 14, Train loss: 0.7759\n",
      "           Test loss: 0.7983\n",
      " [========================================] ...Epoch 15, Loss: 0.7722\n",
      " Epoch 15, Train loss: 0.7733\n",
      "           Test loss: 0.7939\n",
      " [========================================] ...Epoch 16, Loss: 0.7182\n",
      " Epoch 16, Train loss: 0.7734\n",
      "           Test loss: 0.7896\n",
      " [========================================] ...Epoch 17, Loss: 0.7732\n",
      " Epoch 17, Train loss: 0.7710\n",
      "           Test loss: 0.7993\n",
      " [========================================] ...Epoch 18, Loss: 1.0019\n",
      " Epoch 18, Train loss: 0.7709\n",
      "           Test loss: 0.8003\n",
      " [========================================] ...Epoch 19, Loss: 0.6832\n",
      " Epoch 19, Train loss: 0.7678\n",
      "           Test loss: 0.7978\n",
      " [========================================] ...Epoch 20, Loss: 0.6229\n",
      " Epoch 20, Train loss: 0.7683\n",
      "           Test loss: 0.7977\n",
      " [========================================] ...Epoch 21, Loss: 1.0625\n",
      " Epoch 21, Train loss: 0.7677\n",
      "           Test loss: 0.7930\n",
      " [========================================] ...Epoch 22, Loss: 0.7477\n",
      " Epoch 22, Train loss: 0.7668\n",
      "           Test loss: 0.8010\n",
      " [========================================] ...Epoch 23, Loss: 0.8293\n",
      " Epoch 23, Train loss: 0.7638\n",
      "           Test loss: 0.7944\n",
      " [========================================] ...Epoch 24, Loss: 0.8671\n",
      " Epoch 24, Train loss: 0.7636\n",
      "           Test loss: 0.8043\n",
      " [========================================] ...Epoch 25, Loss: 0.6172\n",
      " Epoch 25, Train loss: 0.7624\n",
      "           Test loss: 0.7978\n",
      " [========================================] ...Epoch 26, Loss: 0.6442\n",
      " Epoch 26, Train loss: 0.7682\n",
      "           Test loss: 0.8051\n",
      " [========================================] ...Epoch 27, Loss: 0.6890\n",
      " Epoch 27, Train loss: 0.7623\n",
      "           Test loss: 0.7956\n",
      " [========================================] ...Epoch 28, Loss: 0.9447\n",
      " Epoch 28, Train loss: 0.7588\n",
      "           Test loss: 0.7971\n",
      " [========================================] ...Epoch 29, Loss: 0.7434\n",
      " Epoch 29, Train loss: 0.7610\n",
      "           Test loss: 0.7907\n",
      " [========================================] ...Epoch 30, Loss: 0.6657\n",
      " Epoch 30, Train loss: 0.7597\n",
      "           Test loss: 0.7901\n",
      " [========================================] ...Epoch 31, Loss: 0.7183\n",
      " Epoch 31, Train loss: 0.7571\n",
      "           Test loss: 0.7944\n",
      " [========================================] ...Epoch 32, Loss: 0.8203\n",
      " Epoch 32, Train loss: 0.7598\n",
      "           Test loss: 0.7925\n",
      " [========================================] ...Epoch 33, Loss: 0.9418\n",
      " Epoch 33, Train loss: 0.7576\n",
      "           Test loss: 0.7976\n",
      " [========================================] ...Epoch 34, Loss: 0.7479\n",
      " Epoch 34, Train loss: 0.7549\n",
      "           Test loss: 0.8010\n",
      " [========================================] ...Epoch 35, Loss: 0.7548\n",
      " Epoch 35, Train loss: 0.7554\n",
      "           Test loss: 0.7938\n",
      " [========================================] ...Epoch 36, Loss: 0.8393\n",
      " Epoch 36, Train loss: 0.7549\n",
      "           Test loss: 0.7855\n",
      " [========================================] ...Epoch 37, Loss: 0.7424\n",
      " Epoch 37, Train loss: 0.7517\n",
      "           Test loss: 0.8015\n",
      " [========================================] ...Epoch 38, Loss: 0.6128\n",
      " Epoch 38, Train loss: 0.7532\n",
      "           Test loss: 0.7999\n",
      " [========================================] ...Epoch 39, Loss: 0.6873\n",
      " Epoch 39, Train loss: 0.7544\n",
      "           Test loss: 0.7964\n",
      " [========================================] ...Epoch 40, Loss: 0.8165\n",
      " Epoch 40, Train loss: 0.7499\n",
      "           Test loss: 0.7984\n",
      " [========================================] ...Epoch 41, Loss: 0.9479\n",
      " Epoch 41, Train loss: 0.7494\n",
      "           Test loss: 0.8079\n",
      " [========================================] ...Epoch 42, Loss: 0.7587\n",
      " Epoch 42, Train loss: 0.7490\n",
      "           Test loss: 0.7974\n",
      " [========================================] ...Epoch 43, Loss: 0.7791\n",
      " Epoch 43, Train loss: 0.7526\n",
      "           Test loss: 0.7917\n",
      " [========================================] ...Epoch 44, Loss: 0.8107\n",
      " Epoch 44, Train loss: 0.7476\n",
      "           Test loss: 0.8088\n",
      " [========================================] ...Epoch 45, Loss: 0.7243\n",
      " Epoch 45, Train loss: 0.7491\n",
      "           Test loss: 0.8015\n",
      " [========================================] ...Epoch 46, Loss: 0.7800\n",
      " Epoch 46, Train loss: 0.7472\n",
      "           Test loss: 0.8009\n",
      " [========================================] ...Epoch 47, Loss: 0.7550\n",
      " Epoch 47, Train loss: 0.7446\n",
      "           Test loss: 0.8021\n",
      " [========================================] ...Epoch 48, Loss: 0.6941\n",
      " Epoch 48, Train loss: 0.7456\n",
      "           Test loss: 0.8075\n",
      " [========================================] ...Epoch 49, Loss: 0.8529\n",
      " Epoch 49, Train loss: 0.7452\n",
      "           Test loss: 0.8042\n",
      " [========================================] ...Epoch 50, Loss: 0.7041\n",
      " Epoch 50, Train loss: 0.7462\n",
      "           Test loss: 0.8157\n",
      "Figure(800x800)\n"
     ]
    }
   ],
   "source": [
    "!python main.py DNN_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python main.py DNN_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "<user>\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@personasoda', 'well', 'yeah', ',', \"that's\", 'third', 'parties', '.', 'sony', 'itself', \"isn't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', \"it's\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y', '.', 'o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@fakethom', 'have', 'android', 'tab', 'and', \"don't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':-)']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', \"mg's\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@askplaystation', 'why', \"won't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '\"', 'support', 'team', '\"', '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "[\"sony's\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https://t.co/xbiscrkpl4', '#mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http://t.co/mulhw4cn00', '#sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@truthinsider', '@bertymufc', '@gamerxone720', '@pnf4lyfe', '@yanks2013', '@virtuame', 'lol', \"it's\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', \"bj's\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  67, 1230, 5581,  129,  897,   57, 5442,   34,   38, 4476,  244,\n",
      "        621,  731,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     714,     417,    5581,   21025,   14331,     212,\n",
      "          3626,     329, 1193515,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 10)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,    2241,       2,     329,       2,     400, 1193515,\n",
      "       1193515,     895,      93,    5581,    5720,     100,      14,\n",
      "           412,   17807,      40,    1657,     235,   48173,       2,\n",
      "           194,       5,      87,      12,    1831,     126,      10,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 2, 28)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([     13,   75686, 1193515,       3,      12, 1193515,     625,\n",
      "         13723,      38, 1193515,      36,    2935,    1939,      47,\n",
      "          2632, 1193515,       5, 1193515,       5,    5581,    7161,\n",
      "       1193515,    3380,     354,      36,      14,    9702,     200,\n",
      "       1193515,      13,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 30)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     129,      57,    2555,      17,     164,      75,\n",
      "          5581,   45964,      14,    1903,    3949,      36,      14,\n",
      "       1193515,      29,      34,     531,      65,    1006,      12,\n",
      "           321,     577,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 0, 23)\n",
      "\n",
      "BaseLSTM(\n",
      "  (embeddings): Embedding(1193516, 50)\n",
      "  (lstm): LSTM(50, 8)\n",
      "  (linear): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 0.8142\n",
      " Epoch 1, Train loss: 0.8593\n",
      "           Test loss: 0.9133\n",
      " [========================================] ...Epoch 2, Loss: 0.7518\n",
      " Epoch 2, Train loss: 0.8161\n",
      "           Test loss: 0.8584\n",
      " [========================================] ...Epoch 3, Loss: 1.1488\n",
      " Epoch 3, Train loss: 0.7938\n",
      "           Test loss: 0.8288\n",
      " [========================================] ...Epoch 4, Loss: 0.9536\n",
      " Epoch 4, Train loss: 0.7772\n",
      "           Test loss: 0.8113\n",
      " [========================================] ...Epoch 5, Loss: 0.8088\n",
      " Epoch 5, Train loss: 0.7679\n",
      "           Test loss: 0.8146\n",
      " [========================================] ...Epoch 6, Loss: 0.8398\n",
      " Epoch 6, Train loss: 0.7595\n",
      "           Test loss: 0.8009\n",
      " [========================================] ...Epoch 7, Loss: 0.7109\n",
      " Epoch 7, Train loss: 0.7529\n",
      "           Test loss: 0.7959\n",
      " [========================================] ...Epoch 8, Loss: 0.7348\n",
      " Epoch 8, Train loss: 0.7491\n",
      "           Test loss: 0.8084\n",
      " [========================================] ...Epoch 9, Loss: 0.6750\n",
      " Epoch 9, Train loss: 0.7438\n",
      "           Test loss: 0.7883\n",
      " [========================================] ...Epoch 10, Loss: 0.5558\n",
      " Epoch 10, Train loss: 0.7410\n",
      "           Test loss: 0.8021\n",
      " [========================================] ...Epoch 11, Loss: 0.6875\n",
      " Epoch 11, Train loss: 0.7379\n",
      "           Test loss: 0.7901\n",
      " [========================================] ...Epoch 12, Loss: 0.6959\n",
      " Epoch 12, Train loss: 0.7345\n",
      "           Test loss: 0.7870\n",
      " [========================================] ...Epoch 13, Loss: 0.9538\n",
      " Epoch 13, Train loss: 0.7304\n",
      "           Test loss: 0.7908\n",
      " [========================================] ...Epoch 14, Loss: 0.7854\n",
      " Epoch 14, Train loss: 0.7394\n",
      "           Test loss: 0.8143\n",
      " [========================================] ...Epoch 15, Loss: 0.7166\n",
      " Epoch 15, Train loss: 0.7279\n",
      "           Test loss: 0.7851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [========================================] ...Epoch 16, Loss: 0.7542\n",
      " Epoch 16, Train loss: 0.7297\n",
      "           Test loss: 0.7737\n",
      " [========================================] ...Epoch 17, Loss: 0.7529\n",
      " Epoch 17, Train loss: 0.7279\n",
      "           Test loss: 0.7750\n",
      " [========================================] ...Epoch 18, Loss: 0.8311\n",
      " Epoch 18, Train loss: 0.7219\n",
      "           Test loss: 0.7948\n",
      " [========================================] ...Epoch 19, Loss: 0.6903\n",
      " Epoch 19, Train loss: 0.7209\n",
      "           Test loss: 0.7837\n",
      " [========================================] ...Epoch 20, Loss: 0.6782\n",
      " Epoch 20, Train loss: 0.7198\n",
      "           Test loss: 0.7938\n",
      " [========================================] ...Epoch 21, Loss: 0.8980\n",
      " Epoch 21, Train loss: 0.7213\n",
      "           Test loss: 0.7745\n",
      " [========================================] ...Epoch 22, Loss: 0.5214\n",
      " Epoch 22, Train loss: 0.7189\n",
      "           Test loss: 0.7965\n",
      " [========================================] ...Epoch 23, Loss: 0.8073\n",
      " Epoch 23, Train loss: 0.7166\n",
      "           Test loss: 0.7886\n",
      " [========================================] ...Epoch 24, Loss: 0.8741\n",
      " Epoch 24, Train loss: 0.7140\n",
      "           Test loss: 0.7805\n",
      " [========================================] ...Epoch 25, Loss: 0.9809\n",
      " Epoch 25, Train loss: 0.7144\n",
      "           Test loss: 0.7720\n",
      " [========================================] ...Epoch 26, Loss: 0.7137\n",
      " Epoch 26, Train loss: 0.7132\n",
      "           Test loss: 0.7864\n",
      " [========================================] ...Epoch 27, Loss: 0.9260\n",
      " Epoch 27, Train loss: 0.7109\n",
      "           Test loss: 0.7772\n",
      " [========================================] ...Epoch 28, Loss: 0.6094\n",
      " Epoch 28, Train loss: 0.7127\n",
      "           Test loss: 0.7706\n",
      " [========================================] ...Epoch 29, Loss: 0.6556\n",
      " Epoch 29, Train loss: 0.7107\n",
      "           Test loss: 0.7740\n",
      " [========================================] ...Epoch 30, Loss: 0.7189\n",
      " Epoch 30, Train loss: 0.7090\n",
      "           Test loss: 0.7783\n",
      " [========================================] ...Epoch 31, Loss: 0.5613\n",
      " Epoch 31, Train loss: 0.7132\n",
      "           Test loss: 0.7672\n",
      " [========================================] ...Epoch 32, Loss: 0.9114\n",
      " Epoch 32, Train loss: 0.7079\n",
      "           Test loss: 0.7816\n",
      " [========================================] ...Epoch 33, Loss: 0.5745\n",
      " Epoch 33, Train loss: 0.7079\n",
      "           Test loss: 0.7736\n",
      " [========================================] ...Epoch 34, Loss: 0.5479\n",
      " Epoch 34, Train loss: 0.7143\n",
      "           Test loss: 0.7610\n",
      " [========================================] ...Epoch 35, Loss: 0.7967\n",
      " Epoch 35, Train loss: 0.7067\n",
      "           Test loss: 0.7838\n",
      " [========================================] ...Epoch 36, Loss: 0.7510\n",
      " Epoch 36, Train loss: 0.7044\n",
      "           Test loss: 0.7780\n",
      " [========================================] ...Epoch 37, Loss: 0.7112\n",
      " Epoch 37, Train loss: 0.7078\n",
      "           Test loss: 0.7845\n",
      " [========================================] ...Epoch 38, Loss: 0.5453\n",
      " Epoch 38, Train loss: 0.7063\n",
      "           Test loss: 0.7845\n",
      " [========================================] ...Epoch 39, Loss: 0.8184\n",
      " Epoch 39, Train loss: 0.7080\n",
      "           Test loss: 0.7852\n",
      " [========================================] ...Epoch 40, Loss: 0.6052\n",
      " Epoch 40, Train loss: 0.7033\n",
      "           Test loss: 0.7790\n",
      " [========================================] ...Epoch 41, Loss: 0.9521\n",
      " Epoch 41, Train loss: 0.7030\n",
      "           Test loss: 0.7810\n",
      " [========================================] ...Epoch 42, Loss: 0.6891\n",
      " Epoch 42, Train loss: 0.7039\n",
      "           Test loss: 0.7742\n",
      " [========================================] ...Epoch 43, Loss: 0.7604\n",
      " Epoch 43, Train loss: 0.7051\n",
      "           Test loss: 0.7828\n",
      " [========================================] ...Epoch 44, Loss: 0.7362\n",
      " Epoch 44, Train loss: 0.7049\n",
      "           Test loss: 0.7842\n",
      " [========================================] ...Epoch 45, Loss: 0.9631\n",
      " Epoch 45, Train loss: 0.7023\n",
      "           Test loss: 0.7703\n",
      " [========================================] ...Epoch 46, Loss: 0.6841\n",
      " Epoch 46, Train loss: 0.7026\n",
      "           Test loss: 0.7675\n",
      " [========================================] ...Epoch 47, Loss: 0.7468\n",
      " Epoch 47, Train loss: 0.7007\n",
      "           Test loss: 0.7717\n",
      " [========================================] ...Epoch 48, Loss: 0.7534\n",
      " Epoch 48, Train loss: 0.7023\n",
      "           Test loss: 0.7677\n",
      " [========================================] ...Epoch 49, Loss: 0.7990\n",
      " Epoch 49, Train loss: 0.7042\n",
      "           Test loss: 0.7838\n",
      " [========================================] ...Epoch 50, Loss: 0.6308\n",
      " Epoch 50, Train loss: 0.7010\n",
      "           Test loss: 0.7694\n",
      "Figure(800x800)\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "<user>\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@personasoda', 'well', 'yeah', ',', \"that's\", 'third', 'parties', '.', 'sony', 'itself', \"isn't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', \"it's\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y', '.', 'o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@fakethom', 'have', 'android', 'tab', 'and', \"don't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':-)']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', \"mg's\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@askplaystation', 'why', \"won't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '\"', 'support', 'team', '\"', '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "[\"sony's\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https://t.co/xbiscrkpl4', '#mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http://t.co/mulhw4cn00', '#sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@truthinsider', '@bertymufc', '@gamerxone720', '@pnf4lyfe', '@yanks2013', '@virtuame', 'lol', \"it's\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', \"bj's\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  67, 1230, 5581,  129,  897,   57, 5442,   34,   38, 4476,  244,\n",
      "        621,  731,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     714,     417,    5581,   21025,   14331,     212,\n",
      "          3626,     329, 1193515,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 10)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,    2241,       2,     329,       2,     400, 1193515,\n",
      "       1193515,     895,      93,    5581,    5720,     100,      14,\n",
      "           412,   17807,      40,    1657,     235,   48173,       2,\n",
      "           194,       5,      87,      12,    1831,     126,      10,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 2, 28)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([     13,   75686, 1193515,       3,      12, 1193515,     625,\n",
      "         13723,      38, 1193515,      36,    2935,    1939,      47,\n",
      "          2632, 1193515,       5, 1193515,       5,    5581,    7161,\n",
      "       1193515,    3380,     354,      36,      14,    9702,     200,\n",
      "       1193515,      13,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 30)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     129,      57,    2555,      17,     164,      75,\n",
      "          5581,   45964,      14,    1903,    3949,      36,      14,\n",
      "       1193515,      29,      34,     531,      65,    1006,      12,\n",
      "           321,     577,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 0, 23)\n",
      "\n",
      "BaseLSTM(\n",
      "  (embeddings): Embedding(1193516, 50)\n",
      "  (lstm): LSTM(50, 8)\n",
      "  (linear): Linear(in_features=24, out_features=3, bias=True)\n",
      ")\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 188, in <module>\n",
      "    train_dataset(epoch, train_loader, model, criterion, optimizer,n_classes)\n",
      "  File \"/home/dorotheakal/NLP/lab3/slp-lab3-prep/training.py\", line 49, in train_dataset\n",
      "    output = model(inputs,lengths)\n",
      "  File \"/home/dorotheakal/anaconda3/envs/slp3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/dorotheakal/NLP/lab3/slp-lab3-prep/models.py\", line 207, in forward\n",
      "    rep = torch.cat( (hn, rep),dim = 1) # BS x 3HS\n",
      "RuntimeError: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/TH/generic/THTensor.cpp:680\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "<user>\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@personasoda', 'well', 'yeah', ',', \"that's\", 'third', 'parties', '.', 'sony', 'itself', \"isn't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', \"it's\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y', '.', 'o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@fakethom', 'have', 'android', 'tab', 'and', \"don't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':-)']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', \"mg's\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@askplaystation', 'why', \"won't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '\"', 'support', 'team', '\"', '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "[\"sony's\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https://t.co/xbiscrkpl4', '#mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http://t.co/mulhw4cn00', '#sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@truthinsider', '@bertymufc', '@gamerxone720', '@pnf4lyfe', '@yanks2013', '@virtuame', 'lol', \"it's\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', \"bj's\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  67, 1230, 5581,  129,  897,   57, 5442,   34,   38, 4476,  244,\n",
      "        621,  731,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     714,     417,    5581,   21025,   14331,     212,\n",
      "          3626,     329, 1193515,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 10)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,    2241,       2,     329,       2,     400, 1193515,\n",
      "       1193515,     895,      93,    5581,    5720,     100,      14,\n",
      "           412,   17807,      40,    1657,     235,   48173,       2,\n",
      "           194,       5,      87,      12,    1831,     126,      10,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 2, 28)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([     13,   75686, 1193515,       3,      12, 1193515,     625,\n",
      "         13723,      38, 1193515,      36,    2935,    1939,      47,\n",
      "          2632, 1193515,       5, 1193515,       5,    5581,    7161,\n",
      "       1193515,    3380,     354,      36,      14,    9702,     200,\n",
      "       1193515,      13,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 30)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     129,      57,    2555,      17,     164,      75,\n",
      "          5581,   45964,      14,    1903,    3949,      36,      14,\n",
      "       1193515,      29,      34,     531,      65,    1006,      12,\n",
      "           321,     577,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 0, 23)\n",
      "\n",
      "BaseLSTM(\n",
      "  (embeddings): Embedding(1193516, 50)\n",
      "  (lstm): LSTM(50, 8)\n",
      "  (attention): Attention(\n",
      "    (tanh): Tanh()\n",
      "    (softmax): Softmax(dim=-1)\n",
      "    (lin_attention): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 1.0364\n",
      " Epoch 1, Train loss: 0.8854\n",
      "           Test loss: 0.9499\n",
      " [========================================] ...Epoch 2, Loss: 0.7607\n",
      " Epoch 2, Train loss: 0.8361\n",
      "           Test loss: 0.9004\n",
      " [========================================] ...Epoch 3, Loss: 0.7116\n",
      " Epoch 3, Train loss: 0.8094\n",
      "           Test loss: 0.8708\n",
      " [========================================] ...Epoch 4, Loss: 0.8208\n",
      " Epoch 4, Train loss: 0.7926\n",
      "           Test loss: 0.8441\n",
      " [========================================] ...Epoch 5, Loss: 1.0911\n",
      " Epoch 5, Train loss: 0.7800\n",
      "           Test loss: 0.8282\n",
      " [========================================] ...Epoch 6, Loss: 0.6278\n",
      " Epoch 6, Train loss: 0.7707\n",
      "           Test loss: 0.8242\n",
      " [========================================] ...Epoch 7, Loss: 0.6645\n",
      " Epoch 7, Train loss: 0.7640\n",
      "           Test loss: 0.8257\n",
      " [========================================] ...Epoch 8, Loss: 0.7841\n",
      " Epoch 8, Train loss: 0.7580\n",
      "           Test loss: 0.8158\n",
      " [========================================] ...Epoch 9, Loss: 0.7344\n",
      " Epoch 9, Train loss: 0.7527\n",
      "           Test loss: 0.8063\n",
      " [========================================] ...Epoch 10, Loss: 0.6990\n",
      " Epoch 10, Train loss: 0.7499\n",
      "           Test loss: 0.7957\n",
      " [========================================] ...Epoch 11, Loss: 0.8047\n",
      " Epoch 11, Train loss: 0.7449\n",
      "           Test loss: 0.8065\n",
      " [========================================] ...Epoch 12, Loss: 0.5822\n",
      " Epoch 12, Train loss: 0.7408\n",
      "           Test loss: 0.7965\n",
      " [========================================] ...Epoch 13, Loss: 0.5085\n",
      " Epoch 13, Train loss: 0.7394\n",
      "           Test loss: 0.8051\n",
      " [========================================] ...Epoch 14, Loss: 0.7322\n",
      " Epoch 14, Train loss: 0.7376\n",
      "           Test loss: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [========================================] ...Epoch 15, Loss: 0.7754\n",
      " Epoch 15, Train loss: 0.7343\n",
      "           Test loss: 0.7925\n",
      " [========================================] ...Epoch 16, Loss: 0.6091\n",
      " Epoch 16, Train loss: 0.7311\n",
      "           Test loss: 0.7923\n",
      " [========================================] ...Epoch 17, Loss: 0.6638\n",
      " Epoch 17, Train loss: 0.7299\n",
      "           Test loss: 0.7828\n",
      " [========================================] ...Epoch 18, Loss: 0.5837\n",
      " Epoch 18, Train loss: 0.7304\n",
      "           Test loss: 0.7796\n",
      " [========================================] ...Epoch 19, Loss: 0.6890\n",
      " Epoch 19, Train loss: 0.7307\n",
      "           Test loss: 0.7849\n",
      " [========================================] ...Epoch 20, Loss: 0.7649\n",
      " Epoch 20, Train loss: 0.7255\n",
      "           Test loss: 0.7838\n",
      " [========================================] ...Epoch 21, Loss: 0.6003\n",
      " Epoch 21, Train loss: 0.7270\n",
      "           Test loss: 0.7987\n",
      " [========================================] ...Epoch 22, Loss: 0.7498\n",
      " Epoch 22, Train loss: 0.7277\n",
      "           Test loss: 0.7773\n",
      " [========================================] ...Epoch 23, Loss: 1.0280\n",
      " Epoch 23, Train loss: 0.7214\n",
      "           Test loss: 0.7802\n",
      " [========================================] ...Epoch 24, Loss: 0.5448\n",
      " Epoch 24, Train loss: 0.7218\n",
      "           Test loss: 0.7944\n",
      " [========================================] ...Epoch 25, Loss: 0.6527\n",
      " Epoch 25, Train loss: 0.7197\n",
      "           Test loss: 0.7818\n",
      " [========================================] ...Epoch 26, Loss: 0.8584\n",
      " Epoch 26, Train loss: 0.7211\n",
      "           Test loss: 0.8003\n",
      " [========================================] ...Epoch 27, Loss: 0.5981\n",
      " Epoch 27, Train loss: 0.7192\n",
      "           Test loss: 0.7947\n",
      " [========================================] ...Epoch 28, Loss: 0.7011\n",
      " Epoch 28, Train loss: 0.7215\n",
      "           Test loss: 0.7773\n",
      " [========================================] ...Epoch 29, Loss: 0.7185\n",
      " Epoch 29, Train loss: 0.7148\n",
      "           Test loss: 0.7774\n",
      " [========================================] ...Epoch 30, Loss: 0.6832\n",
      " Epoch 30, Train loss: 0.7161\n",
      "           Test loss: 0.7732\n",
      " [========================================] ...Epoch 31, Loss: 0.6953\n",
      " Epoch 31, Train loss: 0.7140\n",
      "           Test loss: 0.7795\n",
      " [========================================] ...Epoch 32, Loss: 0.7556\n",
      " Epoch 32, Train loss: 0.7128\n",
      "           Test loss: 0.7798\n",
      " [========================================] ...Epoch 33, Loss: 0.8798\n",
      " Epoch 33, Train loss: 0.7126\n",
      "           Test loss: 0.7763\n",
      " [========================================] ...Epoch 34, Loss: 0.6959\n",
      " Epoch 34, Train loss: 0.7111\n",
      "           Test loss: 0.7760\n",
      " [========================================] ...Epoch 35, Loss: 1.0073\n",
      " Epoch 35, Train loss: 0.7106\n",
      "           Test loss: 0.7802\n",
      " [========================================] ...Epoch 36, Loss: 0.9907\n",
      " Epoch 36, Train loss: 0.7099\n",
      "           Test loss: 0.7798\n",
      " [========================================] ...Epoch 37, Loss: 0.5496\n",
      " Epoch 37, Train loss: 0.7113\n",
      "           Test loss: 0.7717\n",
      " [========================================] ...Epoch 38, Loss: 0.6915\n",
      " Epoch 38, Train loss: 0.7096\n",
      "           Test loss: 0.7721\n",
      " [========================================] ...Epoch 39, Loss: 0.5564\n",
      " Epoch 39, Train loss: 0.7105\n",
      "           Test loss: 0.7886\n",
      " [========================================] ...Epoch 40, Loss: 1.0025\n",
      " Epoch 40, Train loss: 0.7075\n",
      "           Test loss: 0.7817\n",
      " [========================================] ...Epoch 41, Loss: 0.8393\n",
      " Epoch 41, Train loss: 0.7118\n",
      "           Test loss: 0.7745\n",
      " [========================================] ...Epoch 42, Loss: 0.6925\n",
      " Epoch 42, Train loss: 0.7078\n",
      "           Test loss: 0.7760\n",
      " [========================================] ...Epoch 43, Loss: 0.6975\n",
      " Epoch 43, Train loss: 0.7062\n",
      "           Test loss: 0.7732\n",
      " [========================================] ...Epoch 44, Loss: 0.7413\n",
      " Epoch 44, Train loss: 0.7069\n",
      "           Test loss: 0.7654\n",
      " [========================================] ...Epoch 45, Loss: 0.7416\n",
      " Epoch 45, Train loss: 0.7053\n",
      "           Test loss: 0.7807\n",
      " [========================================] ...Epoch 46, Loss: 0.7227\n",
      " Epoch 46, Train loss: 0.7059\n",
      "           Test loss: 0.7801\n",
      " [========================================] ...Epoch 47, Loss: 0.6455\n",
      " Epoch 47, Train loss: 0.7052\n",
      "           Test loss: 0.7831\n",
      " [========================================] ...Epoch 48, Loss: 0.5537\n",
      " Epoch 48, Train loss: 0.7043\n",
      "           Test loss: 0.7736\n",
      " [========================================] ...Epoch 49, Loss: 0.6994\n",
      " Epoch 49, Train loss: 0.7036\n",
      "           Test loss: 0.7801\n",
      " [========================================] ...Epoch 50, Loss: 0.6537\n",
      " Epoch 50, Train loss: 0.7079\n",
      "           Test loss: 0.7909\n",
      "Figure(800x800)\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "<user>\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@personasoda', 'well', 'yeah', ',', \"that's\", 'third', 'parties', '.', 'sony', 'itself', \"isn't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', \"it's\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y', '.', 'o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@fakethom', 'have', 'android', 'tab', 'and', \"don't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':-)']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', \"mg's\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@askplaystation', 'why', \"won't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '\"', 'support', 'team', '\"', '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "[\"sony's\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https://t.co/xbiscrkpl4', '#mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http://t.co/mulhw4cn00', '#sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@truthinsider', '@bertymufc', '@gamerxone720', '@pnf4lyfe', '@yanks2013', '@virtuame', 'lol', \"it's\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', \"bj's\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([  67, 1230, 5581,  129,  897,   57, 5442,   34,   38, 4476,  244,\n",
      "        621,  731,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     714,     417,    5581,   21025,   14331,     212,\n",
      "          3626,     329, 1193515,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 10)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,    2241,       2,     329,       2,     400, 1193515,\n",
      "       1193515,     895,      93,    5581,    5720,     100,      14,\n",
      "           412,   17807,      40,    1657,     235,   48173,       2,\n",
      "           194,       5,      87,      12,    1831,     126,      10,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 2, 28)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([     13,   75686, 1193515,       3,      12, 1193515,     625,\n",
      "         13723,      38, 1193515,      36,    2935,    1939,      47,\n",
      "          2632, 1193515,       5, 1193515,       5,    5581,    7161,\n",
      "       1193515,    3380,     354,      36,      14,    9702,     200,\n",
      "       1193515,      13,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 1, 30)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([1193515,     129,      57,    2555,      17,     164,      75,\n",
      "          5581,   45964,      14,    1903,    3949,      36,      14,\n",
      "       1193515,      29,      34,     531,      65,    1006,      12,\n",
      "           321,     577,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0,       0,       0,       0,\n",
      "             0,       0,       0,       0]), 0, 23)\n",
      "\n",
      "BaseLSTM(\n",
      "  (embeddings): Embedding(1193516, 50)\n",
      "  (lstm): LSTM(50, 8, bidirectional=True)\n",
      "  (linear): Linear(in_features=48, out_features=3, bias=True)\n",
      ")\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 188, in <module>\n",
      "    train_dataset(epoch, train_loader, model, criterion, optimizer,n_classes)\n",
      "  File \"/home/dorotheakal/NLP/lab3/slp-lab3-prep/training.py\", line 49, in train_dataset\n",
      "    output = model(inputs,lengths)\n",
      "  File \"/home/dorotheakal/anaconda3/envs/slp3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/dorotheakal/NLP/lab3/slp-lab3-prep/models.py\", line 207, in forward\n",
      "    rep = torch.cat( (hn, rep),dim = 1) # BS x 3HS\n",
      "RuntimeError: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/TH/generic/THTensor.cpp:680\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_pooling_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python main.py LSTM_attention_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word embeddings from cache.\n",
      "Traceback (most recent call last):\n",
      "  File \"EI_reg.py\", line 64, in <module>\n",
      "    checkpoint = torch.load('./checkpoints/Semeval2017A/LSTM_attention_B')['model']\n",
      "  File \"/home/dorotheakal/anaconda3/envs/slp3/lib/python3.7/site-packages/torch/serialization.py\", line 419, in load\n",
      "    f = open(f, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/Semeval2017A/LSTM_attention_B'\n"
     ]
    }
   ],
   "source": [
    "!python EI_reg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με TF-IDF!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n",
      "\n",
      "\u001b[1mQuestion 1:\u001b[0m\n",
      "\n",
      "\n",
      "Labels for 10 first training examples:\n",
      "\n",
      "Original: ['neutral' 'neutral' 'negative' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral']\n",
      "\n",
      "After LabelEncoder: [1 1 0 1 2 0 1 1 1 1]\n",
      "\n",
      "<user>\n",
      "\n",
      "\u001b[1mQuestion 2:\u001b[0m\n",
      "\n",
      "Tokenized sample:\n",
      "['@seemonterey', 'lost', '-', 'sony', 'cell', 'phone', 'with', 'holiday', 'photos', '.', 'early', 'fri', 'morning', ',', 'montreal', 'transit', 'plaza', 'or', 'no', '.', '13', 'bus', 'to', 'airport', '.', 'reward', '!', 'plz', 'rt', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['@personasoda', 'well', 'yeah', ',', \"that's\", 'third', 'parties', '.', 'sony', 'itself', \"isn't\", 'putting', 'out', 'actual', 'games', 'for', 'it', '.', \"it's\", 'got', '1-2', 'yrs', 'of', '3rd', 'party', 'support', 'left', '.']\n",
      "\n",
      "Tokenized sample:\n",
      "['sony', 'rewards', 'app', 'is', 'like', 'a', 'lot', 'of', '19', 'y', '.', 'o', 'female', 'singers', 'and', 'a', 'non', 'retro', 'sale', '.', '2nd', 'one', 'with', 'no', 'info']\n",
      "\n",
      "Tokenized sample:\n",
      "['@fakethom', 'have', 'android', 'tab', 'and', \"don't\", 'use', 'phone', 'much', ',', 'in', 'fact', 'very', 'little', '.', 'may', 'go', 'the', 'sony', 'route', 'then', ':-)']\n",
      "\n",
      "Tokenized sample:\n",
      "['finally', 'i', 'get', 'my', 'ps4', 'back', 'i', 'sent', 'it', 'to', 'sony', 'cause', 'my', 'hdmi', 'was', 'mess', 'up', 'now', 'i', 'can', 'play', \"mg's\", 'tuesday', 'yeaaaaa', 'buddy']\n",
      "\n",
      "Tokenized sample:\n",
      "['@askplaystation', 'why', \"won't\", 'u', 'guys', 'help', 'me', 'out', '?', '!', 'im', 'calling', 'sony', 'tomorrow', '!', 'i', 'want', 'help', 'but', 'the', '\"', 'support', 'team', '\"', '3', 'hours', 'of', 'tweeting', 'and', 'nothing']\n",
      "\n",
      "Tokenized sample:\n",
      "[\"sony's\", '1st', 'teaser', 'package', 'for', 'the', 'launch', 'of', 'the', 'original', 'playstation', 'seems', 'to', 'feature', 'a', 'dominatrix', '?', 'https://t.co/xbiscrkpl4', '#mistresssophia']\n",
      "\n",
      "Tokenized sample:\n",
      "['#tv', 'ind', 'vs', 'sl', '3rd', 'test', 'day', '3', ':', 'cricket', 'live', 'score', 'and', 'sony', 'six', 'live', 'streaming', 'info', ':', 'watch', 'the', 'live', 'teleca', '...', 'http://t.co/mulhw4cn00', '#sony']\n",
      "\n",
      "Tokenized sample:\n",
      "['@truthinsider', '@bertymufc', '@gamerxone720', '@pnf4lyfe', '@yanks2013', '@virtuame', 'lol', \"it's\", 'all', 'about', 'sony', 'sony', 'sony', ',', 'if', 'sony', 'gave', \"bj's\", 'u', 'be', 'the', '1st']\n",
      "\n",
      "Tokenized sample:\n",
      "['@greencapt', 'official', 'reason', ',', 'because', 'the', 'game', 'has', 'to', 'be', 'on', 'our', 'region', 'store', 'and', 'sony', 'wont', 'have', 'it', 'up', 'til', 'tuesday']\n",
      "\n",
      "\n",
      "\u001b[1mQuestion 3:\u001b[0m\n",
      "\n",
      "Original sample:\n",
      "At least Sony will probably be selling it for cheap come Black Friday.\n",
      "\n",
      "Transformed sample:\n",
      "(array([6.70000000e+01, 1.23000000e+03, 5.58100000e+03, 1.29000000e+02,\n",
      "       8.97000000e+02, 5.70000000e+01, 5.44200000e+03, 3.40000000e+01,\n",
      "       3.80000000e+01, 4.47600000e+03, 2.44000000e+02, 6.21000000e+02,\n",
      "       7.31000000e+02, 2.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.54940877e-01, 3.42715695e-01, 3.71489243e-01, 1.88340043e-01,\n",
      "       3.26380930e-01, 1.51633159e-01, 3.72911318e-01, 1.54293464e-01,\n",
      "       1.36976438e-01, 4.29500521e-01, 2.46023438e-01, 2.82627275e-01,\n",
      "       2.01492909e-01, 9.18599646e-02, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]), 1, 14)\n",
      "\n",
      "Original sample:\n",
      "@InnoBystander Might keep SONY monthly subs going beyond tomorrow....\n",
      "\n",
      "Transformed sample:\n",
      "(array([1.19351500e+06, 7.14000000e+02, 4.17000000e+02, 5.58100000e+03,\n",
      "       2.10250000e+04, 1.43310000e+04, 2.12000000e+02, 3.62600000e+03,\n",
      "       3.29000000e+02, 1.19351500e+06, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 2.90563459e-01, 3.09041871e-01, 3.59872570e-01,\n",
      "       4.87798786e-01, 4.69140903e-01, 1.90077897e-01, 4.18889347e-01,\n",
      "       1.44634009e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]), 1, 10)\n",
      "\n",
      "Original sample:\n",
      "@tauriqmoosa Nope. Tomorrow. Wait... tomorrow's also when Sony breaks out the next bundle of PS+ freebies. Oh, what a LOVELY day!\n",
      "\n",
      "Transformed sample:\n",
      "(array([1.19351500e+06, 2.24100000e+03, 2.00000000e+00, 3.29000000e+02,\n",
      "       2.00000000e+00, 4.00000000e+02, 1.19351500e+06, 1.19351500e+06,\n",
      "       8.95000000e+02, 9.30000000e+01, 5.58100000e+03, 5.72000000e+03,\n",
      "       1.00000000e+02, 1.40000000e+01, 4.12000000e+02, 1.78070000e+04,\n",
      "       4.00000000e+01, 1.65700000e+03, 2.35000000e+02, 4.81730000e+04,\n",
      "       2.00000000e+00, 1.94000000e+02, 5.00000000e+00, 8.70000000e+01,\n",
      "       1.20000000e+01, 1.83100000e+03, 1.26000000e+02, 1.00000000e+01,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 3.04663060e-01, 1.89054907e-01, 1.02425560e-01,\n",
      "       1.89054907e-01, 1.81523410e-01, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.98864456e-01, 1.55711933e-01, 2.54851191e-01, 2.89248533e-01,\n",
      "       1.36048043e-01, 5.36193291e-02, 1.75265272e-01, 3.35107886e-01,\n",
      "       9.20702180e-02, 2.94648423e-01, 2.06596469e-01, 3.66564938e-01,\n",
      "       1.89054907e-01, 2.10484763e-01, 8.28767819e-02, 1.55915743e-01,\n",
      "       8.64602688e-02, 2.66053953e-01, 1.28532142e-01, 9.26347655e-02,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]), 2, 28)\n",
      "\n",
      "Original sample:\n",
      "\"Uncharted 4: A Thief's End launches for PS4 in North America on March 18, 2016, Sony announced today.The latest game in the Naughty D...\"\n",
      "\n",
      "Transformed sample:\n",
      "(array([1.30000000e+01, 7.56860000e+04, 1.19351500e+06, 3.00000000e+00,\n",
      "       1.20000000e+01, 1.19351500e+06, 6.25000000e+02, 1.37230000e+04,\n",
      "       3.80000000e+01, 1.19351500e+06, 3.60000000e+01, 2.93500000e+03,\n",
      "       1.93900000e+03, 4.70000000e+01, 2.63200000e+03, 1.19351500e+06,\n",
      "       5.00000000e+00, 1.19351500e+06, 5.00000000e+00, 5.58100000e+03,\n",
      "       7.16100000e+03, 1.19351500e+06, 3.38000000e+03, 3.54000000e+02,\n",
      "       3.60000000e+01, 1.40000000e+01, 9.70200000e+03, 2.00000000e+02,\n",
      "       1.19351500e+06, 1.30000000e+01, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.76482081e-01, 3.41073223e-01, 0.00000000e+00, 1.14640399e-01,\n",
      "       8.94783278e-02, 0.00000000e+00, 2.09631043e-01, 3.15297900e-01,\n",
      "       9.72495435e-02, 0.00000000e+00, 1.74403247e-01, 2.74444979e-01,\n",
      "       2.50022021e-01, 8.79167598e-02, 2.02529845e-01, 0.00000000e+00,\n",
      "       1.71539505e-01, 0.00000000e+00, 1.71539505e-01, 2.63747254e-01,\n",
      "       2.45898075e-01, 0.00000000e+00, 2.72294905e-01, 1.61252665e-01,\n",
      "       1.74403247e-01, 5.54910130e-02, 3.72580783e-01, 2.45085236e-01,\n",
      "       0.00000000e+00, 1.76482081e-01, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]), 1, 30)\n",
      "\n",
      "Original sample:\n",
      "@kewldoode72 Will be interesting to see if Sony addresses the heat issues in the Z5 - it may have found a work around\n",
      "\n",
      "Transformed sample:\n",
      "(array([1.19351500e+06, 1.29000000e+02, 5.70000000e+01, 2.55500000e+03,\n",
      "       1.70000000e+01, 1.64000000e+02, 7.50000000e+01, 5.58100000e+03,\n",
      "       4.59640000e+04, 1.40000000e+01, 1.90300000e+03, 3.94900000e+03,\n",
      "       3.60000000e+01, 1.40000000e+01, 1.19351500e+06, 2.90000000e+01,\n",
      "       3.40000000e+01, 5.31000000e+02, 6.50000000e+01, 1.00600000e+03,\n",
      "       1.20000000e+01, 3.21000000e+02, 5.77000000e+02, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 1.55775268e-01, 1.25415157e-01, 3.01812345e-01,\n",
      "       8.62063186e-02, 1.65406805e-01, 1.64223034e-01, 3.07257213e-01,\n",
      "       4.34044833e-01, 1.29290552e-01, 3.06490459e-01, 3.36517896e-01,\n",
      "       1.01587135e-01, 1.29290552e-01, 0.00000000e+00, 1.57023440e-01,\n",
      "       1.27615484e-01, 1.27487218e-01, 1.45784059e-01, 2.80648939e-01,\n",
      "       1.04239423e-01, 2.30747419e-01, 2.62323381e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]), 0, 23)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseLSTM(\n",
      "  (embeddings): Embedding(1193516, 50)\n",
      "  (lstm): LSTM(50, 8)\n",
      "  (attention): Attention(\n",
      "    (tanh): Tanh()\n",
      "    (softmax): Softmax(dim=-1)\n",
      "    (lin_attention): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n",
      " [========================================] ...Epoch 1, Loss: 0.8312\n",
      " Epoch 1, Train loss: 0.8844\n",
      "           Test loss: 0.9690\n",
      " [========================================] ...Epoch 2, Loss: 0.7467\n",
      " Epoch 2, Train loss: 0.8426\n",
      "           Test loss: 0.9175\n",
      " [========================================] ...Epoch 3, Loss: 0.7554\n",
      " Epoch 3, Train loss: 0.8181\n",
      "           Test loss: 0.8805\n",
      " [========================================] ...Epoch 4, Loss: 0.8414\n",
      " Epoch 4, Train loss: 0.8029\n",
      "           Test loss: 0.8640\n",
      " [========================================] ...Epoch 5, Loss: 0.6589\n",
      " Epoch 5, Train loss: 0.7898\n",
      "           Test loss: 0.8286\n",
      " [========================================] ...Epoch 6, Loss: 0.7819\n",
      " Epoch 6, Train loss: 0.7771\n",
      "           Test loss: 0.8219\n",
      " [========================================] ...Epoch 7, Loss: 0.7459\n",
      " Epoch 7, Train loss: 0.7723\n",
      "           Test loss: 0.8322\n",
      " [========================================] ...Epoch 8, Loss: 0.9693\n",
      " Epoch 8, Train loss: 0.7647\n",
      "           Test loss: 0.8202\n",
      " [========================================] ...Epoch 9, Loss: 0.6973\n",
      " Epoch 9, Train loss: 0.7582\n",
      "           Test loss: 0.7988\n",
      " [========================================] ...Epoch 10, Loss: 0.8261\n",
      " Epoch 10, Train loss: 0.7532\n",
      "           Test loss: 0.7988\n",
      " [========================================] ...Epoch 11, Loss: 0.9318\n",
      " Epoch 11, Train loss: 0.7480\n",
      "           Test loss: 0.7956\n",
      " [========================================] ...Epoch 12, Loss: 0.6956\n",
      " Epoch 12, Train loss: 0.7454\n",
      "           Test loss: 0.7890\n",
      " [========================================] ...Epoch 13, Loss: 0.7293\n",
      " Epoch 13, Train loss: 0.7444\n",
      "           Test loss: 0.7871\n",
      " [========================================] ...Epoch 14, Loss: 0.8495\n",
      " Epoch 14, Train loss: 0.7385\n",
      "           Test loss: 0.7894\n",
      " [========================================] ...Epoch 15, Loss: 0.5921\n",
      " Epoch 15, Train loss: 0.7382\n",
      "           Test loss: 0.8009\n",
      " [========================================] ...Epoch 16, Loss: 0.9408\n",
      " Epoch 16, Train loss: 0.7346\n",
      "           Test loss: 0.7855\n",
      " [========================================] ...Epoch 17, Loss: 0.5988\n",
      " Epoch 17, Train loss: 0.7322\n",
      "           Test loss: 0.7887\n",
      " [========================================] ...Epoch 18, Loss: 0.6904\n",
      " Epoch 18, Train loss: 0.7305\n",
      "           Test loss: 0.7855\n",
      " [========================================] ...Epoch 19, Loss: 0.7173\n",
      " Epoch 19, Train loss: 0.7318\n",
      "           Test loss: 0.7960\n",
      " [========================================] ...Epoch 20, Loss: 0.6246\n",
      " Epoch 20, Train loss: 0.7274\n",
      "           Test loss: 0.7912\n",
      " [========================================] ...Epoch 21, Loss: 0.5892\n",
      " Epoch 21, Train loss: 0.7258\n",
      "           Test loss: 0.7799\n",
      " [========================================] ...Epoch 22, Loss: 0.7524\n",
      " Epoch 22, Train loss: 0.7238\n",
      "           Test loss: 0.7892\n",
      " [========================================] ...Epoch 23, Loss: 0.6862\n",
      " Epoch 23, Train loss: 0.7224\n",
      "           Test loss: 0.7888\n",
      " [========================================] ...Epoch 24, Loss: 0.6838\n",
      " Epoch 24, Train loss: 0.7203\n",
      "           Test loss: 0.7824\n",
      " [========================================] ...Epoch 25, Loss: 0.7104\n",
      " Epoch 25, Train loss: 0.7220\n",
      "           Test loss: 0.7794\n",
      " [========================================] ...Epoch 26, Loss: 0.8769\n",
      " Epoch 26, Train loss: 0.7195\n",
      "           Test loss: 0.7778\n",
      " [========================================] ...Epoch 27, Loss: 0.6388\n",
      " Epoch 27, Train loss: 0.7206\n",
      "           Test loss: 0.7900\n",
      " [========================================] ...Epoch 28, Loss: 0.7586\n",
      " Epoch 28, Train loss: 0.7186\n",
      "           Test loss: 0.7765\n",
      " [========================================] ...Epoch 29, Loss: 0.8909\n",
      " Epoch 29, Train loss: 0.7169\n",
      "           Test loss: 0.7824\n",
      " [========================================] ...Epoch 30, Loss: 0.5275\n",
      " Epoch 30, Train loss: 0.7167\n",
      "           Test loss: 0.7761\n",
      " [========================================] ...Epoch 31, Loss: 0.6958\n",
      " Epoch 31, Train loss: 0.7154\n",
      "           Test loss: 0.7910\n",
      " [========================================] ...Epoch 32, Loss: 0.8667\n",
      " Epoch 32, Train loss: 0.7152\n",
      "           Test loss: 0.7840\n",
      " [========================================] ...Epoch 33, Loss: 0.4972\n",
      " Epoch 33, Train loss: 0.7155\n",
      "           Test loss: 0.7915\n",
      " [========================================] ...Epoch 34, Loss: 0.7632\n",
      " Epoch 34, Train loss: 0.7124\n",
      "           Test loss: 0.7740\n",
      " [========================================] ...Epoch 35, Loss: 0.9317\n",
      " Epoch 35, Train loss: 0.7110\n",
      "           Test loss: 0.7776\n",
      " [========================================] ...Epoch 36, Loss: 0.7985\n",
      " Epoch 36, Train loss: 0.7121\n",
      "           Test loss: 0.7765\n",
      " [========================================] ...Epoch 37, Loss: 0.7026\n",
      " Epoch 37, Train loss: 0.7105\n",
      "           Test loss: 0.7762\n",
      " [========================================] ...Epoch 38, Loss: 0.5358\n",
      " Epoch 38, Train loss: 0.7104\n",
      "           Test loss: 0.7742\n",
      " [========================================] ...Epoch 39, Loss: 0.6361\n",
      " Epoch 39, Train loss: 0.7084\n",
      "           Test loss: 0.7779\n",
      " [========================================] ...Epoch 40, Loss: 0.6931\n",
      " Epoch 40, Train loss: 0.7080\n",
      "           Test loss: 0.7767\n",
      " [========================================] ...Epoch 41, Loss: 0.8498\n",
      " Epoch 41, Train loss: 0.7077\n",
      "           Test loss: 0.7758\n",
      " [========================================] ...Epoch 42, Loss: 0.6131\n",
      " Epoch 42, Train loss: 0.7079\n",
      "           Test loss: 0.7853\n",
      " [========================================] ...Epoch 43, Loss: 0.5878\n",
      " Epoch 43, Train loss: 0.7076\n",
      "           Test loss: 0.7826\n",
      " [========================================] ...Epoch 44, Loss: 0.6129\n",
      " Epoch 44, Train loss: 0.7055\n",
      "           Test loss: 0.7779\n",
      " [========================================] ...Epoch 45, Loss: 0.5274\n",
      " Epoch 45, Train loss: 0.7061\n",
      "           Test loss: 0.7816\n",
      " [========================================] ...Epoch 46, Loss: 0.7256\n",
      " Epoch 46, Train loss: 0.7056\n",
      "           Test loss: 0.7760\n",
      " [========================================] ...Epoch 47, Loss: 0.5717\n",
      " Epoch 47, Train loss: 0.7042\n",
      "           Test loss: 0.7775\n",
      " [========================================] ...Epoch 48, Loss: 0.7209\n",
      " Epoch 48, Train loss: 0.7042\n",
      "           Test loss: 0.7735\n",
      " [========================================] ...Epoch 49, Loss: 0.8286\n",
      " Epoch 49, Train loss: 0.7038\n",
      "           Test loss: 0.7785\n",
      " [========================================] ...Epoch 50, Loss: 0.6339\n"
     ]
    }
   ],
   "source": [
    "!python main.py LSTM_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
