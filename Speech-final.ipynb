{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Επεξεργασία Φωνής και Φυσικής Γλώσσας</h1> \n",
    "<h2 align = \"center\">1η Εργασία</h2> \n",
    "<h3 align = \"center\"> Θεoδωρόπουλος Νικήτας -03115185</h3>\n",
    "<h3 align = \"center\"> Καλλιώρα Δωροθέα - 03115176</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Σκοπός \n",
    "\n",
    "Στόχος της εργαστηριακής άσκησης είναι η δημιουργία ενός απλού ορθογράφου με χρήση μηχανών πεπερασμένων καταστάσεων (**fst**) με τη βοήθεια της βιβλιοθήκης openfst (v1.6.1). Για την εκπαίδευση του μοντέλου χρησιμοποιούμε corpus απο δημόσια διαθέσιμα βιβλία απο τα οποία με κατάλληλο tokenization γίνεται εξαγωγή λέξεων και σχηματισμός λεξικού. Για κάθε λέξη προς διόρθωση υπολογίζουμε την απόσταση Levenshtein πάνω στο λεξικό, η λέξη με την ελάχιστη απόσταση είναι η πρόβλεψη του μοντέλο μας.\n",
    "\n",
    "Τέλος θα γίνει εισαγωγή σε αναπαραστάσεις **word2vec**. Ενα σύνολο μοντέλων που χρησιμοποιούνται για παραγωγή αναπαραστάσεων λέξεων (embeddigns) σε έναν d-διάσταστο διανυσματικό χώρο $\\mathbb{R}^d$, έτσι ώστε λέξεις με κοντινή σημασία να βρίσκονται κοντά και στον διανυσματικό χώρο. Η βασική υπόθεση είναι ότι λέξεις με κοινή κατανομή στο κείμενο θα έχουν και κοινή σημασία. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 1\n",
    "\n",
    "Κατασκευάζουμε το corpus με σύμπτηξη plain text βιβλίων, δημόσια διαθέσιμα στο Project Gutenberg. Θα χρησιμοποιήσουμε corpus των δυο παρακάτω βιβλίων:  \n",
    "<br>\n",
    "<div class=\"image123\">\n",
    "    <div class=\"imgContainer\"  Style = \"float:left\">\n",
    "        <p>The War of the Worlds by H. G. Wells</p>\n",
    "        <img src=\"./img/book36.jpg\" height=auto width=\"250\"/>\n",
    "    </div>\n",
    "    <div class=\"imgContainer\" Style = \"float:right\">\n",
    "        <p>Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm</p>\n",
    "        <img class=\"middle-img\" src=\"./img/book2591.jpg\"/ height=auto width=\"250\"/>\n",
    "    </div>\n",
    "    <div class=\"imgContainer\" Style = \"float:middle\">\n",
    "        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "            Pride and Prejudice by Jane Austen </p>\n",
    "        <img src=\"./img/book1342.jpg\"/ height=auto width=\"250\" align=\"center\"/>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(β) Κατα την κατασκευή γλωσσικών μοντέλων ειναι κοινή πρακτική η σύμπτηξη πολλών βιβλίων για την δημιουργία ενός ενιαίου corpus προς επεξεργασία. Προφανώς για οποιοδήποτε μοντέλο η αύξηση των δεδομένων εκπαίδευσης οδηγεί σε μεγαλύτερη ικανοτητα γενίκευσης. \n",
    "\n",
    "* Για το μοντέλο μας η επιλογή μιας μόνο πηγής δεδομένων εισάγει μεγάλη προκατάληψη (bias) καθώς περιοριζόμαστε στο λεξιλόγιο ενός μόνο συγγραφέα μιας συγκεκριμένης εποχής ή και του context του βιβλίου (όπως η κοινωνική τάξη των χαρακτήρων αν είναι αφηγηματικό). Για να μπορεί να διορθώσει σωστά μια λέξη το μοντέλο μας θα πρέπει πρώτα να την γνωρίζει, συνεπώς η ποικιλία στο λεξιλιλόγιο είναι ενας καθοριστικός παράγωντας για την επιτυχία του μοντέλου. \n",
    "\n",
    "* Σε αναπαραστάσεις που βασίζονται στα συμφραζόμενα, όπως το word2vec, απαιτείται μεγάλο πλήθος δεδομένων έτσι ώστε να προσδιοριστεί σωστά η σημασία μιας λέξης. Αυτο συμβαίνει γιατί πρέπει να αναλυθεί η χρήση της και να αναγνωριστεί η θέση της σε διαφορετικά γλωσσικά περιβάλλοντα και ισχύει ακόμα και για σταθερό λεξιλόγιο. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for step 1\n",
    "\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/36/36-0.txt\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/2591/2591-0.txt\n",
    "#!wget -P ./data/ http://www.gutenberg.org/files/1342/1342-0.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2 \n",
    "\n",
    "Για την προεπεξεργασία του αρχέιου εισόδου υλοποιούμε κατάλληλο tokenizer ο οποίος αγνοεί τα σημεία στίξης, τους αριθμούς και οποιουσδήποτε άλλους μη λεκτικούς χαρακτήρες. Διαβάζουμε το αρχείο γραμμή προς γραμμή εφαρμόζοντας την συνάρτηση και προκύπτει μια λίστα απο lowercase λέξεις."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OqT0WsSYSLb"
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "def identity_preprocess(str):\n",
    "  return str\n",
    "\n",
    "def readfile(path,preprocess=identity_preprocess):\n",
    "  processed_txt=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    processed_txt = processed_txt + preprocess(line)\n",
    "  return processed_txt \n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.strip().lower()\n",
    "    s = ''.join(c if c.isalpha() else ' ' for c in s)\n",
    "    # We replace all non-alpha characters with ' '\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(δ)\n",
    "Ο tokenizer που υλοποιούμε ειναι πολυ απλός και αναγνωρίζει ως tokens μόνο τις απλές λέξεις. Επίσης δεν έχουμε λάβει υπόψην μας ιδιαίτερα γραμματικά φαινόμενα όπως ή σύντμηση ( \"did not $\\rightarrow$ didn't\"). Στην βιλιοθήκη nltk υπάρχουν αρκετά πιο ακριβείς και εκλεπτυσμένοι tokenizers. Παρασουσιάζουμε ενδεικτικά παρακάτω τα αποτέλσματα για την πρόταση:\n",
    "\n",
    "_\"At eight o'clock on Thursday morning....Arthur didn't feel very good. He quickly rushed to the Doctor!\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom :\n",
      " ['at', 'eight', 'o', 'clock', 'on', 'thursday', 'morning', 'arthur', 'didn', 't', 'feel', 'very', 'good', 'he', 'quickly', 'rushed', 'to', 'the', 'doctor'] \n",
      "\n",
      "nltk punkt :\n",
      " ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', '.Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'He', 'quickly', 'rushed', 'to', 'the', 'Doctor', '!'] \n",
      "\n",
      "sentece detector :\n",
      " [\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\", 'He quickly rushed to the Doctor!'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 2 (d)\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning....Arthur didn't feel very good.\n",
    "              He quickly rushed to the Doctor!\"\"\"\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizers = [tokenize,nltk.word_tokenize,sent_detector.tokenize]\n",
    "names = [\"custom\",\"nltk punkt\",\"sentece detector\"]\n",
    "for tokenizer,name in zip(tokenizers,names):\n",
    "    print(name,\":\\n\",tokenizer(sentence),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Περιληπτικά αναλύουμε τους tokenizers.\n",
    "\n",
    "<b>Sentence Tokenizer</b>: Αυτός ο tokenizer χωρίζει το κείμενο σε μία λίστα από προτάσεις χρησιμοποιώντας unsupervised learning για να αναγνωρίσει επιτυχώς τα διαχωριστηκά σημεία στίξης, και να αγνοείσει όπως η απόστροφος σε μία λέξη.  Ο αλγόριθμος αυτός πρέπει να εκπαιδευτεί σε ένα μεγάλο σύνολο δεδομένων πριν μπορέσει να χρησιμοποιηθεί αποτελεσματικά. Το NLTK data package περιλαμβάνει ένα προ-εκπαιδευμένο Punkt tokenizer για την αγγλική γλώσσα.\n",
    "\n",
    "<b>Word Τokenizer</b>: Αυτός ο tokenizer εντοπίζει τις λέξεις σε ένα string και τις αποθηκεύει σε μία λίστα. Από τα σημεία στίξης κρατάει μόνο εκείνα που διαχωρίζουν προτάσεις. Όπως και ο Sentence Tokenizer χρησιμοποιεί ένα προ-εκπαιδευμένο Punkt tokenizer για την αγγλική γλώσσα. \n",
    "\n",
    "Η δικία μας απλή εκδοχή του tokenizer είναι αρκετά αποτελεσματική και υστερεί μόνο όταν σημεία στίξης αποτελούν μέρος της λέξης.  \n",
    "\n",
    "Η βιβλιοθήκη nltk διαθέτει αρκετά εξεζητημένους tokenizers, ενδεικτικά αναφέρουμε τον tweet tokenizer που διατηρεί τα σημεία στίξης και τα emoji ενω αναγνωρίζει και τα hashtags. Ολα αυτά διαθέτουν μεγάλη πληροφορία, χρήσιμη για εφαρμογές ανάλυσης tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 3\n",
    "Με βάση τα tokens που προκύπτουν απο την προηγούμενη ανάλυηση βρίσκουμε τά μοναδικά tokens (λεξιλόγιο) και τους μοναδικούς χαρακτήρες (αλφάβητο) στο corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1574168112504,
     "user": {
      "displayName": "Dorothea Kalliora",
      "photoUrl": "",
      "userId": "17412342597228224634"
     },
     "user_tz": -120
    },
    "id": "YXntX-DgfvSF",
    "outputId": "19b454d9-ca2c-4be2-cb8c-bfe654403163",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295373 words overall\n",
      "12128 unique words in corpus\n",
      "31 symbols in alphabet:\n",
      "['a', 'g', 'w', 'q', 'à', 'm', 'i', 'ç', 'x', 'h', 'n', 'z', 'l', 'f', 'é', 'u', 'o', 'j', 't', 'e', 'b', 's', 'p', 'k', 'æ', 'y', 'c', 'ê', 'r', 'd', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "\n",
    "\n",
    "text_1 = readfile('./data/36-0.txt',tokenize)\n",
    "text_2 = readfile('./data/2591-0.txt',tokenize)\n",
    "text_3 = readfile('./data/1342-0.txt',tokenize)\n",
    "\n",
    "text = text_1 + text_2 + text_3\n",
    "\n",
    "print(len(text),\"words overall\")\n",
    "word_corpus = list(set(text))\n",
    "print(len(word_corpus),\"unique words in corpus\")\n",
    "\n",
    "# Convert list of words to list of chars, get unique chars with set\n",
    "alphabet = list(set([c for word in word_corpus for c in word]))\n",
    "print(len(alphabet),\"symbols in alphabet:\")\n",
    "print(alphabet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε οτι το αλφάβητο είναι επαυξημένο με παραλλαγές γραμμάτων και έχει μέγεθος 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 4\n",
    "\n",
    "Δημιουργούμε τον πίνακα συμβόλων με βάση το αλφάβητο που υπολογίσαμε στο προηγούμενο βήμα. Το $\\epsilon$ αντιστοιχίζεται στο $0$, για τις υπολοιπες αντιστοιχίες ξεκινάμε απο εναν αυθαίρετο αριθμό. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "def create_syms(alphabet):\n",
    "    f = open(\"./chars.syms\",\"w+\")\n",
    "    f.write(f'<epsilon>     0\\n')  #for <epsilon> index 0\n",
    "    for i in range(len(alphabet)):\n",
    "        f.write(f'{alphabet[i]}     {i+50}\\n')  #for other characters index i+50\n",
    "        \n",
    "create_syms(alphabet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 5 \n",
    "Για την δημιουργία ενός ορθογράφου απαιτείται κατάλληλη μετρική για την απόσταση δύο λέξεων. \n",
    "\n",
    "θα χρησιμοποιήσουμε την απόσταση Levenshtein (ή edit distance). Η απόσταση μπορέι να υπολογιστεί αναδρομικά με χρήση dynamic programming (DP). Ανάμεσα σε δύο λέξεις μπορούν να γίνουν τρείς τύποι μετατροπών: εισαγωγή ενός χαρακτήρα ($ \\epsilon \\rightarrow a$), μετατροπή ενός χαρακτήρα σε έναν άλλο ($ a \\rightarrow b$) και διαγραφή ενός χαρακτήρα ($a \\rightarrow \\epsilon$). Θεωρούμε αρχικά ίδιο κόστος για όλες τις μετατροπές.\n",
    "\n",
    "(α) Δημιουργούμε κατάλληλο fst με μία κατάσταση, αντιστοιχίζοντας κάθε χαρακτήρα σε κάθε άλλον και στο $\\epsilon$, και το $\\epsilon$ σε κάθε χαρακτήρα με βάρος 1. Αντιστοιχίζουμε ακόμα τον χαρακτήρα στον εαυτό του με βάρος 0. Αν πάρουμε το shortest path τότε η έξοδος θα είναι ή ίδια η λέξη εισόδου, αφού το ελάχιστο βάρος προκύπτει αν δεν γίνει καμία μετατροπή. \n",
    "\n",
    "(β) Στην υλοποίηση που έγινε για το ερώτημα 5 έχουμε υποθέσει ότι όλα τα edits έχουν ίσο βάρος. Αυτό ουσιαστικά σημαίνει ότι για το μοντέλο μας οποιοδήποτε λάθος σε μία λέξη έχει την ίδια πιθανότητα εμφάνισης. Με βάση την διαίσθηση μας μια τέτοια θεώρηση δεν ανταποκρίνεται στα πραγματικά λάθη που συναντάμε σε κείμενα και προκύπτουν απο τον άνθρωπο. Για παράδειγμα μια λέξη που ξεκινά απο $b$ είναι σχεδόν αδύνατον να γραφεί λανθασμένα με $c$. Ιδανικά θα θέλαμε να γνωρίζουμε την κατανομή του λάθους για κάθε σύμβολο στο αλφάβητο η οποία εδώ έχουμε υποθέσει οτι ειναι ομοιόμορφη. \n",
    "\n",
    "Για να γίνει πειραματικός υπολογισμός θα θέλαμε ενα σύνολο δεδομένων train data της μορφής: $(original~word, wrong~spelling)$. Απο αυτό μπορούμε να υπολογίσουμε κάθε φορά την διόρθωση που απαιτείται. Η πιθανότητα θα προκύψει:\n",
    "\n",
    "$$ Pr[ a\\rightarrow b] = \\frac{|error = b \\rightarrow a|}{|train~samples|}, \\forall a,b \\in \\{A+\\epsilon\\} $$, όπου $A$ το αλφάβητο. \n",
    "\n",
    "Αυτές εινα οι a priori πιθανότητες για κάθε διόρθωση με βάση το training data. Μπορούν τώρα να χρησιμοποιηθούν για να υπολογιστούν τα βάρη στο μοντέλο αναγνώρισης μας με fst. Το βάρος για μια συγκεκριμένη διόρθωση Θα πρέπει να είναι αντιστρόφως ανάλογο της πιθανότητας εμφανίσης του αντίστοιχου λάθους. Η καθιερωμένη συνάρτηση αντιστοίχισης είναι η $ -log(p(X)) $ όπου $p(X)$ η συνάρτηση κατανομής πιθανοτήτων.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "\n",
    "# create a line corresponding to fst edge.\n",
    "def format_arc(src,dst,src_sym,dst_sym,w,f):\n",
    "    f.write(\"{} {} {} {} {} \\n\".format(src,dst,src_sym,dst_sym,w))\n",
    "\n",
    "\n",
    "def create_lev(file,alphabet,w1):\n",
    "    f = open(file,\"w+\")\n",
    "    for i in range(0, len(alphabet)):\n",
    "        format_arc(src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=alphabet[i], w = w1,f=f)\n",
    "        format_arc(src=0, dst=0, src_sym=alphabet[i], dst_sym=alphabet[i], w=0,f=f)\n",
    "        format_arc(src=0, dst=0, src_sym=alphabet[i], dst_sym=\"<epsilon>\", w = w1,f=f)\n",
    "        for j in range(0, len(alphabet)):\n",
    "            if(j!=i):\n",
    "                format_arc(src=0, dst=0, src_sym=alphabet[i], dst_sym=alphabet[j], w = w1,f=f)   \n",
    "    f.write('0\\n')\n",
    "    f.close()\n",
    "\n",
    "create_lev('lev.fst',alphabet,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev.fst lev.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 6 \n",
    "\n",
    "Ένας <b>αποδοχέας (acceptor)</b> είναι ένα fst όπου κάθε μετάβαση έχει ενα label (και προεραιτικά βάρος). Στην βιβλιοθήκη openfst1-6-1 μπορέι να θεωρηθεί ως μετατροπέας(transducer) με ίδιο input και output label. \n",
    "\n",
    "Σε αυτό το ερώτημα κατασκευάζουμε εναν αποδοχέα που αποδέχεται κάθε λέξη του λεξικού. Ο αποδοχέας έχει μία κοινή αρχική κατάσταση και απο εκεί κάθε λέξη επεκτείνεται σε καταστάσεις ανεξάρτητα απο τις άλλες. Δεν έχουμε βάρος στις μεταβάσεις (w = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "\n",
    "def create_acceptor(word_corpus,file,weights = {},model = 'Default'):\n",
    "    \n",
    "    ''' Create a suitable acceptor\n",
    "        word_corpus = dictionary of unique words\n",
    "        file = destination file\n",
    "        weights = dictionary with weights per word/letter/bigram respectively.\n",
    "                  In the case of bigrams it is a tuple.\n",
    "        model = language model type, one of: {Default,Word,Unigram,Bigram}\n",
    "    '''\n",
    "    \n",
    "    f = open(file,\"w+\")\n",
    "    s = 0\n",
    "    final_states = []\n",
    "    for word in word_corpus:\n",
    "        \n",
    "        if model == 'Default':\n",
    "            w1 = 0\n",
    "            w2 = 0\n",
    "        elif model == 'Word':\n",
    "            w1 = weights[word]\n",
    "            w2 = 0\n",
    "        elif model == 'Unigram':\n",
    "            w1 = 0\n",
    "        elif model == 'Bigram':\n",
    "            w1 = 0\n",
    "            prev_letter = ' '\n",
    "            \n",
    "        format_arc(0,s+1,\"<epsilon>\",\"<epsilon>\",w1,f)\n",
    "        s += 1\n",
    "        \n",
    "        for letter in word[0:]:\n",
    "            \n",
    "            if model == 'Unigram':\n",
    "                w2 = weights[letter]\n",
    "            \n",
    "            if model == 'Bigram':\n",
    "                w2 = weights[(prev_letter,letter)]\n",
    "            format_arc(s,s+1,letter, letter,w2,f)\n",
    "            s += 1\n",
    "            prev_letter = letter \n",
    "        final_states.append(s)\n",
    "\n",
    "    for state in final_states:\n",
    "        f.write(f'{state}\\n')\n",
    "    f.close()\n",
    "\n",
    "create_acceptor(word_corpus,\"acceptor.fst\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(β) Καλούμε παρακάτω τις συναρτήσεις $fstrmepsilon, fstdeterminze, fstminimize$ που βελτιώνουν το μοντέλο μας. Επεξηγούμε συνοπτικά την λειτουργία τους:\n",
    "\n",
    "**fstdetermize**: Δέχεται ως είσοδο έναν μετροπέα (transducer) και το αποτέλεσμα είναι ένα ισοδύναμο fst με την ιδιότητα ότι δεν υπάρχει κατάσταση όπου δύο μεταβάσεις έχουν το ίδιο σύμβολο εισόδου. Το αυτόματο γίνεται δηλαδή ντετερμινιστικό ως προς την είσοδο. Η συνάρτηση έχει το μειονέκτημα οτι χρησιμοποιεί το $\\epsilon$ σεμεταβάσεις, θεωρώντας το στοιχείου του αλφαβήτου. Ακόμα αν το αρχικό αυτόματο περιέχει $\\epsilon$-μεταβάσεις μπορεί το αποτέλεσμα να μην είναι ντετερμινιστικό. \n",
    "\n",
    "**fstrmepsilon**: Αφαιρεί απο ένα αυτόματο όλες τις $\\epsilon$-μεταβάσεις (όταν δηλαδή input = output = $\\epsilon$). \n",
    "\n",
    "**fstminimize**: Για εναν acceptor η συνάρτηση παράγει το ελάχιστο ισοδύναμο αυτόματο. Για εναν transducer η ελαχιστότητα δεν μπορεί να επιτευχθεί με την αυστηρή έννοια διότι κάτι τέτοιο θα απαιτούσε output labels με την μορφή συμβολοσειρών που δεν υποστηρίζεται απο την fst. Κάθε τέτοια μετάβαση ειναι ανταυτού μια ακολουθία απο μεταβάσεις με έξοδο χαρακτήρα. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6b\n",
    "\n",
    "!fstdeterminize acceptor.bin.fst acceptor.bin.fst\n",
    "!fstrmepsilon acceptor.bin.fst acceptor.bin.fst\n",
    "!fstminimize acceptor.bin.fst acceptor.bin.fst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 7\n",
    "\n",
    "Για να υλοποιήσουμε τον ορθογράφο ελάχιστης απόστασης (min edit distance spell checker) θα συνθέσουμε τον Levenshtein transducer με τον αποδοχέα του λεξικού που υλοποιήσαμε σε προηγούμενο ερώτημα. Το αποτέλεσμα είναι ένας transducer που διορθώνει τις λέξεις μόνο με κριτήριο τις ελάχιστες δυνατές μετατροπές που απαιτούνται, χωρίς να λαμβάνει υπόψη του καμία γλωσσική πληροφορία. \n",
    "\n",
    "α) \n",
    "\n",
    "* Για ίσα βάρη στα edits όπως αναλύσαμε και σε προηγούμενα ερωτήματα, όλες οι μετατροπές μεταξύ γραμμάτων έχουν ίση πιθανοτηα και αρα οι αντίστοιχες ακμές ίσο βάρος στο fst. Δεν υπάρχει συνεπώς κάποιο bias προς μια συγκεκριμένη κατέυθυνση και η επιλογή γίνεται καθαρά με την ελάχιστη edit distance. Αυτο μπορεί να οδηγήσει σε λάθη παρόλο που η λέξη προς διόρθωση μπορεί να είναι γνωστή. \n",
    "\n",
    "\n",
    "*  Για διαφορετικά βάρη των edits το fst είναι προδιαθετιμένο κάθε φορά να ακολουθήσει ένα συγκεκριμένο μονοπάτι μεταβάσεων. Αυτη η προδιάθεση μειώνει την τυχαιότητα στην εκτέλεση του μοντέλου, καθώς πολυ συχνά η λέξη με την ελάχιστη απόσταση θα είναι μοναδική. Η εισαγωγή bias με προσεκτική επιλογή των βαρών μπορεί να οδηγήσει σε πολυ καλά αποτελέσματα. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "\n",
    "#Step 7(a)\n",
    "!fstarcsort --sort_type=olabel lev.bin.fst lev.bin.fst\n",
    "!fstarcsort --sort_type=ilabel acceptor.bin.fst acceptor.bin.fst\n",
    "!fstcompose  lev.bin.fst acceptor.bin.fst spell_checker.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance prediction for ['cit'] is: \n",
      "fit"
     ]
    }
   ],
   "source": [
    "#Step 7b\n",
    "word = ['cit']\n",
    "create_acceptor(word,\"in.fst\")\n",
    "\n",
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms   in.fst in.bin.fst\n",
    "!fstarcsort --sort_type=ilabel spell_checker.bin.fst spell_checker.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst \n",
    "print(f\"Min distance prediction for {word} is: \")\n",
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!fstcompose in.bin.fst spell_checker.bin.fst |fstshortestpath --nshortest=10   \\\n",
    "| fstrmepsilon |  fsttopsort \\\n",
    "| fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait  | dot -Tjpg > ./img/min_cit.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(β) \n",
    "Παρουσιάζουμε σε μορφή διαγράμματος 10 πιθανές έλάχιστες προβλέψεις για την λέξη _\"cit\"_. Παρατηρούμε ότι λόγω των ίσων βαρών υπάρχουν πολλές διαφορετικές προβλέψεις ελάχιστης απόστασης. Συνεπώς κατω απο αυτές τις συνθήκες είναι αδύνατον το μοντέλο μας να εκτελεί με συνέπεια καλές προβλέψεις. \n",
    "\n",
    "Οι πιθανές προβλέψεις προκύπτουν: \n",
    "|\n",
    "$$\\{ fit, hit, cut, sit, city, it, lit, cat, pit, bit\\}$$ \n",
    "![predictions-cit](./img/min_cit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 8 \n",
    "\n",
    "Θα αξιολογήσουμε την επίδοση του ορθογράφου μας πάνω σε ένα σύνολο δεδομένων για evaluation. Η εκτίμηση μας είναι η λέξη του λεξικού με την ελάχιστη απόσταση απο την λέξη προς διόρθωση. Όπως αναφέραμε η λέξη αυτή δεν είναι μοναδική, και αρα το αποτέλεσμα εμπεριέχει τυχαιότητα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 8:\n",
    "import random\n",
    "random.seed()\n",
    "\n",
    "file = open('./data/spell_checker_test_set',\"r\")\n",
    "y_test = []\n",
    "X_test = []\n",
    "for line in file:\n",
    "    [y,X] = line.split(':')\n",
    "    X = X.split()\n",
    "    for word in X:\n",
    "        X_test.append(word)\n",
    "        y_test.append(y)\n",
    "# Get 20 random words from test set, along with their labels\n",
    "idxs = random.sample(range(0, len(y_test)), 20)\n",
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def predict(Y,X,spell_checker,Show = True):\n",
    "    correct_pred=0\n",
    "    for y,x in zip(Y,X):\n",
    "        create_acceptor([x],\"input.fst\")\n",
    "        !fstcompile  --isymbols=chars.syms --osymbols=chars.syms input.fst input.bin.fst\n",
    "        !fstarcsort --sort_type=olabel input.bin.fst input.bin.fst \n",
    "        command = f'''fstcompose input.bin.fst {spell_checker} |fstshortestpath --nshortest=1 \\\n",
    "        | fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "        | cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n' \n",
    "        '''\n",
    "        prediction = os.popen(command).read()\n",
    "        if Show:\n",
    "            print(\"Input:\",x,\"\\n  --Correct:   \",y,\"\\n  --Prediction:\",prediction,\"\\n\")\n",
    "        if y == prediction:\n",
    "            correct_pred+=1\n",
    "    print(f\"{spell_checker}-accuracy:{correct_pred/len(Y)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: bycicle \n",
      "  --Correct:    bicycle \n",
      "  --Prediction: bicycle \n",
      "\n",
      "Input: unfortunatly \n",
      "  --Correct:    unfortunately \n",
      "  --Prediction: unfortunately \n",
      "\n",
      "Input: courtens \n",
      "  --Correct:    curtains \n",
      "  --Prediction: courteous \n",
      "\n",
      "Input: laught \n",
      "  --Correct:    laugh \n",
      "  --Prediction: laugh \n",
      "\n",
      "Input: remine \n",
      "  --Correct:    remind \n",
      "  --Prediction: remind \n",
      "\n",
      "Input: possition \n",
      "  --Correct:    position \n",
      "  --Prediction: position \n",
      "\n",
      "Input: viseted \n",
      "  --Correct:    visited \n",
      "  --Prediction: visited \n",
      "\n",
      "Input: contende \n",
      "  --Correct:    contented \n",
      "  --Prediction: content \n",
      "\n",
      "Input: opposit \n",
      "  --Correct:    opposite \n",
      "  --Prediction: opposite \n",
      "\n",
      "Input: choise \n",
      "  --Correct:    choice \n",
      "  --Prediction: chose \n",
      "\n",
      "Input: wonted \n",
      "  --Correct:    wanted \n",
      "  --Prediction: wanted \n",
      "\n",
      "Input: magnifecent \n",
      "  --Correct:    magnificent \n",
      "  --Prediction: magnificent \n",
      "\n",
      "Input: varable \n",
      "  --Correct:    variable \n",
      "  --Prediction: marble \n",
      "\n",
      "Input: lugh \n",
      "  --Correct:    laugh \n",
      "  --Prediction: laugh \n",
      "\n",
      "Input: concider \n",
      "  --Correct:    consider \n",
      "  --Prediction: consider \n",
      "\n",
      "Input: perhapse \n",
      "  --Correct:    perhaps \n",
      "  --Prediction: perhaps \n",
      "\n",
      "Input: initails \n",
      "  --Correct:    initials \n",
      "  --Prediction: entail \n",
      "\n",
      "Input: seperate \n",
      "  --Correct:    separate \n",
      "  --Prediction: separate \n",
      "\n",
      "Input: diagrammaticaally \n",
      "  --Correct:    diagrammatically \n",
      "  --Prediction: practically \n",
      "\n",
      "Input: leval \n",
      "  --Correct:    level \n",
      "  --Prediction: level \n",
      "\n",
      "spell_checker.bin.fst-accuracy:0.7%\n"
     ]
    }
   ],
   "source": [
    "predict(Y_rand,X_rand,\"spell_checker.bin.fst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Διορθώνουμε 20 τυχαίες λέξεις στο test set και μετράμε το accuracy.\n",
    "\n",
    "Για διαφορετικές επαναλήψεις παρατηρήσαμε οτι η ακρίβεια προκύπτει απο 0.4% εώς 0.7%. Για λέξεις με μεγάλο μήκος που απέχουν πολύ απο άλλες λέξεις στο λεξιλόγιο το μοντέλο έχει καλή απόδοση και τις αναγνωρίζει με επιτυχία. Για παράδειγμα οι λέξεις: $\\{ biscuits, independent, bicycle, southern, scissors, visitors \\}$ αναγνωρίζονται σωστά. Λέξεις όμως όπως τα: $\\{ poems, cake , awful \\}$ αναγνωρίζονται δυσκολότερα γιατί ειναι μικρές και έχουν κοινά προθέματα και επιθέματα με άλλες λέξεις. \n",
    "\n",
    "Δύο είναι οι κύριοι παράγοντες προς βελτίωση που επηρεάζουν την απόδοση του μοντέλου:\n",
    "\n",
    "* Το μικρό σύνολο εκπαίδευσης. Εάν το μοντέλο δεν γνωρίζει μια λέξη δεν μπορεί να την αναγνωρίσει και αρα η διόρθωση σε αυτή θα προκύπτει πάντα λάθος. Το corpus απο ένωση 2 βιβλίων δεν ειναι αρκετό για να εξαλείψει σε ικανοποιητικό βαθμό αυτον τον τύπο λάθους.\n",
    "\n",
    "* Τα ίσα βάρη στις μετατροπές. Σε πολλές περιπτώσεις ακόμα και αν το μοντέλο γνωρίζει μια λέξη δεν καταφέρνει να διορθώσει σε αυτήν γιατί υπάρχουν ακόμα πολλές λέξεις με την ίδια ελάχιστη απόσταση. Με την \"δίκαιη\" αυτή αντιμετώπιση εισάγεται τυχαιότητα στην απόδοση του μοντέλου καθώς το αν θα προκύψει η σωστή λέξη απο αυτές με την ελάχιστη απόσταση ειναι κατα βάση τυχαίο. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 9\n",
    "\n",
    "Στο ερώτημα αυτο θα ασχοληθούμε με αναπαραστάσεις word2vec. Έχουμε ήδη μιλήσει για διανυσματικές αναπαραστάσεις λέξεων στην εισαγωγή. Στόχος είναι η αναπαράσταση λέξεων στον $\\mathbb{R}^d$ έτσι ώστε να βρίσκονται σημασιολογικά κοντά. Το training γίνεται με βάση την θέση τους στο κείμενο με κυλιόμενο παράθυρο. Οι 2 βασικές προσεγγίσεις είναι Continuous Bag of Words (η θέση στο παράθυρο δεν εχει σημασία) και continuous skip gram ( η λέξη χρησιμοποείται για πρόβλεψη των γειτονικών). \n",
    "\n",
    "Διαβάζουμε αρχικά το corpus σε μία λίστα απο προτάσεις με το tokenization που είχαμε υλοποιείσει σε προηγούμενο ερώτημα. Στην συνέχεια εκπαιδεύουμε 100 διάστατα word2vec embeddings με βάση τις προτάσεις που προκύπτουν. Χρησιμοποιούμε $window=5$ και $epochs=100$.\n",
    "\n",
    "Για 10 τυχαίες λέξεις θα δείξουμε τις σημασιολογικά κοντινότερες τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 9(a)\n",
    "\n",
    "def tokenized_list(path,preprocess=identity_preprocess):\n",
    "  list_of_sentences=[]\n",
    "  f = open(path, \"r\")\n",
    "  for line in f:\n",
    "    l = preprocess(line)\n",
    "    if l:\n",
    "        list_of_sentences.append(l)\n",
    "  return list_of_sentences\n",
    "\n",
    "\n",
    "list1 = tokenized_list('./data/36-0.txt',tokenize)\n",
    "list2 = tokenized_list('./data/2591-0.txt',tokenize)\n",
    "list3 = tokenized_list('./data/1342-0.txt',tokenize)\n",
    "final_list = list1 + list2 + list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"color:red\">NIkhta ta similarity edwwww</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"folks\":\n",
      "     \"pulled\" -- sim: 0.45095887780189514\n",
      "     \"faces\" -- sim: 0.40082991123199463\n",
      "     \"prevented\" -- sim: 0.38809943199157715\n",
      "     \"seized\" -- sim: 0.36616289615631104\n",
      "     \"grounds\" -- sim: 0.3659277558326721\n",
      "     \"stones\" -- sim: 0.35526710748672485\n",
      "     \"forms\" -- sim: 0.3412390351295471\n",
      "     \"lamps\" -- sim: 0.3402010500431061\n",
      "     \"causes\" -- sim: 0.3362317681312561\n",
      "     \"legs\" -- sim: 0.3325420916080475\n",
      "Most similar words to \"dwarf\":\n",
      "     \"boy\" -- sim: 0.4892573952674866\n",
      "     \"manikin\" -- sim: 0.40912240743637085\n",
      "     \"peasant\" -- sim: 0.4005774259567261\n",
      "     \"heinel\" -- sim: 0.38350558280944824\n",
      "     \"horse\" -- sim: 0.3761593997478485\n",
      "     \"dog\" -- sim: 0.3625195026397705\n",
      "     \"princes\" -- sim: 0.35340115427970886\n",
      "     \"marleen\" -- sim: 0.35220906138420105\n",
      "     \"huntsman\" -- sim: 0.34962713718414307\n",
      "     \"enemy\" -- sim: 0.3476199805736542\n",
      "Most similar words to \"goose\":\n",
      "     \"iron\" -- sim: 0.4322069585323334\n",
      "     \"princess\" -- sim: 0.3936925530433655\n",
      "     \"trouble\" -- sim: 0.3676016330718994\n",
      "     \"gretel\" -- sim: 0.35269051790237427\n",
      "     \"horse\" -- sim: 0.3493756353855133\n",
      "     \"apples\" -- sim: 0.34466829895973206\n",
      "     \"things\" -- sim: 0.3323819935321808\n",
      "     \"split\" -- sim: 0.3304502069950104\n",
      "     \"away\" -- sim: 0.3298451602458954\n",
      "     \"luck\" -- sim: 0.3222509026527405\n",
      "Most similar words to \"affair\":\n",
      "     \"honest\" -- sim: 0.380386084318161\n",
      "     \"event\" -- sim: 0.34300702810287476\n",
      "     \"bestowed\" -- sim: 0.3308071196079254\n",
      "     \"corps\" -- sim: 0.3293813169002533\n",
      "     \"electronic\" -- sim: 0.3282354176044464\n",
      "     \"ceased\" -- sim: 0.32638224959373474\n",
      "     \"article\" -- sim: 0.325467586517334\n",
      "     \"domain\" -- sim: 0.3236505091190338\n",
      "     \"solemnity\" -- sim: 0.31985440850257874\n",
      "     \"based\" -- sim: 0.31876325607299805\n",
      "Most similar words to \"planted\":\n",
      "     \"covered\" -- sim: 0.3525996208190918\n",
      "     \"queer\" -- sim: 0.3329737186431885\n",
      "     \"divided\" -- sim: 0.327420175075531\n",
      "     \"destruction\" -- sim: 0.32531899213790894\n",
      "     \"aged\" -- sim: 0.3138568103313446\n",
      "     \"snatched\" -- sim: 0.311213880777359\n",
      "     \"infinite\" -- sim: 0.3105359971523285\n",
      "     \"mischief\" -- sim: 0.30369049310684204\n",
      "     \"slept\" -- sim: 0.29844698309898376\n",
      "     \"narrow\" -- sim: 0.29758474230766296\n",
      "Most similar words to \"drunk\":\n",
      "     \"proud\" -- sim: 0.3850809931755066\n",
      "     \"pine\" -- sim: 0.3408237397670746\n",
      "     \"terrace\" -- sim: 0.33995240926742554\n",
      "     \"aware\" -- sim: 0.3303379416465759\n",
      "     \"wept\" -- sim: 0.32057926058769226\n",
      "     \"five\" -- sim: 0.31171485781669617\n",
      "     \"bottle\" -- sim: 0.3110676407814026\n",
      "     \"raised\" -- sim: 0.3098764419555664\n",
      "     \"bean\" -- sim: 0.30251064896583557\n",
      "     \"meat\" -- sim: 0.3021043837070465\n",
      "Most similar words to \"exercise\":\n",
      "     \"greenwood\" -- sim: 0.36565202474594116\n",
      "     \"owing\" -- sim: 0.3540520668029785\n",
      "     \"embarrassment\" -- sim: 0.35339784622192383\n",
      "     \"confined\" -- sim: 0.34408771991729736\n",
      "     \"taking\" -- sim: 0.34064966440200806\n",
      "     \"chief\" -- sim: 0.33602726459503174\n",
      "     \"relationship\" -- sim: 0.3346342444419861\n",
      "     \"secrecy\" -- sim: 0.3325859606266022\n",
      "     \"plates\" -- sim: 0.32811102271080017\n",
      "     \"roofs\" -- sim: 0.32492685317993164\n",
      "Most similar words to \"bad\":\n",
      "     \"exchange\" -- sim: 0.3860774636268616\n",
      "     \"fortunate\" -- sim: 0.37333041429519653\n",
      "     \"willing\" -- sim: 0.3716159462928772\n",
      "     \"hungry\" -- sim: 0.3554683029651642\n",
      "     \"bingleys\" -- sim: 0.33738842606544495\n",
      "     \"extravagant\" -- sim: 0.32187384366989136\n",
      "     \"happy\" -- sim: 0.31331518292427063\n",
      "     \"besides\" -- sim: 0.3127039968967438\n",
      "     \"reasonable\" -- sim: 0.3086778223514557\n",
      "     \"incapable\" -- sim: 0.30237483978271484\n",
      "Most similar words to \"twenty\":\n",
      "     \"ten\" -- sim: 0.37126317620277405\n",
      "     \"hundred\" -- sim: 0.3412649631500244\n",
      "     \"respectable\" -- sim: 0.3088209629058838\n",
      "     \"compared\" -- sim: 0.30681055784225464\n",
      "     \"fired\" -- sim: 0.30304333567619324\n",
      "     \"dances\" -- sim: 0.3023643493652344\n",
      "     \"teach\" -- sim: 0.295444518327713\n",
      "     \"artillery\" -- sim: 0.28755414485931396\n",
      "     \"fashion\" -- sim: 0.2770172953605652\n",
      "     \"upon\" -- sim: 0.27016496658325195\n",
      "Most similar words to \"posted\":\n",
      "     \"hart\" -- sim: 0.3979283571243286\n",
      "     \"contents\" -- sim: 0.3871959149837494\n",
      "     \"discharged\" -- sim: 0.38455522060394287\n",
      "     \"holder\" -- sim: 0.3786858916282654\n",
      "     \"gutenberg\" -- sim: 0.37156519293785095\n",
      "     \"redistribution\" -- sim: 0.36569762229919434\n",
      "     \"development\" -- sim: 0.3611433804035187\n",
      "     \"located\" -- sim: 0.35831528902053833\n",
      "     \"michael\" -- sim: 0.3565717339515686\n",
      "     \"enumerating\" -- sim: 0.3553636074066162\n"
     ]
    }
   ],
   "source": [
    "#Step 9(b,c)\n",
    "\n",
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "model = Word2Vec(final_list, window=5, size=100, workers=4)\n",
    "model.train(final_list, total_examples=len(final_list), epochs=1000)\n",
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "# get vector size\n",
    "dim = model.vector_size\n",
    "#pick 10 random words from the dictionary\n",
    "idxs = random.sample(range(0, len(voc)), 10)\n",
    "rand_words = [voc[i] for i in idxs]\n",
    "for word in rand_words:\n",
    "    print(f'Most similar words to \"{word}\":')\n",
    "    for word,sim in model.wv.most_similar(word):\n",
    "        print(f'     \"{word}\" -- sim: {sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(γ) Τα αποτελέσματα του μοντέλου δεν ειναι ικανοποιητικα. Οι λέξεις δεν έχουν επι το πλείστον σημασιολογική συσχέτιση, εκτός ενδεχομένως απο κάποιο κοινό θέμα.´Όπως οι μέρες της εβδομάδας, οι αριθμοί, ανθρώπινα ονόματα ή κάποια ανθρώπινη δράση. \n",
    "\n",
    "Θα προσπαθήσουμε τώρα να βελτιώσουμε το αποτέλεσμα του μοντέλου αλλάζωντας τις εξής παραμέτρους:\n",
    "* Αυξάνουμε το μέγεθος του παραθύρου context κρατώντας τον αριθμό εποχών σταθερό\n",
    "\n",
    "* Αυξάνουμε τον αριθμό των εποχών κρατώντας το μέγεθος του παραθύρου σταθερό\n",
    "\n",
    "* Αυξάνουμε και τον αριθμό εποχών και το μέγεθος του παραθύρου\n",
    "\n",
    "Παρακάτω παραθέτουμε τα αποτελέσματα του similarity 20 τυχαίων λέξεων για κάθε μία από τις τρεις περιπτώσεις. Για μέγεθος παραθύρου κοντά στο 1 έχουμε πληροφορία σύνταξης. Για μεγαλύτερο παράθυρο αντιστοιχίζουμε λέξεις με βάση την σημασιολογία. \n",
    "\n",
    "Για μεγαλύτερο μέγεθος παραθύρου θα περιμέναμε λοιπόν καλύτερα αποτελέσματα, εφόσων αναζητούμε σημασιολογικά κοντινές λέξεις. Κάτι τέτοιο δεν συμβαίνει στην πράξη. Εικάζουμε οτι αυτο οφείλεται στο μικρό σύνολο δεδομένων εκπαίδευσης που δεν επιτρέπουν στο μοντέλο να έχει μεγάλη εκφραστικότητα. Συγκεκριμένα για ακριβείς αναπαραστάσεις word2vec θέλουμε εκαττομύρια λέξεις και όχι της ταξής των 3000 που προκύπτουν απο το corpus μας. Παρατηρήσαμε οτι το word2vec όπως ειναι υλοποιημένο στην βιβλιοθήκη αγνοεί σπάνιες λέξεις, για αυτο και υπήρξε μείωση στο αρχικό vocabulary που εξάγαμε. Συγκεκριμένα η παράμετρος _min_count (int, optional)_ καθορίζει πόσο μικρή συχνότητα πρέπει να έχει μια λέξη για να θεωρηθεί οτι δεν μπορεί να δώσει με ακρίβεια πληροφορία, και να αγνοηθεί.\n",
    "\n",
    "Για αύξηση των αριθμών των epochs επίσης δεν έχουμε καλύτερο αποτέλεσμα. Για να επηρεάσει ουσιαστικά ο αριθμός εποχών θα πρέπει να έχουμε ενα αρκετά μεγάλο σύνολο δεδομένων ώστε ο αλγόριθμος να μην κανει πρόωρα converge. Για εμάς και μικρός αριθμός εποχών ειναι αρκετος. \n",
    "\n",
    "Συμπερασματικά  ο πιο καθοριστικός παράγοντας για σωστή εξαγωγή αναπαραστάσεων ειναι ο αριθμός των δεδομένων εκπαίδευσης.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9(c (i))\n",
    "\n",
    "def similarity(w,s,e):\n",
    "    model = Word2Vec(final_list, window=w, size=s, workers=4)\n",
    "    model.train(final_list, total_examples=len(final_list), epochs=e)\n",
    "    voc = model.wv.index2word\n",
    "    dim = model.vector_size\n",
    "    rand_words = [voc[i] for i in idxs]\n",
    "    for word in rand_words:\n",
    "        print(f'Most similar words to \"{word}\":')\n",
    "        for word,sim in model.wv.most_similar(word)[0:2]:\n",
    "            print(f'     \"{word}\" -- sim: {sim}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"folks\":\n",
      "     \"pushing\" -- sim: 0.3648185729980469\n",
      "     \"causes\" -- sim: 0.3617570400238037\n",
      "Most similar words to \"dwarf\":\n",
      "     \"peasant\" -- sim: 0.4489038586616516\n",
      "     \"tailor\" -- sim: 0.44317829608917236\n",
      "Most similar words to \"goose\":\n",
      "     \"chanticleer\" -- sim: 0.39074188470840454\n",
      "     \"princess\" -- sim: 0.3504902124404907\n",
      "Most similar words to \"affair\":\n",
      "     \"dream\" -- sim: 0.39150726795196533\n",
      "     \"assembly\" -- sim: 0.37886327505111694\n",
      "Most similar words to \"planted\":\n",
      "     \"sour\" -- sim: 0.34750694036483765\n",
      "     \"finest\" -- sim: 0.33094674348831177\n",
      "Most similar words to \"drunk\":\n",
      "     \"feet\" -- sim: 0.37177878618240356\n",
      "     \"strike\" -- sim: 0.3667938709259033\n",
      "Most similar words to \"exercise\":\n",
      "     \"inhabitants\" -- sim: 0.3781837821006775\n",
      "     \"greenwood\" -- sim: 0.372259259223938\n",
      "Most similar words to \"bad\":\n",
      "     \"exchange\" -- sim: 0.34380459785461426\n",
      "     \"happy\" -- sim: 0.3355359435081482\n",
      "Most similar words to \"twenty\":\n",
      "     \"wimbledon\" -- sim: 0.3730147182941437\n",
      "     \"past\" -- sim: 0.363569438457489\n",
      "Most similar words to \"posted\":\n",
      "     \"discharged\" -- sim: 0.4093092083930969\n",
      "     \"clapham\" -- sim: 0.36420974135398865\n"
     ]
    }
   ],
   "source": [
    "similarity(15,100,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e8bfd7f8298a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Step 9(c (ii))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-25cac6cc493c>\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(w, s, e)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Step 9(c (ii))\n",
    "\n",
    "similarity(5,100,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9(c (iii))\n",
    "\n",
    "similarity(10,100,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\" >Μέρος 1 </h1>\n",
    "<h1 align = \"center\" >Ορθογράφος </h1>\n",
    "\n",
    "Στο πρώτο μέρος ενισχύουμε τον ορθογράφο που έχουμε ήδη υλοποιήσει χρησιμοποιώντας character level και word level unigram γλωσσικά μοντέλα, ενώ θα γίνει πειραματισμός και με bigram γλωσσικά μοντέλα. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 10\n",
    "Για να βελτιώσουμε την απόδοση του ορθογράφου μας θα πρέπει να πετύχουμε την μέγιστη αξιοποίηση του συνόλου εκπαίδευσης. Για τον σκοπό αυτό εξάγουμε στατιστικά χαρακτηριστικά απο τα δεδομένα και ενσωματόνουμε την πληροφορία αυτή αλλάζοντας τα βάρη του μοντέλου. Οι πηγές στατιστικών θα είναι:\n",
    "* word level: εξάγουμε την πιθανότητα εμφάνισης κάθε λέξης\n",
    "* character level: εξάγουμε την πιθανότητα εμφάνισης κάθε χαρακτήρα\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable word in the dictionary: 'the' with probability: 0.05669103134003446\n",
      "Least probable word in the dictionary: 'inhabited' with probability: 3.38554979635918e-06\n"
     ]
    }
   ],
   "source": [
    "#Step 10\n",
    "from collections import Counter\n",
    "\n",
    "word_prob = Counter(text) # Counter returns a dictionary {word: freq} in a fast way\n",
    "word_prob = {word:prob/len(text) for word,prob in word_prob.items()}\n",
    "\n",
    "chars = [char for word in text for char in word]\n",
    "char_prob = Counter(chars)\n",
    "char_prob = {char:prob/len(chars) for char,prob in char_prob.items()}\n",
    "max_prob_word = max(word_prob, key=word_prob.get)\n",
    "min_prob_word = min(word_prob, key=word_prob.get)\n",
    "print(f\"The most probable word in the dictionary: '{max_prob_word}' with probability: {word_prob[max_prob_word]}\")\n",
    "print(f\"Least probable word in the dictionary: '{min_prob_word}' with probability: {word_prob[min_prob_word]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 11\n",
    "Έχουμε ήδη δουλέψει με την απόσταση **Levenshtein** (ή edit distance). Χρησιμοποιούμε 3 τύπους απο edits: \n",
    "* Εισαγωγή χαρακτήρα \n",
    "* Διαγραφή χαρακτηρα \n",
    "* Αντικατάσταση χαρακτήρα\n",
    "α) Υπολογίζουμε την μέση τιμή των βαρών του word level μοντέλου δηλαδή:\n",
    "\n",
    "$$ W_{word}^{average} = \\sum_{i} \\cdot -log(~p(word_i)~) / |words|$$\n",
    "\n",
    "Τα βάρη δίνονται απο την συνάρτηση $-log(p(x))$ , η οποία διαισθητικά κωδικοποιεί σωστά την πληροφορία, δίνοντας μεγαλύτερο βάρος στα λιγότερο συχνά ενδεχόμενα.\n",
    "Το κόστος των edits για το word-level μοντέλο είναι η μέση τιμή $w  = \\overline{W} $. \n",
    "\n",
    "Εναλλακτικά μπορούμε να υπολογίσουμε την πιθανοτική μέση τιμή των βαρών αν θεωρήσουμε τυχαία μεταβλητή με τιμές τα βάρη και πιθανότητες την πιθανότητα της αντίστοιχης λέξης. Τότε η μέση τιμή των βαρών είναι επίσης η *εντροπία* της κατανομής των λέξεων $p(x), x \\in word~corpus$. \n",
    "\n",
    "$$ \\mathbb{E}[W_{word}] = -\\sum_{i} p(word_i) \\cdot log(~p(word_i)~) $$\n",
    "\n",
    "(β) Κατασκευάζουμε έναν μετατροπέα με μία κατάσταση που υλοποιεί την απόσταση Levenshtein. Για κάθε edit το κόστος είναι $w$, εκτός απο την αντικατάσταση ενός γράμματος με τον εαυτό του που έχει κόστος 0. \n",
    "\n",
    "(γ) Επαναλαμβάνουμε για το unigram γλωσσικό μοντέλο.\n",
    "\n",
    "(δ) Όπως έχουμε αναφέρει αυτός ο τρόπος υπολογισμού των βαρών δεν κωδικοποιεί σημαντική πληροφορία και δεν βελτιώνει την απόδοση του μοντέλου μας.\n",
    "\n",
    "Ιδανικά θα θέλαμε ένα σύνολο labeled δεδομένων της μορφής (original word, wrong spelling). Με αυτό τον τρόπο μπορούμε να εξάγουμε σημαντική πληροφορία βρίσκοντας την πιθανότητα ενός συγκεκριμένου edit. \n",
    "\n",
    "Διαισθητικά δεν είναι όλες οι μετατροπές το ίδιο πιθανές. Για παράδειγμα μια λέξη που ξεκινά απο $a$ είναι σχεδόν αδύνατον να γραφεί λανθασμένα με $z$ (θεωρώντας ρεαλιστικά ανθρώπινα δεδομένα και όχι τυχαίο θωρυβώδες dataset). Θα θέλαμε λοιπόν να υπολογίσουμε την _a priori_ πιθανότητα κάθε edit και αυτήν να κωδικοποιήσουμε στα βάρη μας:\n",
    "\n",
    "$$ Pr[ a\\rightarrow b] = \\frac{|error = b \\rightarrow a|}{|train~samples|}, \\forall a,b \\in \\{A+\\epsilon\\} $$, όπου $A$ το αλφάβητο. \n",
    "\n",
    "Το βάρος για μια συγκεκριμένη διόρθωση Θα πρέπει να είναι αντιστρόφως ανάλογο της πιθανότητας εμφανίσης του αντίστοιχου λάθους. Η ιδιότητα μπορεί να επιτευχθεί με την συνάρτηση  $ -log(p(X)) $ όπου $p(X)$ η συνάρτηση κατανομής πιθανοτήτων.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average weight for word-level model: 16.366455555062522\n",
      "Average weight for unigram model: 7.803325647790889\n"
     ]
    }
   ],
   "source": [
    "#Step 11(a)\n",
    "import math\n",
    "\n",
    "# We create the weight dictionaries with an elegant dict comprehension..\n",
    "weight_words = {word: -math.log(prob,2) for word,prob in word_prob.items()}\n",
    "weight_chars = {char: -math.log(prob,2) for char,prob in char_prob.items()}\n",
    "\n",
    "avg_weight_words = sum(list(weight_words.values()))/len(list(weight_words.values()))\n",
    "print(f\"Average weight for word-level model: {avg_weight_words}\")\n",
    "\n",
    "avg_weight_chars = sum(list(weight_chars.values()))/len(list(weight_chars.values()))\n",
    "print(f\"Average weight for unigram model: {avg_weight_chars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 11(b)\n",
    "\n",
    "create_lev(\"lev_word.fst\",alphabet,avg_weight_words)\n",
    "create_lev(\"lev_unigram.fst\",alphabet,avg_weight_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev_word.fst lev_word.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev_unigram.fst lev_unigram.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 12\n",
    "Κατασκευάζουμε έναν αποδοχέα που αποδέχεται κάθε λέξη του corpus. Για βάρος χρησιμοποιούμε το $-log(P(word))$, δίνοντας στο μοντέλο περισσότερη πληροφορία και βελτιώνωντας το κριτήριο επιλογής λέξης. \n",
    "\n",
    "Ακολουθούμε την διαδικασία τόσο για το unigram όσο και για το word level γλωσσικό μοντέλο. \n",
    "Έχουμε τροποποθήσει την συνάρτηση δημιουργίας αποδοχέα στο βήμα 6 έτσι ώστε να περιλαμβάνει περιπτώσεις για τους διαφορετικούς τύπους μοντέλων: \n",
    "* Simple acceptor (zero weights)\n",
    "* Word level \n",
    "* Unigram\n",
    "* Bigram\n",
    "\n",
    "Στην συνέχεια βελτιστοποιούμε τα μοντέλα με τις _fstdeterminize, fstrmepsilon, fstminimize_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_acceptor(word_corpus,\"acceptor_word.fst\",weight_words,model='Word')\n",
    "create_acceptor(word_corpus,\"acceptor_unigram.fst\",weight_chars,model='Unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor_word.fst acceptor_word.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_word.bin.fst acceptor_word.bin.fst\n",
    "!fstrmepsilon acceptor_word.bin.fst acceptor_word.bin.fst\n",
    "!fstminimize acceptor_word.bin.fst acceptor_word.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor_unigram.fst acceptor_unigram.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_unigram.bin.fst acceptor_unigram.bin.fst\n",
    "!fstrmepsilon acceptor_unigram.bin.fst acceptor_unigram.bin.fst\n",
    "!fstminimize acceptor_unigram.bin.fst acceptor_unigram.bin.fst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 13\n",
    "Ακολουθώντας την διαδικασία του ερωτήματος 7 θα κατασκευάσουμε ορθογράφο με το word-level γλωσσικό μοντέλο και μετατροπέα, και αντίστοιχα με το unigram μοντέλο και μετατροπέα.\n",
    "\n",
    "Περιμένουμε οι ορθογράφοι αυτοί να έχουν καλύτερη απόδοση απο τον απλοϊκό ορθογράφο του βήματος 7, καθώς κωδικοποιούν πληροφορία και στα βάρη τους. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13(a)\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev_word.bin.fst lev_word.bin.fst\n",
    "!fstcompose  lev_word.bin.fst acceptor_word.bin.fst spell_checker_word.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13(b)\n",
    "\n",
    "\n",
    "!fstcompose  lev_word.bin.fst acceptor_unigram.bin.fst spell_checker_unigram.bin.fst\n",
    "#!fstarcsort --sort_type=olabel lev_unigram.bin.fst lev_unigram.bin.fst\n",
    "#!fstcompose  lev_unigram.bin.fst acceptor_unigram.bin.fst spell_checker_unigram.bin.fst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance prediction for ['cit'] is: \n",
      "it"
     ]
    }
   ],
   "source": [
    "#Step 13(c)\n",
    "# We repeat the process in step 7.. \n",
    "\n",
    "\n",
    "word = ['cit']\n",
    "create_acceptor(word,\"in.fst\")\n",
    "\n",
    "# Word-level model\n",
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms in.fst in.bin.fst\n",
    "!fstarcsort --sort_type=ilabel spell_checker_word.bin.fst spell_checker_word.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst \n",
    "\n",
    "print(f\"Min distance prediction for {word} is: \")\n",
    "!fstcompose in.bin.fst spell_checker_word.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance prediction for ['cit'] is: \n",
      "it"
     ]
    }
   ],
   "source": [
    "# Unigram model\n",
    "!fstcompile  --isymbols=chars.syms --osymbols=chars.syms   in.fst in.bin.fst\n",
    "!fstarcsort --sort_type=ilabel spell_checker_unigram.bin.fst spell_checker_unigram.bin.fst \n",
    "!fstarcsort --sort_type=olabel in.bin.fst in.bin.fst \n",
    "\n",
    "print(f\"Min distance prediction for {word} is: \")\n",
    "!fstcompose in.bin.fst spell_checker_unigram.bin.fst |fstshortestpath --nshortest=1 \\\n",
    "| fstrmepsilon |  fsttopsort |fstprint -isymbols=chars.syms  -osymbols=chars.syms\\\n",
    "| cut -f4 | grep -v \"<epsilon>\" |head -n -1 | tr -d '\\n'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(γ) Οι δύο ορθογράφοι που δημιουργήσαμε έχουν ίδια αρχή λειτουργίας και αντιμετωπίζουν παρόμοια προβλήματα. Συγκεκριμένα η εκτίμηση τους είναι η λέξη με την ελάχιστη Levenhstein απόσταση απο την λέξη εισόδου. Η διαφορά είναι οτι στο word-level μοντέλο η επιλογή σταθμίζεται απο το βάρος της λέξης που τελικά επιλέγουμε, ενώ στο unigram μοντέλο σταθμίζεται αντιστοιχα κάθε επιλογή χαρακτήρα.\n",
    "\n",
    "Το μοντέλο επηρεάζεται σημαντικά απο το περιορισμένο λεξιλόγιο του, έτσι αν δεν γνωρίζει την ύπαρξη μίας λέξης δεν μπορεί να διορθώσει σε αυτή. Αυτός είναι ένας λογικός περιορισμός και διορθώνεται με την αύξηση των train δεδομένων. \n",
    "\n",
    "Το δεύτερο σημαντικό ελάττωμα του μοντέλου είναι οτι δεν διαθέτει αρκετά καλο κριτήριο για την επιλογή λέξεων σε περίπτωση ισοπαλίας. Αυτό έχει σε έναν βαθμό διορθωθεί με την χρήση στατιστικών στοιχείων στα παραπάνω 2 μοντέλα. \n",
    "\n",
    "Η αμφισημία προκύπτει γιατί παρά τα βάρη υπάρχουν λέξεις με ίδιο κόστος διόρθωσης και έτσι το μοντέλο πρέπει να επιλέξει τυχαία. (?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 14\n",
    "Θα αξιολογήσουμε τους δύο ορθογράφους που δημιουργήσαμε πάνω στο spell checker test set που είχαμε δεί στο βήμα 8. Θα χρησιμοποιήσουμε την συνάρτηση που ήδη έχουμε γράψει στο βήμα 8. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spell_checker.bin.fst-accuracy:0.5925925925925926%\n",
      "spell_checker_word.bin.fst-accuracy:0.6185185185185185%\n",
      "spell_checker_unigram.bin.fst-accuracy:0.5370370370370371%\n"
     ]
    }
   ],
   "source": [
    "#Too slow, need help :'( \n",
    "predict(y_test,X_test,\"spell_checker.bin.fst\",Show = False)\n",
    "predict(y_test,X_test,\"spell_checker_word.bin.fst\",Show = False)\n",
    "predict(y_test,X_test,\"spell_checker_unigram.bin.fst\",Show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 15\n",
    "Θα εκτελέσουμε τα προηγούμενα βήματα για ενα Bigram γλωσσικό μοντέλο.\n",
    "Το bigram γλωσσικό μοντέλο ανήκει στην ευρύτερη κλάση των n-gram μοντέλων και προκύπτει για n = 2. Με τον όρο n-gram αναφερόμαστε σε μία συνεχή ακολουθία n αντικειμένων απο ένα δείγμα φωνής ή κειμένου Συγκεκριμένα στο πλάισιο των λέξεων ένα n-gram γλωσσικό μοντέλο χρησιμοποιεί ακολουθίες n χαρακτήρων για να προβλέψει το επόμενο γράμμα. Η πρόβλεψη βασίζεται σε (n-1)-order αλυσίδα markov (δηλαδή ισχύει η μαρκοβιανή ιδιότητα αλλα η εξάρτηση σταματάει στα n προηγούμενα δείγματα). \n",
    "\n",
    "$$ \\mathbb{P}[x_i|x_{i-1},x_{i-2},...,x_0] = \\mathbb{P}[x_i|x_{i-1},x_{i-2},...,x_{i-(n-1)}] $$ \n",
    "\n",
    "Δυο πλεονεκτήματα τους ειναι:\n",
    "* Η απλότητα\n",
    "* Η κλιμακωσιμότητα\n",
    "\n",
    "Για το bigram μοντέλο αρκεί να υπολογίσουμε τις πιθανότητες:\n",
    "\n",
    "$$ \\mathbb{P}[x_i|x_{i-1}] $$\n",
    "για κάθε ζεύγος στο αλφάβητο μας (συν το $\\epsilon$).\n",
    "\n",
    "\n",
    "Στο παρακάτω κελί υπολογίζουμε με παρόμοιο τρόπο τις πιθανότητες εμφάνισης για κάθε bigram. Συγκεκριμένα έχουμε επαυξήσει κάθε λέξη του συνόλου ώστε να αρχίζει απο το κενό, αυτή είναι η πιθανότητα να επιλεγεί το γράμμα απο το οποίο αρχίζει η λέξη χωρίς να έχει προηγηθεί κάποιο άλλο. Έπειτα υπολογίζουμε τα κατάλληλα βάρη και υλοποιούμε τον μετατροπέα και τον αποδοχέα για να τους συνδυάσουμε στον bigram spell checker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extended_text = [ \" \" + word for word in text] # \" \" symbolizes epsilon, for bigrams like (<epsilon>,a)\"\n",
    "bigrams = [(char1,char2) for word in extended_text for char1,char2 in zip(word,word[1:])]\n",
    "bigram_prob = Counter(bigrams)\n",
    "bigram_prob = {bigram:prob/len(bigrams) for bigram,prob in bigram_prob.items()}\n",
    "\n",
    "weight_bigrams =  {bigram: -math.log(prob,2) for bigram,prob in bigram_prob.items()}\n",
    "avg_weight_bigrams = sum(list(weight_bigrams.values()))/len(list(weight_bigrams.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spell_checker_bigram.bin.fst-accuracy:0.3814814814814815%\n"
     ]
    }
   ],
   "source": [
    "create_lev(\"lev_bigram.fst\",alphabet,avg_weight_bigrams)\n",
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  lev_bigram.fst lev_bigram.bin.fst\n",
    "\n",
    "create_acceptor(word_corpus,\"acceptor_bigram.fst\",weight_bigrams,model='Bigram')\n",
    "\n",
    "!fstcompile --isymbols=chars.syms --osymbols=chars.syms  acceptor_bigram.fst acceptor_bigram.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstrmepsilon acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstminimize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "\n",
    "\n",
    "!fstarcsort --sort_type=olabel lev_word.bin.fst lev_word.bin.fst\n",
    "#!fstarcsort --sort_type=olabel lev_bigram.bin.fst lev_bigram.bin.fst\n",
    "\n",
    "!fstcompose  lev_word.bin.fst acceptor_bigram.bin.fst spell_checker_bigram.bin.fst\n",
    "#!fstcompose  lev_bigram.bin.fst acceptor_bigram.bin.fst spell_checker_bigram.bin.fst\n",
    "\n",
    "!fstdeterminize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstrmepsilon acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "!fstminimize acceptor_bigram.bin.fst acceptor_bigram.bin.fst\n",
    "\n",
    "\n",
    "predict(y_test,X_test,\"spell_checker_bigram.bin.fst\",Show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε οτι με το bigram μοντέλο η ακρίβεια πάνω στο test set έπεσε στο **_0.185%_**. Αυτο επιβεβαιώνει την αντίληψη μας ότι μοντέλα βασισμένα σε χαρακτήρες δεν λειτουργούν καλά για το συγκεκριμένο πρόβλημα, και μία προσέγγιση με στοιχεία λέξεις θα έχει πολυ καλύτερα αποτελέσματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spell_checker.bin.fst-accuracy:0.6%\n",
      "spell_checker_word.bin.fst-accuracy:0.6%\n",
      "spell_checker_unigram.bin.fst-accuracy:0.5%\n",
      "spell_checker_bigram.bin.fst-accuracy:0.45%\n"
     ]
    }
   ],
   "source": [
    "idxs = random.sample(range(0, len(y_test)), 20)\n",
    "X_rand = [X_test[i] for i in idxs]\n",
    "Y_rand = [y_test[i] for i in idxs]\n",
    "\n",
    "predict(Y_rand,X_rand,\"spell_checker.bin.fst\",Show = False)\n",
    "predict(Y_rand,X_rand,\"spell_checker_word.bin.fst\",Show = False)\n",
    "predict(Y_rand,X_rand,\"spell_checker_unigram.bin.fst\",Show = False)\n",
    "predict(Y_rand,X_rand,\"spell_checker_bigram.bin.fst\",Show = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">ΜΕΡΟΣ 2</h1>\n",
    "\n",
    "<h3 align = \"center\">Χρήση σημασιολογικών αναπαραστάσεων για ανάλυση συναισθήματος.</h3>\n",
    "\n",
    "Στο πρώτο μέρος της άσκησης ασχοληθήκαμε κυρίως με συντακτικά μοντέλα για την κατασκευή ενός ορθογράφου. Εδώ θα ασχοληθούμε με τη χρήση λεξικών αναπαραστάσεων για την κατασκευή ενός ταξινομητή συναισθήματος. Ως δεδομένα θα χρησιμοποιήσουμε σχόλια για ταινίες από την ιστοσελίδα IMDB και θα τα ταξινομήσουμε σε θετικά και αρνητικά ως προς το συναίσθημα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 16(a)\n",
    "\n",
    "#!wget -P ./data/ http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 16(b)\n",
    "\n",
    "import os\n",
    "\n",
    "data_dir = './data/aclImdb_v1/aclImdb/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "pos_train_dir = os.path.join(train_dir, 'pos')\n",
    "neg_train_dir = os.path.join(train_dir, 'neg')\n",
    "pos_test_dir = os.path.join(test_dir, 'pos')\n",
    "neg_test_dir = os.path.join(test_dir, 'neg')\n",
    "\n",
    "# For memory limitations. These parameters fit in 8GB of RAM.\n",
    "# If you have 16G of RAM you can experiment with the full dataset / W2V\n",
    "MAX_NUM_SAMPLES = 5000\n",
    "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
    "# sorted from most frequent to least frequent.\n",
    "# It may yield much worse results for other embeddings corpora\n",
    "NUM_W2V_TO_LOAD = 1000000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Fix numpy random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "try:\n",
    "    import glob2 as glob\n",
    "except ImportError:\n",
    "    import glob\n",
    "\n",
    "import re\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', ' ', s)\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub('\\s+',' ', strip_punctuation(s).lower())\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "def preproc_tok(s):\n",
    "    return tokenize(preprocess(s))\n",
    "\n",
    "def read_samples(folder, preprocess=lambda x: x):\n",
    "    samples = glob.iglob(os.path.join(folder, '*.txt'))\n",
    "    data = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
    "            break\n",
    "        with open(sample, 'r') as fd:\n",
    "            x = [preprocess(l) for l in fd][0]\n",
    "            data.append(x)\n",
    "    return data\n",
    "\n",
    "def create_corpus(pos, neg):\n",
    "    corpus = np.array(pos + neg)\n",
    "    y = np.array([1 for _ in pos] + [0 for _ in neg])\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return list(corpus[indices]), list(y[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 16(b)\n",
    "\n",
    "(X_train,y_train) = create_corpus(read_samples(pos_train_dir),read_samples(neg_train_dir))\n",
    "(X_test,y_test) = create_corpus(read_samples(pos_test_dir),read_samples(neg_test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 17 (α) --- Κατασκευή ΒΟW αναπαραστάσεων και ταξινόμηση***\n",
    "\n",
    "Η πιο βασική αναπαράσταση για μια πρόταση είναι η χρήση Bag of Words. Σε αυτή την αναπαράσταση μια λέξη κωδικοποιείται σαν ένα one hot encoding πάνω στο λεξιλόγιο και μια πρόταση σαν το άθροισμα αυτών των encodings. Επιπλέον μπορούμε να πάρουμε σταθμισμένο άθροισμα των one hot word encodings για την αναπαράσταση μιας πρότασης με βάρη TF-IDF.\n",
    "\n",
    "Το TF-IDF αποτελείται από 2 όρους. Ο πρώτος είναι το **Term Frequency (TF)**:\n",
    "\n",
    "$$ tf(i,d) = \\frac{f(i,d)}{\\sum_{i} f(i,d)}$$\n",
    "\n",
    "Όπου *i* ο όρος στο κείμενο *d*. Το tf είναι στην ουσία η συχνότητα με την οποία εμφανίζεται ο κάθε όρος στο κείμενο. \n",
    "Λέξεις με μεγάλη συχνότητα είναι σημαντικότερες για το κείμενο από ότι οι λέξεις με μικρή συχνότητα.\n",
    "\n",
    "Ο δεύτερος όρος στο TF-IDF είναι το **Inverse Document Frequency**:\n",
    "\n",
    "$$ idf(i) = log \\frac{N}{df(i)}$$\n",
    "\n",
    "Όπου *Ν* ο αριθμός των κειμένων και *df(i)* ο αριθμός των κειμένων στους οποίους εμφανίζεται ο όρος *i*. Το idf είναι ένας δείκτης της πληροφορίας που δίνει η κάθε λέξη. Αν η λέξη εμφανίζεται σε όλα τα κείμενα τότε αυτή δε δίνει καθόλου πληροφορία και το κλάσμα θα γίνει 1, άρα ο λογάριθμος θα μας δώσει την τιμή 0. Αντίθετα σε όσο πιο λίγα κείμενα εμφανίζεται η λέξη, τόσο πιο μεγάλη τιμή θα έχει το κλάσμα. \n",
    "\n",
    "Το TF-IDF υπολογίζεται τελικά ως το γινόμενο των δύο όρων:\n",
    "\n",
    "$$ tfidf(i) = tf(i,d) \\cdot idf(i)$$\n",
    "\n",
    "Άρα, αν το γινόμενο TF-IDF είναι υψηλό, τότε η λέξη i είναι σημαντική πληρορορία στο κείμενο d αφού η λέξη αυτή εμφανίζεται πολλές φορές στο κείμενο και δεν εμφανίζεται σε πολλά από τα Ν κείμενα που που εξετάζονται."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 17 (β, γ, δ)***\n",
    "\n",
    "Σε αυτά τα βήματα θα χρησιμοποιήσουμε τον transformer CountVectorizer του sklearn για την εξαγωγή μη σταθμισμένων BOW αναπαραστάσεων και θα εκπαιδεύσουμε τον ταξινομητή LogisticRegression για να ταξινομήσουμε τα σχόλια σε θετικά και αρνητικά. \n",
    "\n",
    "O **CountVectorizer** μετατρέπει μία συλλογή από κείμενα σε έναν πίνακα στον οποίο αποθηκεύεται ο αριθμός εμφάνισης των tokens. Αυτή η υλοποιήση παράγει μία αραιή αναπαράσταση του αριθμού εφάνισης των tokens χρησιμοποιώντας το scipy.sparse.csr_matrix.\n",
    "\n",
    "Ο **TfidfVectorizer** μετατρέπει μια συλλογή από μη επεξεργασμένα κείμενα σε έναν πίνακα με τις TF-IDF τιμές των tokens. Είναι ισοδύναμος με τον CountVectorizer αλλά χρησιμοποιεί έναν TfidfTransformer. Ο TfidfTransformer υπολογίζει τα γινόμενα TF-IDF για κάθε token του κειμένου.\n",
    "\n",
    "Ο **Logistic Regression** ταξινομητής χρησιμοποιείται για να μοντελοποιήσουμε την πιθανότητα να επιλεγεί μια συγκεκριμένη κλάση ή να συμβεί ένα γεγονός. Μπορεί να χρησιμοποιήθεί για την κατηγοριοποίηση κλάσεων σε δύο κατηγορίες όπως θετικές και αρνητικές αρνητικές κριτικές, είτε για την κατηγοριοποίηση κλάσεων σε περισσότερες από δύο κατηγορίες.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 17(b)\n",
    "\n",
    "#import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 17(b,c)\n",
    "#Count Vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# X_train_BOW_C[i] is the bag of words CountVectorizer representation for the i-th comment\n",
    "X_train_BOW_C = vectorizer.fit_transform(X_train)\n",
    "vectorizer_test_C = CountVectorizer(vocabulary = vectorizer.get_feature_names())\n",
    "X_test_BOW_C = vectorizer_test_C.fit_transform(X_test)\n",
    "LG_C = LogisticRegression(random_state=0, multi_class = 'ovr', solver = 'liblinear',penalty = 'l2')\n",
    "xx_C = LG_C.fit(X_train_BOW_C,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 17(d)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# X_train_BOW_T[i] is the bag of words TfidfVectorizer representation for the i-th comment\n",
    "X_train_BOW_T = vectorizer.fit_transform(X_train)\n",
    "vectorizer_test_T = TfidfVectorizer(vocabulary = vectorizer.get_feature_names())\n",
    "X_test_BOW_T = vectorizer_test_T.fit_transform(X_test)\n",
    "LG_T = LogisticRegression(random_state=0, multi_class = 'ovr', solver = 'liblinear',penalty = 'l2')\n",
    "xx_T = LG_T.fit(X_train_BOW_T,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 17(δ) --- Σύγκριση αποτελεσμάτων*** \n",
    "\n",
    "Συγκρίνοντας τα αποτελέσματα των δύο Vectorizer() παρατηρούμε ότι τα δύο ποσοστά έχουν πολύ μιρκή διαφορά. Καλύτερη ακρίβεια έχει ο TfidfVectorizer. Αυτό συμβαίνει γιατί η χρήση tf-idf συχνοτήτων συμβάλλει στην μείωση της επίδρασης των tokens που εμφανίζονται πολύ συχνά στο κείμενο. Όπως είπαμε και παραπάνω, τα tokens αυτά δεν δίνουν τόση πληροφορία για το κείμενο όσο οι λέξεις που εμφανίζονται λιγότερο και για αυτό είναι καλύτερα να μην λαμβάνονται υπόψιν κατά τη διαδικασία του classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Count Vectorizer is: 0.8621\n",
      "The accuracy of the Tfidf Vectorizer is: 0.8697\n"
     ]
    }
   ],
   "source": [
    "#Step 17(d)\n",
    "\n",
    "acc_vec = 0\n",
    "for sample,label in zip(X_test_BOW_C,y_test):\n",
    "    if xx_C.predict(sample) == label:\n",
    "        acc_vec+=1\n",
    "print(f'The accuracy of the Count Vectorizer is: {acc_vec/len(X_test)}')\n",
    "\n",
    "acc_tfidf = 0\n",
    "for sample,label in zip(X_test_BOW_T,y_test):\n",
    "    if xx_T.predict(sample) == label:\n",
    "        acc_tfidf+=1\n",
    "print(f'The accuracy of the Tfidf Vectorizer is: {acc_tfidf/len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα18  ---  Χρήση Word2Vec αναπαραστάσεων για ταξινόμηση***\n",
    "\n",
    "Ένας άλλος τρόπος για να αναπαραστήσουμε λέξεις και προτάσεις είναι να κάνουμε χρήση προεκπαιδευμένων embeddings. Σε αυτό το βήμα θα εστιάσουμε στα word2vec embeddings.Αυτά τα embeddings προκύπτουν από ένα νευρωνικό δίκτυο με ένα layer το οποίο καλείται να προβλέψει μια λέξη με βάση το context της (παράθυρο 3-5 λέξεων γύρω από αυτή). Αυτό ονομάζεται **CBOW μοντέλο**. Εναλλακτικά το δίκτυο καλείται να προβλέψει το context με βάση τη λέξη (skip-gram μοντέλο).Τα word2vec vectors είναι πυκνές (dense) αναπαραστάσεις σε λιγότερες διαστάσεις από τις BOW και κωδικοποιούν σημασιολογικά χαρακτηριστικά μιας λέξης με βάση την υπόθεση ότι λέξεις με παρόμοιο νόημα εμφανίζονται σε παρόμοια συγκείμενα (contexts). Μια πρόταση μπορεί να αναπαρασταθεί ως ο μέσος όρος των w2v διανυσμάτων κάθε λέξης που περιέχει (Neural Bag of Words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 18(α)***\n",
    "\n",
    "Σε αυτό το βήμα υπολογίσαμε το ποσοστό των out of vocabulary (OOV) words για τις αναπαραστάσεις που υπολογίσαμε στο Βήμα 9. Το ποσοστό που ζητείται δίνεται από τον τύπο \n",
    "$$ OOV =\\frac{\\text{num of unique words in X_train - num of words in  voc of word2vec}}{\\text{num of unique words in X_train}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of OOV is: 95.41862993838348 %\n"
     ]
    }
   ],
   "source": [
    "#Step 18(a)\n",
    "#OOV words\n",
    "\n",
    "c = []\n",
    "for critic in X_train:\n",
    "    c += nltk.word_tokenize(critic)\n",
    "\n",
    "c = set(c)\n",
    "v = set(voc)\n",
    "num = c.difference(voc)\n",
    "    \n",
    "    \n",
    "print(f'The percentage of OOV is: {100*float(len(num))/len(c)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 18(β)***\n",
    "\n",
    "Σε αυτό το βήμα θα χρησιμοποιήσουμε τα embeddings που προκύπτουν από το word_corpus που δημιουργήσαμε από τα τρία βιβλία που αναφέρονται στην αρχή. Στη συνέχεια, θα χρησιμοποιήσουμε αυτά τα embeddings για την κατασκευή Neural Bag of Words αναπαραστάσεων για κάθε σχόλιο στο corpus(κριτικές από το IMDB) και θα εκπαιδεύσουμε ένα Logistic Regression μοντέλο για ταξινόμηση των κριτικών σε θετικές και αρνητικές. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18(b)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy 2d array (n_vocab x vector_size)\n",
    "def to_embeddings_Matrix(model):  \n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n",
    "    word2idx = {}\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_matrix[i] = model.wv[model.wv.index2word[i]]\n",
    "        word2idx[model.wv.index2word[i]] = i\n",
    "    return embedding_matrix, model.wv.index2word, word2idx\n",
    "\n",
    "(embedding_matrix, model.wv.index2word, word2idx) = to_embeddings_Matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18(b)\n",
    "#Neural Bag of Words for movie critics\n",
    "\n",
    "#for every critic in test and train sets we create an Neural bag of words representation\n",
    "#by adding the embeddings for the words in the critic and dividing by the length of it\n",
    "    \n",
    "train = np.zeros((len(X_train),model.vector_size))\n",
    "count_c = 0                      \n",
    "for critic in X_train:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in word2idx.keys():\n",
    "            i = word2idx[word]\n",
    "            train[count_c] += embedding_matrix[i]\n",
    "    train[count_c] /= len(critic)\n",
    "    count_c += 1\n",
    "                    \n",
    "\n",
    "test = np.zeros((len(X_test),model.vector_size))   \n",
    "count_c = 0      \n",
    "for critic in X_test:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in word2idx.keys():\n",
    "            i = word2idx[word]\n",
    "            test[count_c] += embedding_matrix[i]\n",
    "    test[count_c] /= len(critic)\n",
    "    count_c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the LogisticRegression Model with our word_corpus is: 0.7195\n"
     ]
    }
   ],
   "source": [
    "#Step 18(b)\n",
    "\n",
    "LG = LogisticRegression(random_state=0, multi_class = 'ovr', solver = 'liblinear',penalty = 'l2')\n",
    "xx = LG.fit(train,y_train)\n",
    "xx.predict(test)\n",
    "print(f'The accuracy of the LogisticRegression Model with our word_corpus is: {xx.score(test,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Bήμα 18(β)---Συμπεράσματα***\n",
    "\n",
    "Παρατηρούμε ότι το accuracy του μοντέλου μας είναι αρκετά μικρό. Αυτό συμβαίνει γιατί το ποσοστό των OOV είναι υψηλό και άρα πολλές λέξεις που υπάρχουν στις προτάσεις της κάθε κριτικής δεν συμβάλλουν στον υπολογισμό της Neural Bag of Words αναπαράστασης. Συμπεραίνουμε λοιπόν, ότι όσο μεγαλύτερο word_corpus έχουμε για την εκπαίδευση του μοντέλου μας και τη δημιουργία των embeddings, τόσο μεγαλύτερο accuracy θα έχουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18(c)\n",
    "\n",
    "#Download pretrained GoogleNews vectors\n",
    "#from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 18(δ)***\n",
    "\n",
    "Σε αυτό το βήμα φορτώσαμε τα GoogleNews με τη βιβλιοθήκη gensim και εξάγαμε αναπαραστάσεις (word2vec embeddings) με βάση αυτά. Στη συνέχεια, για τις λέξεις που είχαμε βρει το similarity με άλλες λέξεις στο corpus στο βήμα 9γ, υπολογίσαμε το similarity με βάση το μοντέλο με τα GoogleNews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18(d)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin',binary=True, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"bell\":\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorotheakal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \"bells\" -- sim: 0.6517581939697266\n",
      "     \"bell_rang\" -- sim: 0.6194064617156982\n",
      "     \"opening_bell\" -- sim: 0.6053600311279297\n",
      "     \"closing_bell\" -- sim: 0.5642949342727661\n",
      "     \"ringing_bell\" -- sim: 0.5528850555419922\n",
      "     \"bell_rings\" -- sim: 0.5492205619812012\n",
      "     \"bell_chimes\" -- sim: 0.5219579339027405\n",
      "     \"bell_ringing\" -- sim: 0.5187430381774902\n",
      "     \"bell_sounded\" -- sim: 0.5167623162269592\n",
      "     \"knell\" -- sim: 0.4984786808490753\n",
      "Most similar words to \"silver\":\n",
      "     \"gold\" -- sim: 0.8313291668891907\n",
      "     \"precious_metal\" -- sim: 0.6105663776397705\n",
      "     \"bronze\" -- sim: 0.5610242486000061\n",
      "     \"palladium_platinum\" -- sim: 0.5458641052246094\n",
      "     \"copper\" -- sim: 0.5440827012062073\n",
      "     \"##ct_gold\" -- sim: 0.5349764823913574\n",
      "     \"Platinum_palladium\" -- sim: 0.5325815677642822\n",
      "     \"precious_metals\" -- sim: 0.5297213792800903\n",
      "     \"Gold\" -- sim: 0.5211714506149292\n",
      "     \"troy_oz\" -- sim: 0.5147095918655396\n",
      "Most similar words to \"window\":\n",
      "     \"windows\" -- sim: 0.7260904908180237\n",
      "     \"door\" -- sim: 0.6212795972824097\n",
      "     \"window_pane\" -- sim: 0.6208164691925049\n",
      "     \"doorway\" -- sim: 0.6085779666900635\n",
      "     \"windshield\" -- sim: 0.5802630186080933\n",
      "     \"sliding_glass\" -- sim: 0.5717440843582153\n",
      "     \"glass_pane\" -- sim: 0.5539859533309937\n",
      "     \"doors\" -- sim: 0.5523053407669067\n",
      "     \"skylight\" -- sim: 0.5513849258422852\n",
      "     \"rear_driver's_side\" -- sim: 0.5299071073532104\n",
      "Most similar words to \"thank\":\n",
      "     \"sincerely_thank\" -- sim: 0.8009909391403198\n",
      "     \"thanking\" -- sim: 0.7849280834197998\n",
      "     \"Thank\" -- sim: 0.7770504951477051\n",
      "     \"heartfelt_thanks\" -- sim: 0.7628011107444763\n",
      "     \"thanked\" -- sim: 0.7537007331848145\n",
      "     \"sincere_gratitude\" -- sim: 0.7462623119354248\n",
      "     \"sincere_appreciation\" -- sim: 0.7307636141777039\n",
      "     \"congratulate\" -- sim: 0.72502201795578\n",
      "     \"commend\" -- sim: 0.7216262817382812\n",
      "     \"grateful\" -- sim: 0.7013533711433411\n",
      "Most similar words to \"copyright\":\n",
      "     \"copyrights\" -- sim: 0.8414074182510376\n",
      "     \"copyright_infringement\" -- sim: 0.7244922518730164\n",
      "     \"copyrighted\" -- sim: 0.7164671421051025\n",
      "     \"copyright_holders\" -- sim: 0.694472074508667\n",
      "     \"Copyright\" -- sim: 0.6810053586959839\n",
      "     \"copyright_holder\" -- sim: 0.6616141200065613\n",
      "     \"DMCA\" -- sim: 0.6590597629547119\n",
      "     \"Copyrights\" -- sim: 0.6396814584732056\n",
      "     \"intellectual_property\" -- sim: 0.6138449907302856\n",
      "     \"intellectual_property_rights\" -- sim: 0.594609260559082\n",
      "Most similar words to \"bow\":\n",
      "     \"bows\" -- sim: 0.7369688749313354\n",
      "     \"bowing\" -- sim: 0.4897507429122925\n",
      "     \"bowed\" -- sim: 0.45847630500793457\n",
      "     \"bowstring\" -- sim: 0.4376411437988281\n",
      "     \"forelock\" -- sim: 0.4299067556858063\n",
      "     \"velvet_ribbon\" -- sim: 0.42781659960746765\n",
      "     \"sterns\" -- sim: 0.425993412733078\n",
      "     \"satin_ribbon\" -- sim: 0.4177667200565338\n",
      "     \"nock\" -- sim: 0.41291478276252747\n",
      "     \"quiver\" -- sim: 0.4098646640777588\n",
      "Most similar words to \"splendid\":\n",
      "     \"magnificent\" -- sim: 0.8408762812614441\n",
      "     \"marvelous\" -- sim: 0.7875252962112427\n",
      "     \"superb\" -- sim: 0.7729603052139282\n",
      "     \"glorious\" -- sim: 0.7077421545982361\n",
      "     \"brilliant\" -- sim: 0.7077069282531738\n",
      "     \"delightful\" -- sim: 0.687921941280365\n",
      "     \"scintillating\" -- sim: 0.6671627759933472\n",
      "     \"dazzling\" -- sim: 0.6615424156188965\n",
      "     \"superlative\" -- sim: 0.6546359062194824\n",
      "     \"sublime\" -- sim: 0.6518499851226807\n",
      "Most similar words to \"death\":\n",
      "     \"deaths\" -- sim: 0.645300567150116\n",
      "     \"murder\" -- sim: 0.6415224075317383\n",
      "     \"untimely_death\" -- sim: 0.6404935717582703\n",
      "     \"slaying\" -- sim: 0.6001620888710022\n",
      "     \"killing\" -- sim: 0.5760151147842407\n",
      "     \"fatal\" -- sim: 0.5714417099952698\n",
      "     \"murdered\" -- sim: 0.5639129877090454\n",
      "     \"deat\" -- sim: 0.5637020468711853\n",
      "     \"Death\" -- sim: 0.5529274344444275\n",
      "     \"died\" -- sim: 0.544864296913147\n",
      "Most similar words to \"dragging\":\n",
      "     \"dragged\" -- sim: 0.7259875535964966\n",
      "     \"Dragging\" -- sim: 0.6128302216529846\n",
      "     \"drag\" -- sim: 0.6019066572189331\n",
      "     \"drags\" -- sim: 0.5446232557296753\n",
      "     \"pushing\" -- sim: 0.49698585271835327\n",
      "     \"crawling\" -- sim: 0.48572254180908203\n",
      "     \"pulling\" -- sim: 0.4721095561981201\n",
      "     \"stomping\" -- sim: 0.4652405083179474\n",
      "     \"slipping\" -- sim: 0.45722097158432007\n",
      "     \"stumbling\" -- sim: 0.4531708359718323\n",
      "Most similar words to \"seven\":\n",
      "     \"eight\" -- sim: 0.9619014263153076\n",
      "     \"five\" -- sim: 0.9528537392616272\n",
      "     \"six\" -- sim: 0.9523511528968811\n",
      "     \"four\" -- sim: 0.9456813335418701\n",
      "     \"nine\" -- sim: 0.9356343746185303\n",
      "     \"three\" -- sim: 0.9122412800788879\n",
      "     \"two\" -- sim: 0.805267870426178\n",
      "     \"eleven\" -- sim: 0.7146920561790466\n",
      "     \"##\" -- sim: 0.6923109292984009\n",
      "     \"ten\" -- sim: 0.6571058034896851\n"
     ]
    }
   ],
   "source": [
    "#Step 18(d)\n",
    "\n",
    "#Similarity\n",
    "#pick 10 random words from the dictionary\n",
    "for word in rand_words:\n",
    "    print(f'Most similar words to \"{word}\":')\n",
    "    for word,sim in model.wv.most_similar(word):\n",
    "        print(f'     \"{word}\" -- sim: {sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 18(δ) --- Συμπεράσματα***\n",
    "\n",
    "Παρατηρούμε ότι το similarity για τις ίδιες λέξεις του ερωτήματος 9γ έχει πλέον αυξηθεί. Οι λέξεις που βρίσκει το μοντέλο με τα GoogleNews είναι πιο σχετικές με την υπο εξέταση λέξη, καθώς και το ποσοστό ομοιότητας έχει σχεδόν διπλασιαστεί. Αυτό είναι ένα αποτέλεσμα που περιμέναμε, από τη στιγμή που έχουμε ένα word_corpus κατά τάξεις μεγαλύτερο από αυτό που είχαμε δημιουργήσει στα πρώτα ερωτήματα. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 18(ε)***\n",
    "\n",
    "Σε αυτό το βήμα δημιουργήσαμε αναπαραστάσεις Neural Bag of Words για κάθε κριτική με τη χρήση των embeddings για το μοντέλο με τα Google News. Χρησιμοποιώντας αυτές τις αναπαραστάσεις εκπαιδεύσαμε ένα Logistic Regression Model για να αναγνωρίζει αν μία κριτική είναι θετική ή αρνητική.\n",
    "\n",
    "Συγκρίνοντας το αποτέλεσμα του Logistic Regression Model που χρησιμοποιεί αναπαραστάσεις με τη χρησή των Google News, με το μοντέλο που χρησιμοποιεί αναπαραστάσεις με τη χρήση του word_corpus που δημιουργήσαμε, συμπεραίνουμε ότι το πρώτο έχει μεγαλύτερο accuracy. Αυτό συμβαίνει γιατί τα Google News, λόγω του μεγαλύτερου μεγέθους τους, δίνουν embeddings για περισσότερες λέξεις. Αρα η Neural Bag of Words αναπαράσταση για κάθε κριτική, δίνει καλύτερη πληροφορία για το αν είναι θετική ή αρνητικη και έτσι αυξάνεται η πιθανότητα να γίνει η ταξινόμηση της στη σωστή κατηγορία."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18(e)\n",
    "#Neural Bag of Words for movie critics\n",
    "    \n",
    "train_google = np.zeros((len(X_train),model.vector_size))\n",
    "count_c = 0    \n",
    "                      \n",
    "for critic in X_train:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in model:\n",
    "            train_google[count_c] += model[word]\n",
    "    train_google[count_c] /= len(critic)\n",
    "    count_c += 1\n",
    "    \n",
    "test_google = np.zeros((len(X_test),model.vector_size))   \n",
    "count_c = 0\n",
    "      \n",
    "for critic in X_test:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in model:\n",
    "            test_google[count_c] += model[word]\n",
    "    test_google[count_c] /= len(critic)\n",
    "    count_c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the LogisticRegression Model with GoogleNews is: 0.8222\n"
     ]
    }
   ],
   "source": [
    "#Step 18(e)\n",
    "\n",
    "LG = LogisticRegression(random_state=0, multi_class = 'ovr', solver = 'liblinear',penalty = 'l2')\n",
    "xx = LG.fit(train_google,y_train)\n",
    "xx.predict(test_google)\n",
    "print(f'The accuracy of the LogisticRegression Model with GoogleNews is: {xx.score(test_google,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 18(στ)***\n",
    "\n",
    "Σε αυτό το βήμα δημιουργείσαμε αναπαραστάσεις προτάσεων με χρήση σταθμισμένου μέσου των w2v αναπαραστάσεων των λέξεων. Ως βάρη χρησιμοποιήσαμε τα TF-IDF βάρη των λέξεων. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18(f)\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer = nltk.word_tokenize)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "voc_train = tfidf.vocabulary_\n",
    "X_test_tfidf = tfidf.fit_transform(X_test)\n",
    "voc_test = tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18(f)\n",
    "#Neural Bag of Words for movie critics\n",
    "    \n",
    "train_tfidf = np.zeros((len(X_train),model.vector_size))\n",
    "count_c = 0    \n",
    "                      \n",
    "for critic in X_train:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in model and word in voc_train:\n",
    "            train_tfidf[count_c] += model[word]*X_train_tfidf[count_c,voc_train[word]]\n",
    "    count_c += 1\n",
    "                 \n",
    "\n",
    "test_tfidf = np.zeros((len(X_test),model.vector_size))   \n",
    "count_c = 0\n",
    "count = 0\n",
    "                 \n",
    "for critic in X_test:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in model and word in voc_test:\n",
    "            test_tfidf[count_c] += model[word]*X_test_tfidf[count_c,voc_test[word]]\n",
    "    count_c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the LogisticRegression Model with GoogleNews tf-idf embeddings is: 0.8048\n"
     ]
    }
   ],
   "source": [
    "#Step 18(g)\n",
    "\n",
    "LG = LogisticRegression(random_state=0, multi_class = 'ovr', solver = 'liblinear',penalty = 'l2')\n",
    "xx = LG.fit(train_tfidf,y_train)\n",
    "print(f'The accuracy of the LogisticRegression Model with GoogleNews tf-idf embeddings is: {xx.score(test_tfidf,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 18 (ζ) --- Συμπεράσματα***\n",
    "\n",
    "Τρέχοντας το LogisticRegression Model με τα GoogleNews δεδομένα για τα tf-idf embeddings, παρατηρούμε ότι το ποσοστό μειώνεται κατά 2% σε σχέση με τη χρήση embeddings χωρίς τα tf-idf βάρη. Αυτό συμβαίνει γιατί τα tf-idf μειώνουν την επίδραση των λέξεων που εμφανίζονται πολλές φορές στο κείμενο, καθώς και αυτών που εμφανίζονται πολύ λίγες φορές. Άρα, ενδέχεται στις λέξεις που δεν λήφθηκαν υπόψιν, να ήταν και κάποιες που έδιναν κάποια επιπλέον πληροφορία για το αν η κριτική ήταν θετική ή αρνητική."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 19 (α)***\n",
    "\n",
    "Σε αυτό το βήμα επιλέξαμε να συγκρίνουμε την επίδοση των KNN και SVM classifiers. \n",
    "\n",
    "**KNN classifier** : Αυτός ο classifier υποθέτει ότι οι αναπαραστάσεις των λέξεων με παρόμοια σημασία θα είναι και κοντά στον χώρο. Θεωρούμε λοιπόν, ότι για να κάνουμε classify, θα βρούμε τις 3 κοντινότερες αναπαραστάσεις στην αναπαράσταση της κριτικής και με βάση αυτές θα αποφανθούμε αν είναι θετική ή αρνητική.\n",
    "\n",
    "**SVM classifiers** : Αυτός ο classifier αναπαριστά τα train-data σαν σημεία στον χώρο έτσι ώστε τα σημεία από κάθε κατηγορία να απέχουν όσο περισσότερο γίνεται. Τα test-data στη συνέχεια, τοποθετούνται πάνω στον ίδιο χώρο και ανάλογα σε ποιο σημείο πέφτουν γίνεται το classification τους σε κάποια κατηγορία."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy on train_google embeddings is: 0.7085\n",
      "KNN accuracy on train_tfidf embeddings is: 0.6509\n"
     ]
    }
   ],
   "source": [
    "#Step 19(a)\n",
    "\n",
    "#ΚΝΝ\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#Google embeddings\n",
    "neigh.fit(train_google, y_train)\n",
    "print(f'KNN accuracy on train_google embeddings is: {neigh.score(test_google,y_test)}')\n",
    "\n",
    "#TF-IDF embeddings\n",
    "neigh.fit(train_tfidf, y_train)\n",
    "print(f'KNN accuracy on train_tfidf embeddings is: {neigh.score(test_tfidf,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy on train_google embeddings is: 0.6595\n",
      "SVM accuracy on train_tfidf embeddings is: 0.8005\n"
     ]
    }
   ],
   "source": [
    "#Step 19(a)\n",
    "\n",
    "#SVM\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='auto')\n",
    "\n",
    "#Google embeddings\n",
    "clf.fit(train_google, y_train)\n",
    "print(f'SVM accuracy on train_google embeddings is: {clf.score(test_google,y_test)}')\n",
    "\n",
    "#TF-IDF embeddings\n",
    "clf.fit(train_tfidf, y_train)\n",
    "print(f'SVM accuracy on train_tfidf embeddings is: {clf.score(test_tfidf,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 19 (α) --- Συμπεράσματα***\n",
    "\n",
    "Παρατηρούμε λοιπόν ότι για τον KNN Classifier έχουμε καλύτερο accuracy για τα μη σταθμισμένα embeddings ενώ για τον SVM έχουμε καλύτερο accuracy για τα σταθμισμένα με TF-IDF embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Βήμα 19 (β)***\n",
    "\n",
    "Το FastText είναι μια επέκταση του Word2Vec που προτάθηκε από το Facebook το 2016. Αντί να δίνονται ως είσοδος στο νευρωνικό ολόκληρες οι λέξεις, τις \"σπάνε\" σε κάποια n-grams (υπο-λέξεις). Για παράδειγμα η λέξη apple σπάει σε 3-gram ως εξής: app, ppl, και ple. Το embedding αυτής της λέξης θα είναι το άθροισμα των 3-grams (ή n-grams γενικότερα) για αυτή. Μόλις εκπαιδεύσουμε το νευρωνικό θα έχουμε word embeddings για κάθε ένα από τα n-grams στο δεδομένο dataset. Οι σπάνιες λέξεις θα μπορούν πλέον να αναπαρασταθούν καλύτερα αφού είναι αρκετά πιθανό κάποιο από τα n-grams τους να εμφανιστεί σε κάποια άλλη λέξη. \n",
    "\n",
    "Για αυτό το βήμα θα χρησιμοποιήσουμε ήδη εκπαιδευμένα embeddings τα οποία κατεβάσαμε από την σελίδα https://fasttext.cc/docs/en/english-vectors.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 19(b)\n",
    "\n",
    "from gensim.models.fasttext import  FastTextKeyedVectors\n",
    "\n",
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore' , limit = NUM_W2V_TO_LOAD )\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "model_fast = load_vectors(\"./data/wiki-news-300d-1M.vec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 19(b)\n",
    "#Neural Bag of Words for movie critics\n",
    "    \n",
    "train_fast = np.zeros((len(X_train),len(model_fast))\n",
    "count_c = 0                       \n",
    "for critic in X_train:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in model_fast:\n",
    "            train_fast[count_c] += model_fast[word]\n",
    "    train_fast[count_c] /= len(critic)\n",
    "    count_c += 1\n",
    "    \n",
    "test_fast = np.zeros((len(X_test),model.vector_size))   \n",
    "count_c = 0\n",
    "for critic in X_test:\n",
    "    critic = nltk.word_tokenize(critic)\n",
    "    for word in critic:\n",
    "        if word in model_fast:\n",
    "            test_fast[count_c] += model_fast[word]\n",
    "    test_fast[count_c] /= len(critic)\n",
    "    count_c += 1\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
